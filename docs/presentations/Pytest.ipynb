{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition\n",
    "Das Pytest Framework wird dazu verwendet Unit-Tests in Python umzusetzen.<br>\n",
    "In diesem Notebook wird für die Demonstration die ipytest Bibliothek verwendet. (https://github.com/chmp/ipytest)<br>\n",
    "Diese ermöglicht es pytests über magics Befehle auszuführen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Um die ipytest Bibliothek zu installieren muss folgender Befehl in der Singularity Shell ausgeführt werden:<br>\n",
    "`pip install ipytest`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipytest\n",
    "import pytest\n",
    "import sys\n",
    "ipytest.autoconfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Durch `ipytest.autoconfig()` werden verschiedene Konfigurationen durchgeführt, die für das Verwenden von dem Framework notwendig sind. Natürlich können diese auch manuell gesetzt werden, oder es können Werte angepasst werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definieren von Tests\n",
    "Pytest erkennt Funktionen, welche mit dem Keyword \"test\" beginnen als Tests.<br>\n",
    "Über den Befehl `assert` können Ergebnisse von Funktionen mit den erwarteten Ergebnissen verglichen werden. Stimmt die definierte Bedingung, so gilt der Test als Bestanden.<br>\n",
    "Mit dem Ausführen der Zelle wird dank der ipytest Bibliothek auch der Test ausgeführt und das Ergebnis wird unterhalb der Zelle angezeigt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ipytest\n",
    "def increment(x):\n",
    "    return x + 1\n",
    "\n",
    "def test_increment():\n",
    "    assert increment(3) == 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funktionen, welche nicht mit \"test\" beginnen werden auch nicht als solche erkannt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ipytest\n",
    "def increment(x):\n",
    "    return x + 1\n",
    "\n",
    "def increment_test():\n",
    "    assert increment(3) == 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definieren von mehreren Tests\n",
    "Analog zum vorherigen Beispiel können beliebig viele Tests in einer Zelle/Klasse definiert werden. Mit dem Ausführen der Zelle werden alle Tests nacheinander abgearbeitet und als gemeinsames Ergebnis am Ende dargestellt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ipytest\n",
    "def decrease(x):\n",
    "    return x - 1\n",
    "\n",
    "def test_decrease_1():\n",
    "    assert decrease(3) == 2\n",
    "\n",
    "def test_decrease_2():\n",
    "    assert decrease(3) < 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mehrere `assert` Befehle in einer Testfunktion sind möglich. Diese zählen dann jedoch zusammen als ein einzelner Test. Das heißt, wenn ein `assert` fehlschlägt, dann betrifft das den gesamten Test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ipytest\n",
    "def multiply(x, y):\n",
    "    return x * y\n",
    "\n",
    "def test_multiply():\n",
    "    assert multiply(2, 4) == 8\n",
    "    assert multiply(2, 4) > (2 + 4)\n",
    "    assert multiply(2, 4) < 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prüfen auf Exceptions\n",
    "Es kann unter Umständen sinnvoll sein zu überprüfen ob gewisse Exceptions an gewollten Stellen geworfen werden.<br>\n",
    "Dafür stellt Pytest den `with` Befehl zur Verfügung. Mit `with pytest.raises(ExceptionXYZ...)` kann überprüft werden ob diese Spezielle Exception geworfen wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ipytest\n",
    "def divide(x, y):\n",
    "    return x / y\n",
    "\n",
    "def test_divide():\n",
    "    with pytest.raises(ZeroDivisionError):\n",
    "        divide(1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Falls mehrere Exceptions über bestimmte Keywords zusammengefasst werden können, kann über einen Regex-Ausdruck geprüft werden, ob dieser in der Exception auftritt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ipytest\n",
    "def func(x):\n",
    "    if x == 1:\n",
    "        raise ValueError(\"Exception 123 1\")\n",
    "    elif x == 2:\n",
    "        raise ValueError(\"Exception 123 2\")\n",
    "        \n",
    "def test_func1():\n",
    "    with pytest.raises(ValueError, match=r\".* 123 .*\"):\n",
    "        func(1)\n",
    "        \n",
    "def test_func2():\n",
    "    with pytest.raises(ValueError, match=r\".* 123 .*\"):\n",
    "        func(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytest Marker\n",
    "Pytest bietet verschiedene Möglichkeiten an Tests vorweg zu markieren, um ihnen ein entsprechendes Verhalten zuzuweisen.<br>\n",
    "`@pytest.mark.skip` überspringt den Test. Dieser wird nicht ausgeführt und somit ist auch das potentielle Ergebnis irrelevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ipytest\n",
    "\n",
    "@pytest.mark.skip(reason='this is broken')\n",
    "def test_skip():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Über `@pytest.mark.skipif(...)` kann eine Bedingung angegeben werden unter welcher der Test übersprungen werden soll."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ipytest\n",
    "\n",
    "@pytest.mark.skipif(sys.platform == 'linux', reason='Windows behaviour')\n",
    "def test_skip_conditionally():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`@pytest.mark.xfail` gibt an, dass erwartet wird, dass der Test fehlschlägt. (Expected Failure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ipytest\n",
    "\n",
    "@pytest.mark.xfail\n",
    "def test_expected_failure():\n",
    "    assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sollte ein Test, welcher als `xfail` markiert ist, dennoch auf mysteriöse weise erfolgreich sein, wird dieser als `xpass` gewertet. (Unexpectedly Passed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ipytest\n",
    "\n",
    "@pytest.mark.xfail\n",
    "def test_expected_failure_but_passed_mysteriously():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracebacks\n",
    "Über folgende Parameteter kann der Traceback im Output konfiguriert werden:\n",
    "```\n",
    "--tb=long\n",
    "--tb=short\n",
    "--tb=line\n",
    "--tb=native\n",
    "--tb=no\n",
    "``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ipytest --tb=long\n",
    "def func(x):\n",
    "    return x + x\n",
    "\n",
    "def test_func1():\n",
    "    assert func(1) == 1\n",
    "    \n",
    "def test_func2():\n",
    "    assert func(2) == 2\n",
    "    \n",
    "def test_func3():\n",
    "    assert func(3) == 3\n",
    "    \n",
    "def test_func4():\n",
    "    assert func(4) == 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximale Fails\n",
    "Über `--maxfail=x` kann angegeben werden wie viele Tests fehlschlagen dürfen bevor die Tests abgebrochen werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ipytest --maxfail=2\n",
    "def func(x):\n",
    "    return x + x\n",
    "\n",
    "def test_func1():\n",
    "    assert func(1) == 1\n",
    "    \n",
    "def test_func2():\n",
    "    assert func(2) == 2\n",
    "    \n",
    "def test_func3():\n",
    "    assert func(3) == 3\n",
    "    \n",
    "def test_func4():\n",
    "    assert func(4) == 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mögliche Testergebnisse\n",
    "Hier werden alle möglichen Testergebnisse noch einmal zusammengefasst dargestellt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ipytest -ra\n",
    "\n",
    "@pytest.fixture\n",
    "def error_fixture():\n",
    "    assert 0\n",
    "\n",
    "\n",
    "def test_ok():\n",
    "    print(\"ok\")\n",
    "\n",
    "\n",
    "def test_fail():\n",
    "    assert 0\n",
    "\n",
    "\n",
    "def test_error(error_fixture):\n",
    "    pass\n",
    "\n",
    "\n",
    "def test_skip():\n",
    "    pytest.skip(\"skipping this test\")\n",
    "\n",
    "\n",
    "def test_xfail():\n",
    "    pytest.xfail(\"xfailing this test\")\n",
    "\n",
    "\n",
    "@pytest.mark.xfail(reason=\"always xfail\")\n",
    "def test_xpass():\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

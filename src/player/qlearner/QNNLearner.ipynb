{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QNN (Q-Wertbasiertes Neuronales Netzwerk)\n",
    "\n",
    "Für die Erweiterbarkeit unseres Codes ist uns aufgefallen, dass der Q-Algorithmus seine Grenzen bei Spielen mit einem beinahe unendlichen Zustandsraum aufweist. Spiele wie Schach oder Go haben einen derart riesigen Zustandsraum aller möglichen Züge, der ohne weitere Hilfe nicht einfach durch ausprobieren komplett erkundet werden kann. Um dem Prinzip des bestärkenden Lernens nahe zu kommen, müssen andere Methoden gefunden werden, um zukünftige Züge oder sogar Strategien bei unendlich wirkenden Zustandsräumen hervorzusagen. Hierfür soll ein neuronales Netzwerk mit mehreren Schichten zum Einsatz kommen.\n",
    "\n",
    "Der Gedanke dahinter ist, dass man nicht mehr versucht alle Zustände zu erkunden und perfekt vorherzusagen, sondern eine Struktur oder sogar Strategie in gewissen Zügen zu erkennen. Der Fokus des neuronalen Netzes soll damit sein, Strukturen in zeitlich aufeinanderfolgender Züge zu erkennen und bei unbekannten Zuständen einen Schätzwert auf Basis bisher bekannter Strategien ausgeben.\n",
    "\n",
    "Ein neuronales Netzwerk besteht zumeist aus Eingabevektoren, verschiedenste versteckte Schichten sowie Ausgabevektoren. Genauso wie die Q-Funktion, sollen dem QNN gewisse Parameter als Eingabevektoren mitgegeben werden. Dazu gehört der aktuelle Zustand des Bretts, die ausgewählte Aktion auf Basis vorheriger Werte aus dem QNN Modell und ggf. die Belohnung. Als Ausgabevektor soll, genauso wie bei der Q-Funktion, ein Q-Wert sein, der die maximale Belohnung des aktuellen Zustand-Aktion-Paares beschreibt.\n",
    "\n",
    "Die Lerndaten erstellt der QNN Agent selbst, durch das explorative Erkunden mittels dem Explorationsfaktor namens \"Theta\". Anhand der explorierten Daten und erlangten Belohnungen wird das QNN selbst die besten Züge herausfinden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from build.Board import Board\n",
    "from build.player.qlearner.QLearner import QLearner\n",
    "\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class QNNLearner(QLearner):\n",
    "    \"\"\"\n",
    "    QLearner specification for neural network learning\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model = None, learn_rate=0.1, discount_factor=0.8, batch_size=10, learn_online=False):\n",
    "        \"\"\"\n",
    "        :param model: used qnn model structure\n",
    "        :param learn_rate: learning rate of this q learner\n",
    "        :param discount_factor: discount factor of this q learner\n",
    "        \"\"\"\n",
    "        self.learn_rate = learn_rate\n",
    "        self.model = model\n",
    "        self.discount_factor = discount_factor\n",
    "        self.batch_size = batch_size\n",
    "        self.learn_online = learn_online\n",
    "        self.model.online = True if learn_online else False\n",
    "\n",
    "    def update(self, prev_state, state, prev_move, reward):\n",
    "        \"\"\"\n",
    "        Update q players knowledge by learning offline or online\n",
    "        :param prev_state: previous known state\n",
    "        :param state: new state\n",
    "        :param prev_move: previous made move\n",
    "        :param reward: previous reward\n",
    "        \"\"\"\n",
    "        if self.learn_online:\n",
    "            # Online training\n",
    "            self.model.train_model_online(prev_state, prev_move, state, reward)\n",
    "        else:\n",
    "            # Offline training\n",
    "            self.model.train_model_offline(prev_state, prev_move, state, reward)\n",
    "\n",
    "\n",
    "    def select_move(self, state, theta=0.1) -> int:\n",
    "        \"\"\"\n",
    "        Select the best move or, if exploring, a random move\n",
    "        :param state: current state\n",
    "        :param theta: temperature value (optional)\n",
    "        :return: chosen action\n",
    "        \"\"\"\n",
    "        p = random.uniform(0, 1)\n",
    "\n",
    "        if p > theta:\n",
    "            action = self.model.predict_model(state, self.learn_online)\n",
    "        else:\n",
    "            action = random.choice(list(Board.POSSIBLE_ACTIONS))\n",
    "\n",
    "        return action # return choosen move\n",
    "\n",
    "    def is_known_state(self, state) -> bool:\n",
    "        \"\"\"\n",
    "        Does nothing because it doesn't make sense for a qnn learner. QNN should not store previous states,\n",
    "        instead the network learns patterns of states and corresponding actions.\n",
    "        \"\"\"\n",
    "        raise RuntimeError('State reduction is not applicable for qnn!')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "von Kevin Bücher"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
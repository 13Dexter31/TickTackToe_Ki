{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 QNN (Q-Wertbasiertes Neuronales Netzwerk)\n",
    "\n",
    "Für die Erweiterbarkeit unseres Codes ist uns aufgefallen, dass der Q-Algorithmus seine Grenzen bei Spielen mit einem beinahe unendlichen Zustandsraum aufweist. Spiele wie Schach oder Go haben einen derart riesigen Zustandsraum aller möglichen Züge, der ohne weitere Hilfe nicht einfach durch ausprobieren komplett erkundet werden kann. Um dem Prinzip des bestärkenden Lernens nahe zu kommen, müssen andere Methoden gefunden werden, um zukünftige Züge oder sogar Strategien bei unendlich wirkenden Zustandsräumen hervorzusagen. Hierfür soll ein neuronales Netzwerk mit mehreren Schichten zum Einsatz kommen.\n",
    "\n",
    "Der Gedanke dahinter ist, dass man nicht mehr versucht alle Zustände zu erkunden und perfekt vorherzusagen, sondern eine Struktur oder sogar Strategie in gewissen Zügen zu erkennen. Der Fokus des neuronalen Netzes soll damit sein, Strukturen in zeitlich aufeinanderfolgender Züge zu erkennen und bei unbekannten Zuständen einen Schätzwert auf Basis bisher bekannter Strategien ausgeben.\n",
    "\n",
    "Ein neuronales Netzwerk besteht zumeist aus Eingabevektoren, verschiedenste versteckte Schichten sowie Ausgabevektoren. Genauso wie die Q-Funktion, sollen dem QNN gewisse Parameter als Eingabevektoren mitgegeben werden. Dazu gehört der aktuelle Zustand des Bretts, die ausgewählte Aktion auf Basis vorheriger Werte aus dem QNN Modell und ggf. die Belohnung. Als Ausgabevektor soll, genauso wie bei der Q-Funktion, ein Q-Wert sein, der die maximale Belohnung des aktuellen Zustand-Aktion-Paares beschreibt.\n",
    "\n",
    "Die Lerndaten erstellt der QNN Agent selbst, durch das explorative Erkunden mittels dem Explorationsfaktor namens \"Theta\". Anhand der explorierten Daten und erlangten Belohnungen wird das QNN selbst die besten Züge herausfinden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from abc import abstractmethod\n",
    "from pathlib import Path\n",
    "from build.Board import Board\n",
    "from build.player.QPlayer import QPlayer\n",
    "from build.player.qlearner.QLearner import QLearner\n",
    "\n",
    "import numpy as np\n",
    "import keras.models as Km\n",
    "import keras.layers as kl\n",
    "\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class QNNLearner(QLearner):\n",
    "    \"\"\"\n",
    "    QLearner specification for neural network learning\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model = None, learn_rate=0.1, discount_factor=0.8, batch_size=10):\n",
    "        \"\"\"\n",
    "        :param model: used qnn model structure\n",
    "        :param learn_rate: learning rate of this q learner\n",
    "        :param discount_factor: discount factor of this q learner\n",
    "        \"\"\"\n",
    "        self.learn_rate = learn_rate\n",
    "        self.model = model\n",
    "        self.discount_factor = discount_factor\n",
    "        self.memory = []\n",
    "        self.count_memory = 0\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def update(self, prev_state, state, prev_move, reward):\n",
    "        \"\"\"\n",
    "        Update q players knowledge by learning offline or online\n",
    "        :param prev_state: previous known state\n",
    "        :param state: new state\n",
    "        :param prev_move: previous made move\n",
    "        :param reward: previous reward\n",
    "        \"\"\"\n",
    "        self.__load_to_memory(prev_state, prev_move, state, reward)\n",
    "\n",
    "        self.count_memory += 1\n",
    "\n",
    "        #print(self.count_memory)\n",
    "        if self.count_memory == self.batch_size:\n",
    "            self.count_memory = 0\n",
    "            # Offline training\n",
    "            self.model.learn_batch(self.memory)\n",
    "            # Online training\n",
    "            #self.learn(self.prev_state, self.prev_move, state,  -1, self.reward)\n",
    "            self.memory = []\n",
    "\n",
    "    def select_move(self, state, theta=0.1):\n",
    "        \"\"\"\n",
    "        Select the best move or, if exploring, a random move\n",
    "        :param state: current state\n",
    "        :param theta: temperature value (optional)\n",
    "        :return: chosen action\n",
    "        \"\"\"\n",
    "        p = random.uniform(0, 1)\n",
    "\n",
    "        if p > theta:\n",
    "            action = self.__choose_optimal_move(state)\n",
    "            #action = self.possible_actions[idx]\n",
    "        else:\n",
    "            action = random.choice(Board.POSSIBLE_ACTIONS)\n",
    "\n",
    "        return action # return choosen move\n",
    "\n",
    "    def is_known_state(self, state) -> bool:\n",
    "        pass\n",
    "\n",
    "    def __load_to_memory(self, prev_state, prev_move, state, reward):\n",
    "        \"\"\"\n",
    "        Load all q related things into memory to learn in batch\n",
    "        :param prev_state: previous known state\n",
    "        :param prev_move: previous made move\n",
    "        :param state: new state\n",
    "        :param reward: previous reward\n",
    "        \"\"\"\n",
    "        self.memory.append([prev_state, prev_move, state, reward])\n",
    "\n",
    "    def __choose_optimal_move(self, state):\n",
    "        \"\"\"\n",
    "        Choose optimal move based on the calculated and best predicted values of current state.\n",
    "        :param state: current state\n",
    "        :return: best move with highest value (randomly select for equal values)\n",
    "        \"\"\"\n",
    "        v = -float('Inf') # most negative value (negative infinity float)\n",
    "        v_list = [] # list of all calculated values\n",
    "        idx = [] # move index for chosen move\n",
    "        for move in Board.POSSIBLE_ACTIONS:\n",
    "            value = self.model.calc_value(state, move)\n",
    "            v_list.append(round(float(value), 5))\n",
    "\n",
    "            if value > v:\n",
    "                v = value\n",
    "                idx = [move]\n",
    "            elif v == value:\n",
    "                idx.append(move)\n",
    "\n",
    "        idx = random.choice(idx)\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Model:\n",
    "    \"\"\"\n",
    "    Model class for all 2 player based games with neural network training\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tag):\n",
    "        \"\"\"\n",
    "        :param tag: used tag for neural network model (e.g. 1 for first player and -1 for second)\n",
    "        \"\"\"\n",
    "        self.tag = tag\n",
    "        self.epsilon = 0.1\n",
    "        self.alpha = 0.5\n",
    "        self.gamma = 1\n",
    "        self.model = self.load_model()\n",
    "\n",
    "    def load_model(self):\n",
    "        \"\"\"\n",
    "        Loads previously saved model\n",
    "        :return: loaded model\n",
    "        \"\"\"\n",
    "        if self.tag == 1:\n",
    "            tag = '_first'\n",
    "        else:\n",
    "            tag = '_second'\n",
    "\n",
    "        s = 'model_values' + tag + '.h5'\n",
    "        model_file = Path(s)\n",
    "\n",
    "        if model_file.is_file():\n",
    "            print('load model')\n",
    "            model = Km.load_model(s)\n",
    "            print('load model: ' + s)\n",
    "        else:\n",
    "            model = self.create_model()\n",
    "        return model\n",
    "\n",
    "    @abstractmethod\n",
    "    def create_model(self):\n",
    "        \"\"\"\n",
    "        Create new model with appropriate number of layers and network structure\n",
    "        :return: created model\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def state_to_tensor(self, state, move):\n",
    "        \"\"\"\n",
    "        Creates a tensor (2 dim array) based on a state and a move as input vector for nn\n",
    "        :param state: current state\n",
    "        :param move: current move\n",
    "        :return: created tensor\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def calc_value(self, state, move):\n",
    "        \"\"\"\n",
    "        Calculate a tensor and predict the reward\n",
    "        :param state: current state\n",
    "        :param move: current move\n",
    "        :return: most predicted value (predicted reward)\n",
    "        \"\"\"\n",
    "        tensor = self.state_to_tensor(state, move)\n",
    "        value = self.model.predict(tensor)\n",
    "        return value\n",
    "\n",
    "    def calc_target(self, prev_state, prev_move, state, reward):\n",
    "        \"\"\"\n",
    "        Calculate the target vector (q value or reward)\n",
    "        :param prev_state: previous state\n",
    "        :param prev_move: previous move\n",
    "        :param state: current state\n",
    "        :param reward: previous reward\n",
    "        :return: calculated target value\n",
    "        \"\"\"\n",
    "\n",
    "        qvalue = self.calc_value(prev_state, prev_move)\n",
    "        v = []\n",
    "        tensor = self.state_to_tensor(prev_state, prev_move)\n",
    "\n",
    "        for move in range(len(tensor[:,0][0])):\n",
    "            v.append(self.calc_value(state, move))\n",
    "\n",
    "        if reward == 0:\n",
    "            v_s_tag = self.gamma * np.max(v)\n",
    "            target = np.array(qvalue + self.alpha * (reward + v_s_tag - qvalue))\n",
    "        else:\n",
    "            target = reward\n",
    "\n",
    "        return target\n",
    "\n",
    "    def train_model(self, prev_state, prev_move, target, epochs):\n",
    "        \"\"\"\n",
    "        Train the model based on an input tensor\n",
    "        :param prev_state: previous state\n",
    "        :param prev_move: previous move\n",
    "        :param target: calculated q value or reward (target vector)\n",
    "        :param epochs: number of epochs\n",
    "        \"\"\"\n",
    "\n",
    "        tensor = self.state_to_tensor(prev_state, prev_move)\n",
    "\n",
    "        if target is not None:\n",
    "\n",
    "            if self.tag == 1:\n",
    "                print('value before training:', self.model.predict(tensor))\n",
    "            self.model.fit(tensor, target, epochs=epochs, verbose=0)\n",
    "            # K.backend.clear_session()\n",
    "\n",
    "            if self.tag == 1:\n",
    "                print('target:', target)\n",
    "                print('value after training:', self.model.predict(tensor))\n",
    "\n",
    "    def save_model(self):\n",
    "        \"\"\"\n",
    "        save model as h5 file\n",
    "        \"\"\"\n",
    "        if self.tag == 1:\n",
    "            tag = '_first'\n",
    "        else:\n",
    "            tag = '_second'\n",
    "        s = 'model_values' + tag + '.h5'\n",
    "\n",
    "        try:\n",
    "            os.remove(s)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        self.model.save(s)\n",
    "\n",
    "    def learn_batch(self, memory):\n",
    "        \"\"\"\n",
    "        Learn model with a batch of states and actions from memory\n",
    "        :param memory: saved states, actions and rewards\n",
    "        \"\"\"\n",
    "        print('start learning player', self.tag)\n",
    "        print('data length:', len(memory))\n",
    "\n",
    "        # build x_train\n",
    "        ind = 0\n",
    "        x_train = np.zeros((len(memory), 2, 9))\n",
    "        for v in memory:\n",
    "            [prev_state, prev_move, _, _] = v\n",
    "            sample = self.state_to_tensor(prev_state, prev_move)\n",
    "            x_train[ind, :, :] = sample\n",
    "            ind += 1\n",
    "\n",
    "        # train with planning\n",
    "        loss = 20\n",
    "        count = 0\n",
    "        while loss > 0.02 and count < 10:\n",
    "            y_train = self.create_targets(memory)\n",
    "            self.model.fit(x_train, y_train, epochs=5, batch_size=256, verbose=0)\n",
    "            loss = self.model.evaluate(x_train, y_train, batch_size=256, verbose=0)[0]\n",
    "            count += 1\n",
    "            print('planning number:', count, 'loss', loss)\n",
    "\n",
    "        loss = self.model.evaluate(x_train, y_train, batch_size=256, verbose=0)\n",
    "        print('player:', self.tag, loss, 'loops', count)\n",
    "\n",
    "        self.save_model()\n",
    "\n",
    "    def create_targets(self, memory):\n",
    "        \"\"\"\n",
    "        Create target vector for each state-action-pair in memory\n",
    "        :param memory: saved states, actions and rewards\n",
    "        :return: target vector\n",
    "        \"\"\"\n",
    "        y_train_ = np.zeros((len(memory), 1))\n",
    "        count_ = 0\n",
    "        for v_ in memory:\n",
    "            [prev_state_, prev_move_, state_, reward_] = v_\n",
    "            target = self.calc_target(prev_state_, prev_move_, state_, reward_)\n",
    "            y_train_[count_, :] = target\n",
    "            count_ += 1\n",
    "\n",
    "        return y_train_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class TicTacToeModel(Model):\n",
    "    \"\"\"\n",
    "    Special model for tic tac toe games.\n",
    "\n",
    "    Consists of 2x9 input vector, dense network of 9 layers and a 9 sized target vector.\n",
    "    Input vector consists an array with length 9 for the chosen move and an array for the state.\n",
    "    Target vector consists of 9 sized array for 9 possible rewards (one for each action).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tag):\n",
    "        super().__init__(tag)\n",
    "        pass\n",
    "\n",
    "    def create_model(self):\n",
    "        \"\"\"\n",
    "        Creates keras model\n",
    "        :return: keras model\n",
    "        \"\"\"\n",
    "        print('new model')\n",
    "\n",
    "        model = Km.Sequential()\n",
    "        model.add(kl.Flatten(input_shape=(2, 9)))\n",
    "        model.add(kl.Dense(18))\n",
    "        model.add(kl.LeakyReLU(alpha=0.3))\n",
    "        model.add(kl.Dense(18))\n",
    "        model.add(kl.LeakyReLU(alpha=0.3))\n",
    "        model.add(kl.Dense(18))\n",
    "        model.add(kl.LeakyReLU(alpha=0.3))\n",
    "        model.add(kl.Dense(18))\n",
    "        model.add(kl.LeakyReLU(alpha=0.3))\n",
    "        model.add(kl.Dense(18))\n",
    "        model.add(kl.LeakyReLU(alpha=0.3))\n",
    "        model.add(kl.Dense(18))\n",
    "        model.add(kl.LeakyReLU(alpha=0.3))\n",
    "        model.add(kl.Dense(18))\n",
    "        model.add(kl.LeakyReLU(alpha=0.3))\n",
    "        model.add(kl.Dense(9))\n",
    "        model.add(kl.LeakyReLU(alpha=0.3))\n",
    "        model.add(kl.Dense(1, activation='linear'))\n",
    "\n",
    "        model.compile(optimizer='Adam', loss='mean_absolute_error', metrics=['accuracy'])\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        return model\n",
    "\n",
    "    def state_to_tensor(self, state, move):\n",
    "        \"\"\"\n",
    "        Generates a tensor of state and move index\n",
    "        :param state: current state\n",
    "        :param move: current move\n",
    "        :return: tensor (2 dim array)\n",
    "        \"\"\"\n",
    "\n",
    "        state = np.array(state)\n",
    "        state = state.flatten() # flatten 3x3 matrix because of 1 length input vector for state\n",
    "        state = self.__one_hot_encode_state(state)\n",
    "\n",
    "        a = np.zeros(9).astype('float32')\n",
    "        a[move] = 1 # one hot encoding for chosen action (1 for the chosen action an 0 for none)\n",
    "\n",
    "        state = np.asarray(state).astype('float32')\n",
    "        tensor = np.array((a, state))\n",
    "        tensor = tensor.reshape((1, 2, 9))\n",
    "\n",
    "        return tensor\n",
    "\n",
    "    def __one_hot_encode_state(self, state):\n",
    "        \"\"\"\n",
    "        One hot encoding for the state.\n",
    "        Each field input of 3x3 matrix will be displayed with 0 (blank), 1 (player 1), -1 (player 2)\n",
    "        :param state: state to encode\n",
    "        :return: encoded state\n",
    "        \"\"\"\n",
    "        for i in range(len(state)):\n",
    "            if state[i] is None:\n",
    "                state[i] = 0\n",
    "            if state[i] == 'x':\n",
    "                state[i] = 1\n",
    "            if state[i] == 'o':\n",
    "                state[i] = -1\n",
    "\n",
    "        return state"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Klassen für das Erstellen von Modellen eines Brettspiels\n",
    "\n",
    "Es gibt eine generalisierte Klasse *Model*, die für alle 1 gegen 1 Brettspiele mit wertebasiertem bestärkendem Lernmodell dient. Das Lernen wird mittels neuronalen Netzen und nach der Logik des Q-Algorithmus umgesetzt.\n",
    "\n",
    "Es werden 2 wesentliche Lernvarianten unterstützt:\n",
    "\n",
    "* Online Training\n",
    "* Offline Training\n",
    "\n",
    "Unter *Online* wird das direkte Lernen pro Zustand verstanden. Nach jedem Zug wird der aktuelle Wert antrainiert und im Model gespeichert.\n",
    "Unter *Offline* wird das Lernen von einer vorgegebenen Größe an gespeicherten Zuständen verstanden. Dabei wird erst eine Anzahl an Zügen gespielt. Danach wird das Spiel gestoppt und das Modell wird mit den eben gespielten Zuständen angelernt."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from abc import abstractmethod\n",
    "from pathlib import Path\n",
    "import os\n",
    "import numpy as np\n",
    "import keras.models as Km\n",
    "import keras.layers as kl\n",
    "import random\n",
    "\n",
    "from build.Board import Board"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Model:\n",
    "    \"\"\"\n",
    "    Model class for all 2 player based games with neural network training\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tag, online=False):\n",
    "        \"\"\"\n",
    "        :param tag: used tag for neural network model (e.g. 1 for first player and -1 for second)\n",
    "        \"\"\"\n",
    "        self.tag = tag\n",
    "        self.epsilon = 0.1\n",
    "        self.alpha = 0.5\n",
    "        self.gamma = 1\n",
    "        self.model = self.__load_model()\n",
    "        self.history = []\n",
    "        self.memory = []\n",
    "        self.count_memory = 0\n",
    "        self.batch_size = 10\n",
    "        self.online = online\n",
    "\n",
    "    def __load_model(self):\n",
    "        \"\"\"\n",
    "        Loads previously saved model\n",
    "        :return: loaded model\n",
    "        \"\"\"\n",
    "        tag = '_first' if self.tag == 1 else '_second'\n",
    "\n",
    "        if self.online:\n",
    "            tag += '_online'\n",
    "\n",
    "        s = 'model_values' + tag + '.h5'\n",
    "        model_file = Path(s)\n",
    "\n",
    "        if model_file.is_file():\n",
    "            print('load model')\n",
    "            model = Km.load_model(s)\n",
    "            print('load model: ' + s)\n",
    "        else:\n",
    "            model = self.create_model()\n",
    "        return model\n",
    "\n",
    "    @abstractmethod\n",
    "    def create_model(self):\n",
    "        \"\"\"\n",
    "        Create new model with appropriate number of layers and network structure\n",
    "        :return: created model\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def state_to_tensor(self, state, move):\n",
    "        \"\"\"\n",
    "        Creates a tensor (2 dim array) based on a state and a move as input vector for nn\n",
    "        :param state: current state\n",
    "        :param move: current move\n",
    "        :return: created tensor\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def one_hot_encode_state(self, state):\n",
    "        \"\"\"\n",
    "        One hot encoding for the state.\n",
    "        Each field input of 3x3 matrix will be displayed with 0 (blank), 1 (player 1), -1 (player 2)\n",
    "        :param state: state to encode\n",
    "        :return: encoded state\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def __choose_optimal_move(self, state) -> int:\n",
    "        \"\"\"\n",
    "        Choose optimal move based on the calculated and best predicted values of current state.\n",
    "        :param state: current state\n",
    "        :return: best move with highest value (randomly select for equal values)\n",
    "        \"\"\"\n",
    "        v = -float('Inf') # most negative value (negative infinity float)\n",
    "        v_list = [] # list of all calculated values\n",
    "        idx = [] # move index for chosen move\n",
    "        for move in Board.POSSIBLE_ACTIONS:\n",
    "            value = self.__calc_value(state, move)\n",
    "            v_list.append(round(float(value), 5))\n",
    "\n",
    "            if value > v:\n",
    "                v = value\n",
    "                idx = [move]\n",
    "            elif v == value:\n",
    "                idx.append(move)\n",
    "\n",
    "        idx = random.choice(idx)\n",
    "        return idx\n",
    "\n",
    "    def __calc_value(self, state, move) -> float:\n",
    "        \"\"\"\n",
    "        Calculate a tensor and predict the reward\n",
    "        :param state: current state\n",
    "        :param move: current move\n",
    "        :return: most predicted value (predicted reward)\n",
    "        \"\"\"\n",
    "\n",
    "        tensor = self.state_to_tensor(state, move)\n",
    "        value = self.model.predict(tensor)\n",
    "        return value\n",
    "\n",
    "    def __calc_target(self, prev_state, prev_move, state, reward):\n",
    "        \"\"\"\n",
    "        Calculate the target vector (q value or reward)\n",
    "        :param prev_state: previous state\n",
    "        :param prev_move: previous move\n",
    "        :param state: current state\n",
    "        :param reward: previous reward\n",
    "        :return: calculated target value\n",
    "        \"\"\"\n",
    "        qvalue = self.__calc_value(prev_state, prev_move)\n",
    "        v = []\n",
    "        tensor = self.state_to_tensor(prev_state, prev_move)\n",
    "\n",
    "        for move in range(len(tensor[:,0][0])):\n",
    "            v.append(self.__calc_value(state, move))\n",
    "\n",
    "        if reward == 0:\n",
    "            v_s_tag = self.gamma * np.max(v)\n",
    "            target = np.array(qvalue + self.alpha * (reward + v_s_tag - qvalue))\n",
    "        else:\n",
    "            target = reward\n",
    "\n",
    "        return target\n",
    "\n",
    "    def __save_model(self):\n",
    "        \"\"\"\n",
    "        save model as h5 file\n",
    "        \"\"\"\n",
    "        tag = '_first' if self.tag == 1 else '_second'\n",
    "\n",
    "        if self.online:\n",
    "            tag += '_online'\n",
    "\n",
    "        s = 'model_values' + tag + '.h5'\n",
    "\n",
    "        try:\n",
    "            os.remove(s)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        self.model.save(s)\n",
    "\n",
    "    def predict_model(self, state, online: bool):\n",
    "        if online:\n",
    "            state = self.one_hot_encode_state(state)\n",
    "            target = int(np.argmax(self.model.predict(state)))\n",
    "        else:\n",
    "            target = self.__choose_optimal_move(state)\n",
    "\n",
    "        return target\n",
    "\n",
    "    def __learn_batch(self, memory):\n",
    "        \"\"\"\n",
    "        Learn model with a batch of states and actions from memory\n",
    "        :param memory: saved states, actions and rewards\n",
    "        \"\"\"\n",
    "        print('start learning player', self.tag)\n",
    "        print('data length:', len(memory))\n",
    "\n",
    "        # build x_train\n",
    "        ind = 0\n",
    "        x_train = np.zeros((len(memory), 2, 9))\n",
    "        for v in memory:\n",
    "            [prev_state, prev_move, _, _] = v\n",
    "            sample = self.state_to_tensor(prev_state, prev_move)\n",
    "            x_train[ind, :, :] = sample\n",
    "            ind += 1\n",
    "\n",
    "        # train with planning\n",
    "        loss = 20\n",
    "        count = 0\n",
    "        while loss > 0.02 and count < 10:\n",
    "            y_train = self.__create_targets(memory)\n",
    "            history = self.model.fit(x_train, y_train, epochs=5, batch_size=256, verbose=0)\n",
    "            self.history.append(history.history)\n",
    "            loss = self.model.evaluate(x_train, y_train, batch_size=256, verbose=0)[0]\n",
    "            count += 1\n",
    "            print('planning number:', count, 'loss', loss)\n",
    "\n",
    "        loss = self.model.evaluate(x_train, y_train, batch_size=256, verbose=0)\n",
    "        print('player:', self.tag, loss, 'loops', count)\n",
    "\n",
    "        self.__save_model()\n",
    "\n",
    "    def train_model_offline(self, prev_state, prev_move, state, reward):\n",
    "\n",
    "        self.__load_to_memory(prev_state, prev_move, state, reward)\n",
    "        self.count_memory += 1\n",
    "\n",
    "        if self.count_memory == self.batch_size:\n",
    "            self.count_memory = 0\n",
    "            self.__learn_batch(self.memory)\n",
    "            self.memory = []\n",
    "\n",
    "    def __load_to_memory(self, prev_state, prev_move, state, reward):\n",
    "        \"\"\"\n",
    "        Load all q related things into memory to learn in batch\n",
    "        :param prev_state: previous known state\n",
    "        :param prev_move: previous made move\n",
    "        :param state: new state\n",
    "        :param reward: previous reward\n",
    "        \"\"\"\n",
    "        self.memory.append([prev_state, prev_move, state, reward])\n",
    "\n",
    "    def __create_targets(self, memory):\n",
    "        \"\"\"\n",
    "        Create target vector for each state-action-pair in memory\n",
    "        :param memory: saved states, actions and rewards\n",
    "        :return: target vector\n",
    "        \"\"\"\n",
    "        y_train_ = np.zeros((len(memory), 1))\n",
    "        count_ = 0\n",
    "        for v_ in memory:\n",
    "            [prev_state_, prev_move_, state_, reward_] = v_\n",
    "            target = self.__calc_target(prev_state_, prev_move_, state_, reward_)\n",
    "            y_train_[count_, :] = target\n",
    "            count_ += 1\n",
    "\n",
    "        return y_train_\n",
    "\n",
    "    def train_model_online(self, prev_state, prev_move,new_state, reward, discount_factor=0.8):\n",
    "        \"\"\"\n",
    "        Train the model based on the current state, action and received reward\n",
    "        :param prev_state: previous state\n",
    "        :param new_state: new state\n",
    "        :param prev_move: previous move\n",
    "        :param discount_factor: discount factor of this q learner\n",
    "        :param reward: received reward after prev action\n",
    "        \"\"\"\n",
    "        prev_state = self.one_hot_encode_state(prev_state)\n",
    "        new_state = self.one_hot_encode_state(new_state)\n",
    "\n",
    "        target = reward + discount_factor * np.max( self.model.predict(new_state))\n",
    "        target_vector = self.model.predict(prev_state)[0]\n",
    "        target_vector[prev_move] = target\n",
    "        history = self.model.fit(prev_state, target_vector.reshape(-1, len(Board.POSSIBLE_ACTIONS)), epochs=1, verbose=0)\n",
    "        if self.history is not None:\n",
    "            self.history = self.history.append(history.history)\n",
    "        else:\n",
    "            self.history = [history.history]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Spezialisiertes Modell für eine TicTacToe Spiel\n",
    "\n",
    "Hier wird eine spezialisierte Modellvariante für ein TicTacToe Spiel beschrieben.\n",
    "Dieses Modell besitzt eine vielzahl an tiefen Schichten, damit ein möglichst geringer Loss Wert erreicht werden kann.\n",
    "Die Schichten sind für den Observations und Aktionsraum eines TicTacToe Spiels angepasst (also jeweils die Größe 9)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](../../../docs/qnn_model.png)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Für die Umsetzung des neuronalen Netzes haben wir uns stark an dem Prinzip des Q-Algorithmus gehalten. Das Prinzip der temporalen Verbindung und der Aktion-Zustand-Paare sind mit in die Entscheidung über den Aufbau des Netzes eingeflossen.\n",
    "\n",
    "Die obere Abbildung zeigt das von uns ausgewählte Modell des Netzes. Es gibt genau zwei Eingangsvektoren, ein Vektor um den aktuellen Zustand abzubilden und einer für die ausgewählte Aktion. Die Eingangswerte werden zuvor noch vorverarbeitet, sodass z.B. der Zustand des 3x3 Bretts und die Aktion eindimensionell dargestellt werden. Die Werte werden zudem noch über ein One-Hot-Encoding für die Aktivierungsfunktionen der Neuronen vereinfacht. Im Fall des Spielbretts wird ein Spieler mit einer positiven 1 versehen und der Gegner mit einer -1. Leere Felder werden mit 0 initialisiert. Damit die ausgewählte Aktion genau einem Feld der neun verfügbaren zugeschrieben werden kann, wird die Aktion mittels One-Hot als 1 dargestellt und die nicht ausgewählten Felder mit einer 0 versehen. Der Eingangsvektor hat somit folgendes Format:\n",
    "\n",
    "1. Vektor (Zustand): [0, 0, 0, 1, 0, -1, 1, 0, -1]\n",
    "2. Vektor (Aktion): [1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "In diesem Beispiel sah das aktuelle Feld folgendermaßen aus:\n",
    "&ensp;\n",
    "\n",
    "|   |   |   |\n",
    "|---|---|---|\n",
    "| -  | - | - |\n",
    "| x  | - | o |\n",
    "| x  | - | o |\n",
    "\n",
    "Und nach der ausgewählten Aktion:\n",
    "&ensp;\n",
    "\n",
    "|   |   |   |\n",
    "|---|---|---|\n",
    "| x  | - | - |\n",
    "| x  | - | o |\n",
    "| x  | - | o |\n",
    "\n",
    "Letzendlich wird ein Array der Größe 2x9 als Eingangsvektor genutzt. Um diesen im Netz jedoch noch mehr zu vereinfachen wird in der ersten Schicht des Netzes der zweidimensionale Eingangsvektor in einen 1x18 Vektor transformiert. Damit erhält der obige Array folgendes Format:\n",
    "\n",
    "[0, 0, 0, 1, 0, -1, 1, 0, -1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "Danach werden 9 Dense Layers als Hidden Layer angefügt. Die Anzahl ergab sich aus der Feldgröße des Bretts. Diese werden schließlich mit der ReLU Aktivierungsfunkion (rectified linear unit) versehen (s. Funktion unten).\n",
    "\n",
    "$$f(x) = max(0,x)$$\n",
    "\n",
    "Die ReLU Aktivierungsfunktion nochmal als Graph dargestellt:\n",
    "\n",
    "![](../../../docs/relu.png)\n",
    "\n",
    "Die Entscheidung auf ReLU fiel deshalb, da wir während des Spielens die Züge aufzeichnen und ab einer gewissen Größe direkt an des Netz zum Lernen weitergeben. Damit wollten wir ein Online Lernen ermöglichen, indem das Netz die eben geführten Züge und erhaltenen Belohnungen direkt anlernt. Dabei entsteht eine sehr hohe Rechenlast, wenn neue Daten erneut gelernt werden müssen. Um diese Last zu vermeiden, haben wir uns dazu entschieden das Netz schmal und schnell zu machen. Gewichtungen werden somit schnell bestimmt und Ergebnisse aus einem Lernzyklus schnell ausgegeben. Der Nachteil dabei ist die geringe Genauigkeit. Aus diesem Grund muss der Lerndurchgang mehrere Male druchgelaufen werden, bis ein möglichst geringer Verlustwert erreicht wurde.\n",
    "\n",
    "Der Ausgabevektor ist schließlich nur noch ein einziger Wert, nämlich der eigentliche Q-Wert für das angegebene Aktion-Zustand-Paar. Da wir festgestellt haben, dass der Q-Algorithmus eine schnelle Gewinnstrategie mittels hoher Belohnungswerte lernt, dachten wir uns, dass das neuronale Netz verhersagen muss ob gutes oder eher schlechtes Feedback erwartet wird. Demnach wird beim fitten des Modells als Zielvektor für das Training die jeweilige Belohnung angegeben. Im folgendem Beispiel wird dies besser verdeutlicht:\n",
    "\n",
    "* Der Agent sieht einen aktuellen Zustand\n",
    "* Eine beste Aktion wird mittels Vorhersage aus dem verfügbaren Modell ausgesucht oder eine zufällige Aktion\n",
    "* Die Umgebung gibt dem Agenten Feedback (als Belohnung oder Bestrafung)\n",
    "* der letzte Zustand und die ausgewählte Aktion wird nun als Eingabevektor für das Training zusammengestellt\n",
    "* die erlangene Belohnung/Bestrafung wird als Zielvektor bestimmt\n",
    "* der Agent lernt nun, dass diese Aktion auf dem letzten Zustand zu einer Belohnung oder Bestrafung geführt hat\n",
    "\n",
    "Der Hintergedanke dabei ist, dass das neuronale Netz eine feste Bindung zwischen Aktion-Zustand-Paar und Feedback aufbaut, in der Hoffnung, dass gleichartige neue Zustände mit ähnlichen Feedback bestimmt werden können. Damit müssen in der Theorie nicht mehr alle Zustände erlernt werden, sondern nur noch jene die strukturelle Unterschiede vorweisen. Die restlichen können mittels Schätzungen möglichst genau beschrieben werden, ähnlich nach dem Prinzip der linearen Regression.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class TicTacToeModel(Model):\n",
    "    \"\"\"\n",
    "    Special model for tic tac toe games.\n",
    "\n",
    "    Consists of 2x9 input vector, dense network of 9 layers and a 1 sized target vector.\n",
    "    Input vector consists an array with length 9 for the chosen move and an array for the state.\n",
    "    Target vector consists of one value for the q value.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tag, online=False):\n",
    "        self.online = online\n",
    "        super().__init__(tag, self.online)\n",
    "        pass\n",
    "\n",
    "    def create_model(self):\n",
    "        \"\"\"\n",
    "        Creates keras model\n",
    "        :return: keras model\n",
    "        \"\"\"\n",
    "        print('new model')\n",
    "\n",
    "        model = Km.Sequential()\n",
    "        model.add(kl.Flatten(input_shape=(2, 9)))\n",
    "        model.add(kl.Dense(18))\n",
    "        model.add(kl.LeakyReLU(alpha=0.3))\n",
    "        model.add(kl.Dense(18))\n",
    "        model.add(kl.LeakyReLU(alpha=0.3))\n",
    "        model.add(kl.Dense(18))\n",
    "        model.add(kl.LeakyReLU(alpha=0.3))\n",
    "        model.add(kl.Dense(18))\n",
    "        model.add(kl.LeakyReLU(alpha=0.3))\n",
    "        model.add(kl.Dense(18))\n",
    "        model.add(kl.LeakyReLU(alpha=0.3))\n",
    "        model.add(kl.Dense(18))\n",
    "        model.add(kl.LeakyReLU(alpha=0.3))\n",
    "        model.add(kl.Dense(18))\n",
    "        model.add(kl.LeakyReLU(alpha=0.3))\n",
    "        model.add(kl.Dense(9))\n",
    "        model.add(kl.LeakyReLU(alpha=0.3))\n",
    "        model.add(kl.Dense(1, activation='linear'))\n",
    "\n",
    "        model.compile(optimizer='Adam', loss='mean_absolute_error', metrics=['accuracy'])\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        return model\n",
    "\n",
    "    def state_to_tensor(self, state, move):\n",
    "        \"\"\"\n",
    "        Generates a tensor of state and move index\n",
    "        :param state: current state\n",
    "        :param move: current move\n",
    "        :return: tensor (2 dim array)\n",
    "        \"\"\"\n",
    "        state = self.one_hot_encode_state(state)\n",
    "\n",
    "        a = np.zeros(9).astype('float32')\n",
    "        a[move] = 1 # one hot encoding for chosen action (1 for the chosen action an 0 for none)\n",
    "\n",
    "        state = np.asarray(state).astype('float32')\n",
    "        tensor = np.array((a, state))\n",
    "        tensor = tensor.reshape((1, 2, 9))\n",
    "\n",
    "        return tensor\n",
    "\n",
    "    def one_hot_encode_state(self, state):\n",
    "        \"\"\"\n",
    "        One hot encoding for the state.\n",
    "        Each field input of 3x3 matrix will be displayed with 0 (blank), 1 (player 1), -1 (player 2)\n",
    "        :param state: state to encode\n",
    "        :return: encoded state\n",
    "        \"\"\"\n",
    "        state = state.flatten() # flatten 3x3 matrix because of 1 length input vector for state\n",
    "\n",
    "        for i in range(len(state)):\n",
    "            if state[i] is None:\n",
    "                state[i] = 0\n",
    "            if state[i] == 'x':\n",
    "                state[i] = 1\n",
    "            if state[i] == 'o':\n",
    "                state[i] = -1\n",
    "\n",
    "        state = np.asarray(state).astype('float32')\n",
    "        state = state.reshape((1, 9))\n",
    "\n",
    "        return state"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Schmale Modellvariante eines TicTacToe Spiels\n",
    "\n",
    "Da wir auch ein online Lernverfahren probieren wollten und das letzte Modell dafür zu vielschichtig war, musste eine schmale Variante entwickelt werden. Diese Variante baut nur noch auf eine tiefe Schicht auf.\n",
    "\n",
    "Als Eingangsvektor wird diesmal nur der eindimensionale Zustandsraum (Länge 9) übermittelt. Das Ziel des kurzfristigen Lernens soll dieses Mal direkt die Ausgabe von 9 möglichen Q-Werten für jede mögliche Aktion von diesem Zustand aus sein. Damit wird die Anzahl an Lerndurchgängen deutlich verringert, da pro Zustand direkt der Trainingsvorgang gestartet wird.\n",
    "\n",
    "Auch in diesem Modell wird die ReLU Aktivierungsfunktion einegsetzt, sowie der Adam Optimierungsalgorithmus.\n",
    "\n",
    "Das entstandene Modell sieht demnach wie folgt aus:\n",
    "\n",
    "![](../../../docs/qnn_small_model.png)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class TicTacToeModelSmall(TicTacToeModel):\n",
    "    \"\"\"\n",
    "    Special model for tic tac toe games.\n",
    "\n",
    "    Consists of 1x9 input vector, dense network of 1 layer and a 9 sized target vector.\n",
    "    Input vector consists an array with length 9 for the state.\n",
    "    Target vector consists of 9 sized array for 9 possible rewards (one for each action).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tag, observation_space=9, action_space=9, online=False):\n",
    "        self.observation_space = observation_space\n",
    "        self.action_space = action_space\n",
    "        self.online = online\n",
    "        super().__init__(tag, self.online)\n",
    "        pass\n",
    "\n",
    "    def create_model(self):\n",
    "        \"\"\"\n",
    "        Creates keras model\n",
    "        :return: keras model\n",
    "        \"\"\"\n",
    "        print('new model')\n",
    "\n",
    "        model = Km.Sequential()\n",
    "        model.add(kl.InputLayer(batch_input_shape=(1, self.observation_space)))\n",
    "        model.add(kl.Dense(20, activation='relu'))\n",
    "        model.add(kl.Dense(self.action_space, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer='adam', metrics=['mae'])\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
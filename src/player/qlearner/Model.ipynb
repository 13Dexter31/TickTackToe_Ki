{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from abc import abstractmethod\n",
    "from pathlib import Path\n",
    "import os\n",
    "import numpy as np\n",
    "import keras.models as Km\n",
    "import keras.layers as kl\n",
    "import random\n",
    "\n",
    "from build.Board import Board"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Model:\n",
    "    \"\"\"\n",
    "    Model class for all 2 player based games with neural network training\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tag):\n",
    "        \"\"\"\n",
    "        :param tag: used tag for neural network model (e.g. 1 for first player and -1 for second)\n",
    "        \"\"\"\n",
    "        self.tag = tag\n",
    "        self.epsilon = 0.1\n",
    "        self.alpha = 0.5\n",
    "        self.gamma = 1\n",
    "        self.model = self.__load_model()\n",
    "        self.history = []\n",
    "        self.memory = []\n",
    "        self.count_memory = 0\n",
    "        self.batch_size = 10\n",
    "\n",
    "    def __load_model(self):\n",
    "        \"\"\"\n",
    "        Loads previously saved model\n",
    "        :return: loaded model\n",
    "        \"\"\"\n",
    "        if self.tag == 1:\n",
    "            tag = '_first'\n",
    "        else:\n",
    "            tag = '_second'\n",
    "\n",
    "        s = 'model_values' + tag + '.h5'\n",
    "        model_file = Path(s)\n",
    "\n",
    "        if model_file.is_file():\n",
    "            print('load model')\n",
    "            model = Km.load_model(s)\n",
    "            print('load model: ' + s)\n",
    "        else:\n",
    "            model = self.create_model()\n",
    "        return model\n",
    "\n",
    "    @abstractmethod\n",
    "    def create_model(self):\n",
    "        \"\"\"\n",
    "        Create new model with appropriate number of layers and network structure\n",
    "        :return: created model\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def state_to_tensor(self, state, move):\n",
    "        \"\"\"\n",
    "        Creates a tensor (2 dim array) based on a state and a move as input vector for nn\n",
    "        :param state: current state\n",
    "        :param move: current move\n",
    "        :return: created tensor\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def one_hot_encode_state(self, state):\n",
    "        \"\"\"\n",
    "        One hot encoding for the state.\n",
    "        Each field input of 3x3 matrix will be displayed with 0 (blank), 1 (player 1), -1 (player 2)\n",
    "        :param state: state to encode\n",
    "        :return: encoded state\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def __choose_optimal_move(self, state) -> int:\n",
    "        \"\"\"\n",
    "        Choose optimal move based on the calculated and best predicted values of current state.\n",
    "        :param state: current state\n",
    "        :return: best move with highest value (randomly select for equal values)\n",
    "        \"\"\"\n",
    "        v = -float('Inf') # most negative value (negative infinity float)\n",
    "        v_list = [] # list of all calculated values\n",
    "        idx = [] # move index for chosen move\n",
    "        for move in Board.POSSIBLE_ACTIONS:\n",
    "            value = self.model.calc_value(state, move)\n",
    "            v_list.append(round(float(value), 5))\n",
    "\n",
    "            if value > v:\n",
    "                v = value\n",
    "                idx = [move]\n",
    "            elif v == value:\n",
    "                idx.append(move)\n",
    "\n",
    "        idx = random.choice(idx)\n",
    "        return idx\n",
    "\n",
    "    def __calc_value(self, state, move) -> float:\n",
    "        \"\"\"\n",
    "        Calculate a tensor and predict the reward\n",
    "        :param state: current state\n",
    "        :param move: current move\n",
    "        :return: most predicted value (predicted reward)\n",
    "        \"\"\"\n",
    "\n",
    "        tensor = self.state_to_tensor(state, move)\n",
    "        value = self.model.predict(tensor)\n",
    "        return value\n",
    "\n",
    "    def __calc_target(self, prev_state, prev_move, state, reward):\n",
    "        \"\"\"\n",
    "        Calculate the target vector (q value or reward)\n",
    "        :param prev_state: previous state\n",
    "        :param prev_move: previous move\n",
    "        :param state: current state\n",
    "        :param reward: previous reward\n",
    "        :return: calculated target value\n",
    "        \"\"\"\n",
    "        qvalue = self.__calc_value(prev_state, prev_move)\n",
    "        v = []\n",
    "        tensor = self.state_to_tensor(prev_state, prev_move)\n",
    "\n",
    "        for move in range(len(tensor[:,0][0])):\n",
    "            v.append(self.__calc_value(state, move))\n",
    "\n",
    "        if reward == 0:\n",
    "            v_s_tag = self.gamma * np.max(v)\n",
    "            target = np.array(qvalue + self.alpha * (reward + v_s_tag - qvalue))\n",
    "        else:\n",
    "            target = reward\n",
    "\n",
    "        return target\n",
    "\n",
    "    def __save_model(self):\n",
    "        \"\"\"\n",
    "        save model as h5 file\n",
    "        \"\"\"\n",
    "        if self.tag == 1:\n",
    "            tag = '_first'\n",
    "        else:\n",
    "            tag = '_second'\n",
    "        s = 'model_values' + tag + '.h5'\n",
    "\n",
    "        try:\n",
    "            os.remove(s)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        self.model.save(s)\n",
    "\n",
    "    def predict_model(self, state, online: bool):\n",
    "        if online:\n",
    "            state = self.one_hot_encode_state(state)\n",
    "            target = self.model.predict(state)\n",
    "        else:\n",
    "            target = self.__choose_optimal_move(state)\n",
    "\n",
    "        return target\n",
    "\n",
    "    def __learn_batch(self, memory):\n",
    "        \"\"\"\n",
    "        Learn model with a batch of states and actions from memory\n",
    "        :param memory: saved states, actions and rewards\n",
    "        \"\"\"\n",
    "        print('start learning player', self.tag)\n",
    "        print('data length:', len(memory))\n",
    "\n",
    "        # build x_train\n",
    "        ind = 0\n",
    "        x_train = np.zeros((len(memory), 2, 9))\n",
    "        for v in memory:\n",
    "            [prev_state, prev_move, _, _] = v\n",
    "            sample = self.state_to_tensor(prev_state, prev_move)\n",
    "            x_train[ind, :, :] = sample\n",
    "            ind += 1\n",
    "\n",
    "        # train with planning\n",
    "        loss = 20\n",
    "        count = 0\n",
    "        while loss > 0.02 and count < 10:\n",
    "            y_train = self.__create_targets(memory)\n",
    "            history = self.model.fit(x_train, y_train, epochs=5, batch_size=256, verbose=0)\n",
    "            self.history.append(history.history)\n",
    "            loss = self.model.evaluate(x_train, y_train, batch_size=256, verbose=0)[0]\n",
    "            count += 1\n",
    "            print('planning number:', count, 'loss', loss)\n",
    "\n",
    "        loss = self.model.evaluate(x_train, y_train, batch_size=256, verbose=0)\n",
    "        print('player:', self.tag, loss, 'loops', count)\n",
    "\n",
    "        self.__save_model()\n",
    "\n",
    "    def train_model_offline(self, prev_state, prev_move, state, reward):\n",
    "\n",
    "        self.__load_to_memory(prev_state, prev_move, state, reward)\n",
    "        self.count_memory += 1\n",
    "\n",
    "        if self.count_memory == self.batch_size:\n",
    "            self.count_memory = 0\n",
    "            self.__learn_batch(self.memory)\n",
    "            self.memory = []\n",
    "\n",
    "    def __load_to_memory(self, prev_state, prev_move, state, reward):\n",
    "        \"\"\"\n",
    "        Load all q related things into memory to learn in batch\n",
    "        :param prev_state: previous known state\n",
    "        :param prev_move: previous made move\n",
    "        :param state: new state\n",
    "        :param reward: previous reward\n",
    "        \"\"\"\n",
    "        self.memory.append([prev_state, prev_move, state, reward])\n",
    "\n",
    "    def __create_targets(self, memory):\n",
    "        \"\"\"\n",
    "        Create target vector for each state-action-pair in memory\n",
    "        :param memory: saved states, actions and rewards\n",
    "        :return: target vector\n",
    "        \"\"\"\n",
    "        y_train_ = np.zeros((len(memory), 1))\n",
    "        count_ = 0\n",
    "        for v_ in memory:\n",
    "            [prev_state_, prev_move_, state_, reward_] = v_\n",
    "            target = self.__calc_target(prev_state_, prev_move_, state_, reward_)\n",
    "            y_train_[count_, :] = target\n",
    "            count_ += 1\n",
    "\n",
    "        return y_train_\n",
    "\n",
    "    def train_model_online(self, prev_state, new_state, prev_move, reward, discount_factor=0.8):\n",
    "        \"\"\"\n",
    "        Train the model based on the current state, action and received reward\n",
    "        :param prev_state: previous state\n",
    "        :param new_state: new state\n",
    "        :param prev_move: previous move\n",
    "        :param discount_factor: discount factor of this q learner\n",
    "        :param reward: received reward after prev action\n",
    "        \"\"\"\n",
    "        prev_state = self.one_hot_encode_state(prev_state)\n",
    "        new_state = self.one_hot_encode_state(new_state)\n",
    "\n",
    "        target = reward + discount_factor * np.max( self.model.predict(new_state))\n",
    "        target_vector = self.model.predict(prev_state)[0]\n",
    "        target_vector[prev_move] = target\n",
    "        history = self.model.fit(prev_state, target_vector.reshape(-1, len(Board.POSSIBLE_ACTIONS)), epochs=1, verbose=0)\n",
    "        self.history = self.history.append(history.history)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class TicTacToeModel(Model):\n",
    "    \"\"\"\n",
    "    Special model for tic tac toe games.\n",
    "\n",
    "    Consists of 2x9 input vector, dense network of 9 layers and a 1 sized target vector.\n",
    "    Input vector consists an array with length 9 for the chosen move and an array for the state.\n",
    "    Target vector consists of one value for the q value.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tag):\n",
    "        super().__init__(tag)\n",
    "        pass\n",
    "\n",
    "    def create_model(self):\n",
    "        \"\"\"\n",
    "        Creates keras model\n",
    "        :return: keras model\n",
    "        \"\"\"\n",
    "        print('new model')\n",
    "\n",
    "        model = Km.Sequential()\n",
    "        model.add(kl.Flatten(input_shape=(2, 9)))\n",
    "        model.add(kl.Dense(18))\n",
    "        model.add(kl.LeakyReLU(alpha=0.3))\n",
    "        model.add(kl.Dense(18))\n",
    "        model.add(kl.LeakyReLU(alpha=0.3))\n",
    "        model.add(kl.Dense(18))\n",
    "        model.add(kl.LeakyReLU(alpha=0.3))\n",
    "        model.add(kl.Dense(18))\n",
    "        model.add(kl.LeakyReLU(alpha=0.3))\n",
    "        model.add(kl.Dense(18))\n",
    "        model.add(kl.LeakyReLU(alpha=0.3))\n",
    "        model.add(kl.Dense(18))\n",
    "        model.add(kl.LeakyReLU(alpha=0.3))\n",
    "        model.add(kl.Dense(18))\n",
    "        model.add(kl.LeakyReLU(alpha=0.3))\n",
    "        model.add(kl.Dense(9))\n",
    "        model.add(kl.LeakyReLU(alpha=0.3))\n",
    "        model.add(kl.Dense(1, activation='linear'))\n",
    "\n",
    "        model.compile(optimizer='Adam', loss='mean_absolute_error', metrics=['accuracy'])\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        return model\n",
    "\n",
    "    def state_to_tensor(self, state, move):\n",
    "        \"\"\"\n",
    "        Generates a tensor of state and move index\n",
    "        :param state: current state\n",
    "        :param move: current move\n",
    "        :return: tensor (2 dim array)\n",
    "        \"\"\"\n",
    "        state = self.one_hot_encode_state(state)\n",
    "\n",
    "        a = np.zeros(9).astype('float32')\n",
    "        a[move] = 1 # one hot encoding for chosen action (1 for the chosen action an 0 for none)\n",
    "\n",
    "        state = np.asarray(state).astype('float32')\n",
    "        tensor = np.array((a, state))\n",
    "        tensor = tensor.reshape((1, 2, 9))\n",
    "\n",
    "        return tensor\n",
    "\n",
    "    def one_hot_encode_state(self, state):\n",
    "        \"\"\"\n",
    "        One hot encoding for the state.\n",
    "        Each field input of 3x3 matrix will be displayed with 0 (blank), 1 (player 1), -1 (player 2)\n",
    "        :param state: state to encode\n",
    "        :return: encoded state\n",
    "        \"\"\"\n",
    "        state = state.flatten() # flatten 3x3 matrix because of 1 length input vector for state\n",
    "\n",
    "        for i in range(len(state)):\n",
    "            if state[i] is None:\n",
    "                state[i] = 0\n",
    "            if state[i] == 'x':\n",
    "                state[i] = 1\n",
    "            if state[i] == 'o':\n",
    "                state[i] = -1\n",
    "\n",
    "        state = state.reshape((1, 9))\n",
    "\n",
    "        return state"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class TicTacToeModelSmall(TicTacToeModel):\n",
    "    \"\"\"\n",
    "    Special model for tic tac toe games.\n",
    "\n",
    "    Consists of 1x9 input vector, dense network of 1 layer and a 9 sized target vector.\n",
    "    Input vector consists an array with length 9 for the state.\n",
    "    Target vector consists of 9 sized array for 9 possible rewards (one for each action).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tag, observation_space=9, action_space=9):\n",
    "        self.observation_space = observation_space\n",
    "        self.action_space = action_space\n",
    "        super().__init__(tag)\n",
    "        pass\n",
    "\n",
    "    def create_model(self):\n",
    "        \"\"\"\n",
    "        Creates keras model\n",
    "        :return: keras model\n",
    "        \"\"\"\n",
    "        print('new model')\n",
    "\n",
    "        model = Km.Sequential()\n",
    "        model.add(kl.InputLayer(batch_input_shape=(1, self.observation_space)))\n",
    "        model.add(kl.Dense(20, activation='relu'))\n",
    "        model.add(kl.Dense(self.action_space, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer='adam', metrics=['mae'])\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
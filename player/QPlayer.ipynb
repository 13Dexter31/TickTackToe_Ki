{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Content"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "from Board import Result\n",
    "from player.Player import Player\n",
    "from Decorators import debug\n",
    "from abc import abstractmethod\n",
    "from collections import defaultdict\n",
    "\n",
    "import os\n",
    "import dill\n",
    "import numpy as np\n",
    "import random"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "class QPlayer(Player):\n",
    "    \"\"\"\n",
    "    A Player specification used to handle various learning algorithms.\n",
    "\n",
    "    To specify a learning algorithm a QLearner is defined.\n",
    "    Different QLearner will be used to handle the learning process and to choose the best next move.\n",
    "    QPlayer and QLearner interact according to the player-role pattern.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, representationChar, q_learner):\n",
    "        \"\"\"\n",
    "        The constructor for the Q-Player\n",
    "        :param representationChar: As from the Player class inherited the QPlayer is given a single Char representing the user on the bord.\n",
    "        Longer strings would also get accepted but would yield a worse representation.\n",
    "        :param q_learner: The QLearner used to handle the learning process as well as the choosing of the best option\n",
    "\n",
    "        Terms:\n",
    "        state => array representing the Board.field\n",
    "        action => array representing the chosen field on the board\n",
    "        result => entity of the Result class used to assess an previously chosen move\n",
    "\n",
    "        For more specific examples see the description of the given Q-Learner\n",
    "        \"\"\"\n",
    "\n",
    "        Player.__init__(self, representationChar)\n",
    "        self.q_learner = q_learner\n",
    "        self.prev_state = None\n",
    "        self.state = None\n",
    "        self.prev_action = None\n",
    "        self.action = None\n",
    "        self.prev_reward = 0\n",
    "\n",
    "    @debug\n",
    "    #@log(aiReadable=True)\n",
    "    def makeMove(self, board):\n",
    "        \"\"\"\n",
    "        Makes a move on the board\n",
    "        :param board: current state of the board\n",
    "        :return: list of a random column (first) and row number (second)\n",
    "        \"\"\"\n",
    "        self.state = np.array(board.field)\n",
    "        self.action = self.q_learner.select_move(state=self.state)\n",
    "\n",
    "        return self.action\n",
    "\n",
    "    def giveResult(self, result):\n",
    "        \"\"\"\n",
    "        Set own reward for given result and updates the q table\n",
    "        \"\"\"\n",
    "        reward = -0.1\n",
    "        if Result.INVALID_MOVE == result:\n",
    "            reward = -100\n",
    "\n",
    "        elif Result.GAME_LOST == result:\n",
    "            reward = -10\n",
    "            self.stats.incrLost()\n",
    "\n",
    "        elif Result.GAME_WON == result:\n",
    "            reward = 100\n",
    "            self.stats.incrWon()\n",
    "\n",
    "        elif Result.GAME_DRAW == result:\n",
    "            reward = -1\n",
    "            self.stats.incrDraw()\n",
    "\n",
    "        self.update_q_lerner()\n",
    "\n",
    "        self.prev_action = self.action\n",
    "        self.prev_state = np.array(self.state)\n",
    "        self.prev_reward = reward\n",
    "\n",
    "        if result in [Result.GAME_WON, Result.GAME_LOST, Result.GAME_DRAW]:\n",
    "            self.update_q_lerner()\n",
    "            self.prev_action = None\n",
    "\n",
    "    def update_q_lerner(self):\n",
    "        \"\"\"\n",
    "        Triggers the learning process of the Q-Player\n",
    "        \"\"\"\n",
    "        if self.prev_action is not None:\n",
    "            action_idx = list(self.q_learner.possible_actions.keys())[\n",
    "                list(self.q_learner.possible_actions.values()).index(self.prev_action)\n",
    "            ]\n",
    "\n",
    "            self.q_learner.update(self.prev_state, self.state, action_idx, self.prev_reward)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class QLearner:\n",
    "    \"\"\"\n",
    "    The abstract class generalizing all q-learning methods\n",
    "    \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def update(self, prev_state, state, prev_action_idx, result):\n",
    "        \"\"\"\n",
    "        Calculates the new q value from the new state and action pair\n",
    "        :param prev_state: last state of the board\n",
    "        :param state: new state of the board, including the new action\n",
    "        :param prev_action_idx: chosen action\n",
    "        :param result: reward for chosen action\n",
    "        :return: update algorithm for the new results\n",
    "        \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def select_move(self, state, theta=0.9):\n",
    "        \"\"\"\n",
    "        Choose action according to softmax function in state\n",
    "        :param state: state of the environment\n",
    "        :param theta: \"temperature\" parameter\n",
    "        :return: the action that got calculated as the best next move\n",
    "        \"\"\"\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "class QTableLearner(QLearner):\n",
    "    \"\"\"\n",
    "    QLearner specification for q-table\n",
    "\n",
    "    Terms:\n",
    "        state\n",
    "            the snapshot of the board fields\n",
    "            example: [[ 'x',  'o',  'o'],\n",
    "                      [None,  'x', None],\n",
    "                      [None, None, None]]\n",
    "\n",
    "        action\n",
    "            equals a chosen value from the defined possible_actions\n",
    "            example: [3, 3]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, q_table=None, learn_rate=0.1, discount_factor=0.8):\n",
    "        \"\"\"\n",
    "        :param learn_rate: learning rate of this q learner\n",
    "        :param discount_factor: discount factor of this q learner\n",
    "        \"\"\"\n",
    "\n",
    "        if q_table is None:\n",
    "            q_table = defaultdict(lambda: np.zeros(shape=[3, 3]))\n",
    "\n",
    "        self.possible_actions = {0: [1, 1], 1: [2, 1], 2: [3, 1], 3: [1, 2], 4: [2, 2], 5: [3, 2], 6: [1, 3], 7: [2, 3],\n",
    "                                 8: [3, 3]}\n",
    "        self.q_table = q_table\n",
    "        self.learn_rate = learn_rate\n",
    "        self.discount_factor = discount_factor\n",
    "\n",
    "    #@save_q_table\n",
    "    def update(self, prev_state, state, prev_action_idx, result):\n",
    "        \"\"\"\n",
    "        see QLearner\n",
    "        \"\"\"\n",
    "        prev_state, state = np.array_str(prev_state), np.array_str(state)\n",
    "\n",
    "        old_value = self.q_table[prev_state][prev_action_idx // 3, prev_action_idx % 3]\n",
    "        max_q = 0 if result in [Result.GAME_WON, Result.GAME_LOST, Result.GAME_DRAW] else np.max(self.q_table[state])\n",
    "        new_value = result + self.discount_factor * max_q\n",
    "        temporal_difference = new_value - old_value\n",
    "\n",
    "        self.q_table[prev_state][prev_action_idx // 3, prev_action_idx % 3] = old_value + self.learn_rate * temporal_difference\n",
    "\n",
    "        return self.q_table\n",
    "\n",
    "    def select_move(self, state, theta=0.9):\n",
    "        \"\"\"\n",
    "        see QLearner\n",
    "        \"\"\"\n",
    "        if np.random.uniform(0, 1) > theta:  # then exploit the env --> use Qtable or memory info\n",
    "            idx = np.argmax(self.q_table[np.array_str(state)])\n",
    "            action = self.possible_actions[idx]\n",
    "\n",
    "        else:  # then explore the enviroment --> randomly sample a move from available moves\n",
    "            action = random.choice(self.possible_actions)  # that is the agent always explores the enviroment\n",
    "\n",
    "        return action  # return choosen move"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "class QUtils:\n",
    "\n",
    "    @staticmethod\n",
    "    def pretty_print_q_table(dict_q_table):\n",
    "        \"\"\"\n",
    "        prints a human readable representation of the given q_table\n",
    "        :param dict_q_table:\n",
    "        \"\"\"\n",
    "\n",
    "        for key, values in dict_q_table.items():\n",
    "            values = np.round_(values, 2)\n",
    "\n",
    "            pretty_key = str(key).replace(\"None\", \"' '\").replace('[[', ' [').replace(']]', ']')\n",
    "            pretty_value = str(values).replace('[[', ' [').replace(']]', ']')\n",
    "            print(f\"{pretty_key}\\n\\n{pretty_value}\\n\\n-------------------------\\n\")\n",
    "\n",
    "    @staticmethod\n",
    "    def get_dict_from_file(filepath):\n",
    "        \"\"\"\n",
    "        uses the given filepath to read a dict from the specified file\n",
    "        :param filepath: the file to read in\n",
    "        :return: the dict read from the file\n",
    "        \"\"\"\n",
    "\n",
    "        if os.path.isfile(filepath) and os.path.getsize(filepath) > 0:\n",
    "            with open(filepath, \"rb\") as file:\n",
    "                return dill.load(file)\n",
    "\n",
    "    @staticmethod\n",
    "    def save_dict_to_file(filepath, dict):\n",
    "        \"\"\"\n",
    "        save a dict to a specified file\n",
    "        :param filepath: the file to write to\n",
    "        :param dict: the savable dict\n",
    "        \"\"\"\n",
    "\n",
    "        with open(filepath, 'wb') as file:\n",
    "            dill.dump(dict, file)\n",
    "\n",
    "    @staticmethod\n",
    "    def merge_dicts(d0: dict, d1: dict):\n",
    "        \"\"\"\n",
    "        merges 2 given dicts\n",
    "        :param d0: first dict\n",
    "        :param d1: second dict\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        d = d0.copy()\n",
    "        for k, v in d1.items():\n",
    "            if (k not in d):\n",
    "                d[k] = d1[k]\n",
    "\n",
    "        return d"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
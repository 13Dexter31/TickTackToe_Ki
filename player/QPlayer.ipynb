{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Content"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "from Board import Result\n",
    "from player.Player import Player\n",
    "from Decorators import debug\n",
    "from abc import abstractmethod\n",
    "from collections import defaultdict\n",
    "\n",
    "import os\n",
    "import dill\n",
    "import numpy as np\n",
    "import random"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "class QPlayer(Player):\n",
    "    \"\"\"\n",
    "    A Player specification used to handle various learning algorithms.\n",
    "\n",
    "    To specify a learning algorithm a QLearner is defined.\n",
    "    Different QLearner will be used to handle the learning process and to choose the best next move.\n",
    "    QPlayer and QLearner interact according to the player-role pattern.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, representationChar, q_learner):\n",
    "        \"\"\"\n",
    "        The constructor for the Q-Player\n",
    "        :param representationChar: As from the Player class inherited the QPlayer is given a single Char representing the user on the bord.\n",
    "        Longer strings would also get accepted but would yield a worse representation.\n",
    "        :param q_learner: The QLearner used to handle the learning process as well as the choosing of the best option\n",
    "\n",
    "        Terms:\n",
    "        state => array representing the Board.field\n",
    "        action => array representing the chosen field on the board\n",
    "        result => entity of the Result class used to assess an previously chosen move\n",
    "\n",
    "        For more specific examples see the description of the given Q-Learner\n",
    "        \"\"\"\n",
    "\n",
    "        Player.__init__(self, representationChar)\n",
    "        self.q_learner = q_learner\n",
    "        self.prev_state = None\n",
    "        self.state = None\n",
    "        self.prev_action = None\n",
    "        self.action = None\n",
    "        self.prev_reward = 0\n",
    "\n",
    "    @debug\n",
    "    #@log(aiReadable=True)\n",
    "    def makeMove(self, board):\n",
    "        \"\"\"\n",
    "        Makes a move on the board\n",
    "        :param board: current state of the board\n",
    "        :return: list of a random column (first) and row number (second)\n",
    "        \"\"\"\n",
    "        self.state = board.field\n",
    "        self.action = self.q_learner.select_move(state=self.state)\n",
    "\n",
    "        return self.action\n",
    "\n",
    "    def giveResult(self, result) -> None:\n",
    "        \"\"\"\n",
    "        Set own reward for given result and updates the q table\n",
    "        \"\"\"\n",
    "        reward = 0\n",
    "        if Result.INVALID_MOVE == result:\n",
    "            reward = -100\n",
    "\n",
    "        elif Result.GAME_LOST == result:\n",
    "            reward = -10\n",
    "            self.stats.incrLost()\n",
    "\n",
    "        elif Result.GAME_WON == result:\n",
    "            reward = 100\n",
    "            self.stats.incrWon()\n",
    "\n",
    "        elif Result.GAME_DRAW == result:\n",
    "            reward = -1\n",
    "            self.stats.incrDraw()\n",
    "\n",
    "        self.update_q_lerner()\n",
    "\n",
    "        self.prev_action = self.action\n",
    "        self.prev_state = np.array(self.state)\n",
    "        self.prev_reward = reward\n",
    "\n",
    "    def update_q_lerner(self) -> None:\n",
    "        \"\"\"\n",
    "        Triggers the learning process of the Q-Player\n",
    "        \"\"\"\n",
    "        if self.prev_action is not None:\n",
    "            action_idx = list(self.q_learner.get_possible_actions().keys())[\n",
    "                list(self.q_learner.get_possible_actions().values()).index(self.prev_action)\n",
    "            ]\n",
    "\n",
    "            self.q_learner.update(self.prev_state, self.state, action_idx, self.prev_reward)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "class QLearner:\n",
    "    \"\"\"\n",
    "    The abstract class generalizing all q-learning methods\n",
    "    \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def update(self, state, new_state, action, result) -> None:\n",
    "        \"\"\"\n",
    "        Calculates the new q value from the new state and action pair\n",
    "        :param state: last state of the board\n",
    "        :param new_state: new state of the board, including the new action\n",
    "        :param action: chosen action\n",
    "        :param result: reward for chosen action\n",
    "        \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def select_move(self, state, theta=0.9):\n",
    "        \"\"\"\n",
    "        Choose action according to softmax function in state\n",
    "        :param state: state of the environment\n",
    "        :param theta: \"temperature\" parameter\n",
    "        :return: the action that got calculated as the best next move\n",
    "        \"\"\"\n",
    "\n",
    "    def get_possible_actions(self) -> dict:\n",
    "        \"\"\"\n",
    "        :return: a dict of possible actions (moves)\n",
    "        \"\"\"\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "class QTableLearner(QLearner):\n",
    "    \"\"\"\n",
    "    QLearner specification for q-table\n",
    "\n",
    "    Terms:\n",
    "        state\n",
    "            the snapshot of the board fields\n",
    "            example: [[ 'x',  'o',  'o'],\n",
    "                      [None,  'x', None],\n",
    "                      [None, None, None]]\n",
    "\n",
    "        action\n",
    "            equals a chosen value from the defined possible_actions\n",
    "            example: [3, 3]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, q_table=None, learn_rate=0.1, discount_factor=0.8):\n",
    "        \"\"\"\n",
    "        :param learn_rate: learning rate of this q learner\n",
    "        :param discount_factor: discount factor of this q learner\n",
    "        \"\"\"\n",
    "\n",
    "        if q_table is None:\n",
    "            q_table = defaultdict(lambda: np.zeros(shape=[3, 3]))\n",
    "\n",
    "        self.possible_actions = {0: [1, 1], 1: [2, 1], 2: [3, 1], 3: [1, 2], 4: [2, 2], 5: [3, 2], 6: [1, 3], 7: [2, 3],\n",
    "                                 8: [3, 3]}\n",
    "        self.q_table = q_table\n",
    "        self.learn_rate = learn_rate\n",
    "        self.discount_factor = discount_factor\n",
    "\n",
    "    #@save_q_table\n",
    "    def update(self, state, new_state, action, result) -> None:\n",
    "        \"\"\"\n",
    "        Updates the Q-Table for future best move evaluations\n",
    "        :param state: last state of the board\n",
    "        :param new_state: new state of the board, including the new action\n",
    "        :param action: the index of a possible_action\n",
    "        :param result: reward for chosen action\n",
    "        \"\"\"\n",
    "        state, new_state = np.array_str(state), np.array_str(new_state)\n",
    "\n",
    "        self.q_table[state][action // 3, action % 3] = self.q_table[state][action // 3, action % 3] \\\n",
    "                                                       * (1 - self.learn_rate) \\\n",
    "                                                       + self.learn_rate \\\n",
    "                                                       * (result + self.discount_factor * np.max(\n",
    "                                                            self.q_table[new_state]))\n",
    "\n",
    "    def select_move(self, state, theta=0.9):\n",
    "        \"\"\"\n",
    "        see QLearner\n",
    "        \"\"\"\n",
    "        if np.random.uniform(0, 1) > theta:  # then exploit the env --> use Qtable or memory info\n",
    "            idx = np.argmax(self.q_table[np.array_str(state)])\n",
    "            action = self.possible_actions[idx]\n",
    "\n",
    "        else:  # then explore the enviroment --> randomly sample a move from available moves\n",
    "            action = random.choice(self.possible_actions)  # that is the agent always explores the enviroment\n",
    "\n",
    "        return action  # return choosen move\n",
    "\n",
    "    def get_possible_actions(self) -> dict:\n",
    "        return self.possible_actions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "class QUtils:\n",
    "\n",
    "    @staticmethod\n",
    "    def pretty_print_q_table(dict_q_table) -> None:\n",
    "        \"\"\"\n",
    "        prints a human readable representation of the given q_table\n",
    "        :param dict_q_table:\n",
    "        \"\"\"\n",
    "\n",
    "        for key, values in dict_q_table.items():\n",
    "            values = np.round_(values, 2)\n",
    "\n",
    "            pretty_key = str(key).replace(\"None\", \"' '\").replace('[[', ' [').replace(']]', ']')\n",
    "            pretty_value = str(values).replace('[[', ' [').replace(']]', ']')\n",
    "            print(f\"{pretty_key}\\n\\n{pretty_value}\\n\\n-------------------------\\n\")\n",
    "\n",
    "    @staticmethod\n",
    "    def get_dict_from_file(filepath) -> dict:\n",
    "        \"\"\"\n",
    "        uses the given filepath to read a dict from the specified file\n",
    "        :param filepath: the file to read in\n",
    "        :return: the dict read from the file\n",
    "        \"\"\"\n",
    "\n",
    "        if os.path.isfile(filepath) and os.path.getsize(filepath) > 0:\n",
    "            with open(filepath, \"rb\") as file:\n",
    "                return dill.load(file)\n",
    "\n",
    "    @staticmethod\n",
    "    def save_dict_to_file(filepath, dict) -> None:\n",
    "        \"\"\"\n",
    "        save a dict to a specified file\n",
    "        :param filepath: the file to write to\n",
    "        :param dict: the savable dict\n",
    "        \"\"\"\n",
    "\n",
    "        with open(filepath, 'wb') as file:\n",
    "            dill.dump(dict, file)\n",
    "\n",
    "    @staticmethod\n",
    "    def merge_dicts(d0: dict, d1: dict) -> dict:\n",
    "        \"\"\"\n",
    "        merges 2 given dicts\n",
    "        :param d0: first dict\n",
    "        :param d1: second dict\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        d = d0.copy()\n",
    "        for k, v in d1.items():\n",
    "            if (k not in d):\n",
    "                d[k] = d1[k]\n",
    "\n",
    "        return d"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "import ipytest\n",
    "import pytest\n",
    "import sys\n",
    "\n",
    "from Board import Board\n",
    "from player.QPlayer import QTableLearner\n",
    "from unittest import mock\n",
    "\n",
    "ipytest.autoconfig()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## QTableLearner"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### PyTest"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[31mF\u001B[0m\u001B[31m                                                                                         [100%]\u001B[0m\n",
      "============================================ FAILURES =============================================\n",
      "\u001B[31m\u001B[1m_______________________________________ test_q_table_update _______________________________________\u001B[0m\n",
      "\n",
      "    \u001B[94mdef\u001B[39;49;00m \u001B[92mtest_q_table_update\u001B[39;49;00m():\n",
      "    \n",
      "        q_table_lerner = QTableLearner()\n",
      "    \n",
      "        state = Board().field\n",
      "        new_state = state.copy()\n",
      "        new_state[\u001B[94m0\u001B[39;49;00m, \u001B[94m0\u001B[39;49;00m] = \u001B[33m'\u001B[39;49;00m\u001B[33mx\u001B[39;49;00m\u001B[33m'\u001B[39;49;00m\n",
      "        action = [\u001B[94m1\u001B[39;49;00m, \u001B[94m1\u001B[39;49;00m]\n",
      "    \n",
      ">       q_table_lerner.update(state, new_state, action, Result.GAME_WON)\n",
      "\n",
      "\u001B[1m\u001B[31mC:\\Users\\Hannes\\AppData\\Local\\Temp/ipykernel_10880/2649106753.py\u001B[0m:25: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "\n",
      "self = <player.QPlayer.QTableLearner object at 0x000001CAEFF7F760>\n",
      "state = '[[None None None]\\n [None None None]\\n [None None None]]'\n",
      "new_state = \"[['x' None None]\\n [None None None]\\n [None None None]]\", action = [1, 1]\n",
      "result = <Result.GAME_WON: 1>\n",
      "\n",
      "    \u001B[94mdef\u001B[39;49;00m \u001B[92mupdate\u001B[39;49;00m(\u001B[96mself\u001B[39;49;00m, state, new_state, action, result):\n",
      "        \u001B[33m\"\"\"\u001B[39;49;00m\n",
      "    \u001B[33m    see QLearner\u001B[39;49;00m\n",
      "    \u001B[33m    \"\"\"\u001B[39;49;00m\n",
      "        state, new_state = np.array_str(state), np.array_str(new_state)\n",
      "    \n",
      ">       \u001B[96mself\u001B[39;49;00m.q_table[state][action // \u001B[94m3\u001B[39;49;00m, action % \u001B[94m3\u001B[39;49;00m] = \u001B[96mself\u001B[39;49;00m.q_table[state][action // \u001B[94m3\u001B[39;49;00m, action % \u001B[94m3\u001B[39;49;00m]                                                        * (\u001B[94m1\u001B[39;49;00m - \u001B[96mself\u001B[39;49;00m.learn_rate)                                                        + \u001B[96mself\u001B[39;49;00m.learn_rate                                                        * (result + \u001B[96mself\u001B[39;49;00m.discount_factor * np.max(\n",
      "                                                            \u001B[96mself\u001B[39;49;00m.q_table[new_state]))\n",
      "\u001B[1m\u001B[31mE       TypeError: unsupported operand type(s) for //: 'list' and 'int'\u001B[0m\n",
      "\n",
      "\u001B[1m\u001B[31mQPlayer.py\u001B[0m:177: TypeError\n",
      "===================================== short test summary info =====================================\n",
      "FAILED tmpf5fe94p1.py::test_q_table_update - TypeError: unsupported operand type(s) for //: 'list...\n",
      "\u001B[31m\u001B[31m\u001B[1m1 failed\u001B[0m, \u001B[32m3 passed\u001B[0m\u001B[31m in 0.04s\u001B[0m\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest\n",
    "\n",
    "def test_q_table_class():\n",
    "\n",
    "    q_table_lerner = QTableLearner()\n",
    "    assert type(q_table_lerner.q_table) == defaultdict\n",
    "\n",
    "def test_q_table_content():\n",
    "\n",
    "    q_table_lerner = QTableLearner()\n",
    "    assert len(q_table_lerner.q_table) == 0\n",
    "\n",
    "def test_q_table_select_move():\n",
    "\n",
    "    q_table_lerner = QTableLearner()\n",
    "    assert q_table_lerner.select_move(Board().field) is not None\n",
    "\n",
    "def test_q_table_update():\n",
    "\n",
    "    q_table_lerner = QTableLearner()\n",
    "\n",
    "    state = Board().field\n",
    "    new_state = state.copy()\n",
    "    new_state[0, 0] = 'x'\n",
    "    action_idx = 1\n",
    "\n",
    "    q_table_lerner.update(state, new_state, action_idx, Result.GAME_WON)\n",
    "\n",
    "    assert True"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Mock"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
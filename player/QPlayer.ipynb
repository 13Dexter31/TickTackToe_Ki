{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Content"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from Board import Result\n",
    "from player.Player import Player\n",
    "from Decorators import debug, log\n",
    "from abc import abstractmethod\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "import os\n",
    "import dill\n",
    "import numpy as np\n",
    "import random"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "class QPlayer(Player):\n",
    "    \"\"\"\n",
    "    Q learner player class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, representationChar, q_learner):\n",
    "\n",
    "        Player.__init__(self ,representationChar)\n",
    "        self.q_learner = q_learner\n",
    "        self.prev_state = None\n",
    "        self.state = None\n",
    "        self.prev_action = []\n",
    "        self.action = []\n",
    "        self.prev_reward = 0\n",
    "\n",
    "    @debug\n",
    "    #@log(aiReadable=True)\n",
    "    def makeMove(self, board):\n",
    "        \"\"\"\n",
    "        Makes a move on the board\n",
    "        :param board: current state of the board\n",
    "        :return: list of a random column (first) and row number (second)\n",
    "        \"\"\"\n",
    "        self.state = board.field\n",
    "        self.action = self.q_learner.select_move(state=self.state)\n",
    "\n",
    "        return self.action\n",
    "\n",
    "    def giveResult(self, result):\n",
    "        \"\"\"\n",
    "        Set own reward for given result and updates the q table\n",
    "        \"\"\"\n",
    "        reward = 0\n",
    "        if Result.INVALID_MOVE == result: reward = -100\n",
    "        if Result.GAME_LOST == result: reward = -10\n",
    "        if Result.GAME_WON == result: reward = 100\n",
    "        if Result.GAME_DRAW == result: reward = -1\n",
    "\n",
    "        if len(self.prev_action) > 0:\n",
    "            action_idx = list(self.q_learner.possible_actions.keys())[\n",
    "                list(self.q_learner.possible_actions.values()).index(self.prev_action)\n",
    "            ]\n",
    "\n",
    "            self.q_learner.update(self.prev_state, self.state, action_idx, self.prev_reward)\n",
    "\n",
    "        self.prev_action = self.action\n",
    "        self.prev_state = self.state\n",
    "        self.prev_reward = reward"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class QLearner:\n",
    "    \"\"\"\n",
    "    Q learner player class\n",
    "    \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def update(self, state, new_state, action, result):\n",
    "        \"\"\"\n",
    "        Calculates the new q value from the new state and action pair\n",
    "        :param state: last state of the board\n",
    "        :param new_state: new state of the board, including the new action\n",
    "        :param action: chosen action\n",
    "        :param result: reward for chosen action\n",
    "        :return: updated q table with the new value\n",
    "        \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def select_move(self, state, theta=0.9):\n",
    "        \"\"\"\n",
    "        Choose action according to softmax function in state s\n",
    "        :param state: state of the environment\n",
    "        :param theta: \"temperature\" parameter\n",
    "        :return: selected action\n",
    "        \"\"\"\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class QTableLearner (QLearner):\n",
    "    \"\"\"\n",
    "    QLearner specification for q-table\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, q_table = None, learn_rate=0.1, discount_factor=0.8):\n",
    "        \"\"\"\n",
    "        :param learn_rate: learning rate of this q learner\n",
    "        :param discount_factor: discount factor of this q learner\n",
    "        \"\"\"\n",
    "        self.possible_actions = {0:[1, 1], 1:[1, 2], 2:[1, 3], 3:[2, 1], 4:[2, 2], 5:[2, 3], 6:[3, 1], 7:[3, 2], 8:[3, 3]}\n",
    "        self.q_table = q_table if q_table is None else defaultdict(lambda: np.zeros(len(self.possible_actions)))\n",
    "        self.learn_rate = learn_rate\n",
    "        self.discount_factor = discount_factor\n",
    "\n",
    "    #@save_q_table\n",
    "    def update(self, state, new_state, action, result):\n",
    "        \"\"\"\n",
    "        Calculates the new q value from the new state and action pair\n",
    "        :param state: last state of the board\n",
    "        :param new_state: new state of the board, including the new action\n",
    "        :param action: chosen action\n",
    "        :param result: reward for chosen action\n",
    "        :return: updated q table with the new value\n",
    "        \"\"\"\n",
    "        state, new_state= np.array_str(state), np.array_str(new_state)\n",
    "\n",
    "        self.q_table[state][action] = self.q_table[state][action] \\\n",
    "                                      * (1 - self.learn_rate) \\\n",
    "                                      + self.learn_rate \\\n",
    "                                      * (result + self.discount_factor * np.max(self.q_table[new_state]))\n",
    "\n",
    "        return self.q_table\n",
    "\n",
    "    def select_move(self, state, theta=0.9):\n",
    "        \"\"\"\n",
    "        Choose action according to softmax function in state s\n",
    "        :param state: state of the environment\n",
    "        :param theta: \"temperature\" parameter\n",
    "        :return: selected action\n",
    "        \"\"\"\n",
    "        if np.random.uniform(0, 1) > theta: # then exploit the env --> use Qtable or memory info\n",
    "            q_table = self.q_table#self.get_q_table_from_file()\n",
    "            idx = np.argmax(q_table[np.array_str(state)])\n",
    "            action = self.possible_actions[idx]\n",
    "\n",
    "        else: # then explore the enviroment --> randomly sample a move from available moves\n",
    "            action = random.choice(self.possible_actions) # that is the agent always explores the enviroment\n",
    "\n",
    "        return action # return choosen move"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class QUtils:\n",
    "\n",
    "    @staticmethod\n",
    "    def pretty_print_q_table(dictQTable):\n",
    "        for key, values in dictQTable.items():\n",
    "\n",
    "            prettyValues = \"\"\n",
    "            for value in values:\n",
    "                prettyValues += str(round(value, 1)).rjust(7)\n",
    "\n",
    "            print(prettyValues)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_dict_from_file(filepath):\n",
    "\n",
    "        if os.path.isfile(filepath) and os.path.getsize(filepath) > 0:\n",
    "            with open(filepath, \"rb\") as file:\n",
    "                return dill.load(file)\n",
    "\n",
    "    @staticmethod\n",
    "    def save_dict_to_file(filepath, dict):\n",
    "\n",
    "        with open(filepath, 'wb') as file:\n",
    "            dill.dump(dict, file)\n",
    "\n",
    "    @staticmethod\n",
    "    def merge_dicts(d0:dict, d1:dict):\n",
    "\n",
    "        d = d0.copy()\n",
    "        for k,v in d1.items():\n",
    "            if (k not in d):\n",
    "                d[k] = d1[k]\n",
    "\n",
    "        return d"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tests"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# QTableHelper.prettyPrintQTavle(QTableHelper.readInDict(\"..\\\\qtable.pkl\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Doku"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Content"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from Board import Result\n",
    "from player.Player import Player\n",
    "from Decorators import debug\n",
    "from abc import abstractmethod\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "import os\n",
    "import dill\n",
    "import numpy as np\n",
    "import random"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "class QPlayer(Player):\n",
    "    \"\"\"\n",
    "    The Q-Player using a specified Q-Learner to find the best next move\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, representationChar, q_learner):\n",
    "\n",
    "        Player.__init__(self ,representationChar)\n",
    "        self.q_learner = q_learner\n",
    "        self.prev_state = None\n",
    "        self.state = None\n",
    "        self.prev_action = None\n",
    "        self.action = None\n",
    "        self.prev_reward = 0\n",
    "\n",
    "    @debug\n",
    "    #@log(aiReadable=True)\n",
    "    def makeMove(self, board):\n",
    "        \"\"\"\n",
    "        Makes a move on the board\n",
    "        :param board: current state of the board\n",
    "        :return: list of a random column (first) and row number (second)\n",
    "        \"\"\"\n",
    "        self.state = board.field\n",
    "        self.action = self.q_learner.select_move(state=self.state)\n",
    "\n",
    "        return self.action\n",
    "\n",
    "    def giveResult(self, result):\n",
    "        \"\"\"\n",
    "        Set own reward for given result and updates the q table\n",
    "        \"\"\"\n",
    "        reward = 0\n",
    "        if Result.INVALID_MOVE == result:\n",
    "            reward = -100\n",
    "\n",
    "        elif Result.GAME_LOST == result:\n",
    "            reward = -10\n",
    "            self.stats.incrLost()\n",
    "\n",
    "        elif Result.GAME_WON == result:\n",
    "            reward = 100\n",
    "            self.stats.incrWon()\n",
    "\n",
    "        elif Result.GAME_DRAW == result:\n",
    "            reward = -1\n",
    "            self.stats.incrDraw()\n",
    "\n",
    "        if self.prev_action is not None:\n",
    "            action_idx = list(self.q_learner.possible_actions.keys())[\n",
    "                list(self.q_learner.possible_actions.values()).index(self.prev_action)\n",
    "            ]\n",
    "\n",
    "            self.q_learner.update(self.prev_state, self.state, action_idx, self.prev_reward)\n",
    "\n",
    "        self.prev_action = self.action\n",
    "        self.prev_state = self.state\n",
    "        self.prev_reward = reward"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class QLearner:\n",
    "    \"\"\"\n",
    "    The abstract class generalizing all q-learning methods\n",
    "    \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def update(self, state, new_state, action, result):\n",
    "        \"\"\"\n",
    "        Calculates the new q value from the new state and action pair\n",
    "        :param state: last state of the board\n",
    "        :param new_state: new state of the board, including the new action\n",
    "        :param action: chosen action\n",
    "        :param result: reward for chosen action\n",
    "        :return: update algorithm for the new results\n",
    "        \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def select_move(self, state, theta=0.9):\n",
    "        \"\"\"\n",
    "        Choose action according to softmax function in state\n",
    "        :param state: state of the environment\n",
    "        :param theta: \"temperature\" parameter\n",
    "        :return: the action that got calculated as the best next move\n",
    "        \"\"\"\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class QTableLearner (QLearner):\n",
    "    \"\"\"\n",
    "    QLearner specification for q-table\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, q_table = None, learn_rate=0.1, discount_factor=0.8):\n",
    "        \"\"\"\n",
    "        :param learn_rate: learning rate of this q learner\n",
    "        :param discount_factor: discount factor of this q learner\n",
    "        \"\"\"\n",
    "        self.possible_actions = {0:[1, 1], 1:[2, 1], 2:[3, 1], 3:[1, 2], 4:[2, 2], 5:[3, 2], 6:[1, 3], 7:[2, 3], 8:[3, 3]}\n",
    "        self.q_table = q_table if q_table is None else defaultdict(lambda: np.empty([3, 3]))\n",
    "        self.learn_rate = learn_rate\n",
    "        self.discount_factor = discount_factor\n",
    "\n",
    "    #@save_q_table\n",
    "    def update(self, state, new_state, action, result):\n",
    "        \"\"\"\n",
    "        see QLearner\n",
    "        \"\"\"\n",
    "        state, new_state= np.array_str(state), np.array_str(new_state)\n",
    "\n",
    "        self.q_table[state][action // 3, action % 3] = self.q_table[state][action // 3, action % 3] \\\n",
    "                                      * (1 - self.learn_rate) \\\n",
    "                                      + self.learn_rate \\\n",
    "                                      * (result + self.discount_factor * np.max(self.q_table[new_state]))\n",
    "\n",
    "        return self.q_table\n",
    "\n",
    "    def select_move(self, state, theta=0.9):\n",
    "        \"\"\"\n",
    "        see QLearner\n",
    "        \"\"\"\n",
    "        if np.random.uniform(0, 1) > theta: # then exploit the env --> use Qtable or memory info\n",
    "            idx = np.argmax(self.q_table[np.array_str(state)])\n",
    "            action = self.possible_actions[idx]\n",
    "\n",
    "        else: # then explore the enviroment --> randomly sample a move from available moves\n",
    "            action = random.choice(self.possible_actions) # that is the agent always explores the enviroment\n",
    "\n",
    "        return action # return choosen move"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class QNNLearner (QLearner):\n",
    "    \"\"\"\n",
    "    QLearner specification for q-table\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model = None, learn_rate=0.1, discount_factor=0.8):\n",
    "        \"\"\"\n",
    "        :param learn_rate: learning rate of this q learner\n",
    "        :param discount_factor: discount factor of this q learner\n",
    "        \"\"\"\n",
    "        self.possible_actions = {0:[1, 1], 1:[2, 1], 2:[3, 1], 3:[1, 2], 4:[2, 2], 5:[3, 2], 6:[1, 3], 7:[2, 3], 8:[3, 3]}\n",
    "        self.learn_rate = learn_rate\n",
    "        self.model = model\n",
    "        self.discount_factor = discount_factor\n",
    "        self.memory = []\n",
    "        self.count_memory = 0\n",
    "\n",
    "#@save_q_table\n",
    "    def update(self, state, new_state, action, result):\n",
    "        \"\"\"\n",
    "        see QLearner\n",
    "        \"\"\"\n",
    "        self.load_to_memory(state, action, new_state, result)\n",
    "\n",
    "        self.count_memory += 1\n",
    "\n",
    "        #print(self.count_memory)\n",
    "        if self.count_memory == 2:\n",
    "            self.count_memory = 0\n",
    "            # Offline training\n",
    "            self.model.learn_batch(self.memory)\n",
    "            # Online training\n",
    "            #self.learn(self.prev_state, self.prev_move, state,  -1, self.reward)\n",
    "            self.memory = []\n",
    "\n",
    "\n",
    "    def load_to_memory(self, prev_state, prev_move, state, reward):\n",
    "        self.memory.append([prev_state, prev_move, state, reward])\n",
    "\n",
    "    def select_move(self, state, theta=0.1):\n",
    "        \"\"\"\n",
    "        see QLearner\n",
    "        \"\"\"\n",
    "\n",
    "        p = random.uniform(0, 1)\n",
    "\n",
    "        if p > theta:\n",
    "            idx = self.choose_optimal_move(state)\n",
    "            action = self.possible_actions[idx]\n",
    "        else:\n",
    "            action = random.choice(self.possible_actions)\n",
    "\n",
    "        return action # return choosen move\n",
    "\n",
    "\n",
    "    def choose_optimal_move(self, state):\n",
    "\n",
    "        v = -float('Inf')\n",
    "        v_list = []\n",
    "        idx = []\n",
    "        for move in self.possible_actions:\n",
    "            value = self.model.calc_value(state, move)\n",
    "            v_list.append(round(float(value), 5))\n",
    "\n",
    "            if value > v:\n",
    "                v = value\n",
    "                idx = [move]\n",
    "            elif v == value:\n",
    "                idx.append(move)\n",
    "\n",
    "        idx = random.choice(idx)\n",
    "        return idx\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class QUtils:\n",
    "\n",
    "    @staticmethod\n",
    "    def pretty_print_q_table(dict_q_table):\n",
    "        \"\"\"\n",
    "        prints a human readable representation of the given q_table\n",
    "        :param dict_q_table:\n",
    "        \"\"\"\n",
    "        for key, values in dict_q_table.items():\n",
    "            values = np.round_(values, 2)\n",
    "\n",
    "            str_field = \"{a1} | {b1} | {c1}\\n{a2} | {b2} | {c2}\\n{a3} | {b3} | {c3}\"\n",
    "            result = str_field.format(a1=values[0][0], b1=values[1][0], c1=values[2][0],\n",
    "                                  a2=values[0][1], b2=values[1][1], c2=values[2][1],\n",
    "                                  a3=values[0][2], b3=values[1][2], c3=values[2][2])\n",
    "\n",
    "            pretty_key = str(key).replace(\"None\", \"' '\").replace('[[', ' [').replace(']]', ']')\n",
    "            pretty_value = str(values).replace('[[', ' [').replace(']]', ']')\n",
    "            print(f\"{pretty_key}\\n\\n{result}\\n\\n-------------------------\\n\")\n",
    "\n",
    "    @staticmethod\n",
    "    def get_dict_from_file(filepath):\n",
    "        \"\"\"\n",
    "        uses the given filepath to read a dict from the specified file\n",
    "        :param filepath: the file to read in\n",
    "        :return: the dict read from the file\n",
    "        \"\"\"\n",
    "\n",
    "        if os.path.isfile(filepath) and os.path.getsize(filepath) > 0:\n",
    "            with open(filepath, \"rb\") as file:\n",
    "                return dill.load(file)\n",
    "\n",
    "    @staticmethod\n",
    "    def save_dict_to_file(filepath, dict):\n",
    "        \"\"\"\n",
    "        save a dict to a specified file\n",
    "        :param filepath: the file to write to\n",
    "        :param dict: the savable dict\n",
    "        \"\"\"\n",
    "\n",
    "        with open(filepath, 'wb') as file:\n",
    "            dill.dump(dict, file)\n",
    "\n",
    "    @staticmethod\n",
    "    def merge_dicts(d0:dict, d1:dict):\n",
    "        \"\"\"\n",
    "        merges 2 given dicts\n",
    "        :param d0: first dict\n",
    "        :param d1: second dict\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        d = d0.copy()\n",
    "        for k,v in d1.items():\n",
    "            if (k not in d):\n",
    "                d[k] = d1[k]\n",
    "\n",
    "        return d"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "from abc import abstractmethod\n",
    "import os\n",
    "from pathlib import Path\n",
    "import keras.models as Km\n",
    "import keras as K\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\n",
    "class Model:\n",
    "\n",
    "    def __init__(self, tag):\n",
    "        self.tag = tag\n",
    "        self.epsilon = 0.1\n",
    "        self.alpha = 0.5\n",
    "        self.gamma = 1\n",
    "        self.model = self.load_model()\n",
    "\n",
    "    def load_model(self):\n",
    "        \"\"\"\n",
    "        if self.tag == 1:\n",
    "            tag = '_first'\n",
    "        else:\n",
    "            tag = '_second'\n",
    "            \"\"\"\n",
    "       # s = 'model_values' + tag + '.h5'\n",
    "        s = 'model_values.h5'\n",
    "        model_file = Path(s)\n",
    "\n",
    "        if model_file.is_file():\n",
    "            print('load model')\n",
    "            model = Km.load_model(s)\n",
    "            print('load model: ' + s)\n",
    "        else:\n",
    "            model = self.create_model()\n",
    "        return model\n",
    "\n",
    "    @abstractmethod\n",
    "    def create_model(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def state_to_tensor(self, state, move):\n",
    "        pass\n",
    "\n",
    "    def calc_value(self, state, move):\n",
    "        tensor = self.state_to_tensor(state, move)\n",
    "        value = self.model.predict(tensor)\n",
    "        # K.backend.clear_session()\n",
    "        return value\n",
    "\n",
    "    def calc_target(self, prev_state, prev_move, state, reward):\n",
    "\n",
    "        qvalue = self.calc_value(prev_state, prev_move)\n",
    "        v = []\n",
    "        tensor = self.state_to_tensor(prev_state, prev_move)\n",
    "\n",
    "        for move in range(len(tensor[:,0][0])):\n",
    "            v.append(self.calc_value(state, move))\n",
    "\n",
    "        if reward == 0:\n",
    "            v_s_tag = self.gamma * np.max(v)\n",
    "            target = np.array(qvalue + self.alpha * (reward + v_s_tag - qvalue))\n",
    "        else:\n",
    "            # v_s_tag = 0\n",
    "            target = reward\n",
    "\n",
    "        # target = np.array(v_s + self.alpha * (reward + v_s_tag - v_s))\n",
    "\n",
    "        # if self.tag == 1:\n",
    "        #     print('learn general')\n",
    "        #     print(prev_state, prev_move, state, ava_moves, reward)\n",
    "        # print('target: ', target)\n",
    "\n",
    "        return target\n",
    "\n",
    "    def train_model(self, prev_state, prev_move, target, epochs):\n",
    "\n",
    "        tensor = self.state_to_tensor(prev_state, prev_move)\n",
    "\n",
    "        if target is not None:\n",
    "\n",
    "            if self.tag == 1:\n",
    "                print('value before training:', self.model.predict(tensor))\n",
    "            self.model.fit(tensor, target, epochs=epochs, verbose=0)\n",
    "            # K.backend.clear_session()\n",
    "\n",
    "            if self.tag == 1:\n",
    "                print('target:', target)\n",
    "                print('value after training:', self.model.predict(tensor))\n",
    "\n",
    "    def save_model(self):\n",
    "        if self.tag == 1:\n",
    "            tag = '_first'\n",
    "        else:\n",
    "            tag = '_second'\n",
    "        s = 'model_values' + tag + '.h5'\n",
    "\n",
    "        try:\n",
    "            os.remove(s)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        self.model.save(s)\n",
    "\n",
    "    def learn_batch(self, memory):\n",
    "        print('start learning player', self.tag)\n",
    "        print('data length:', len(memory))\n",
    "\n",
    "        # build x_train\n",
    "        ind = 0\n",
    "        #x_train = np.zeros((len(memory), 7, 7, 1))\n",
    "        x_train = np.zeros((len(memory), 2, 9))\n",
    "        for v in memory:\n",
    "            [prev_state, prev_move, _, _] = v\n",
    "            sample = self.state_to_tensor(prev_state, prev_move)\n",
    "            x_train[ind, :, :] = sample\n",
    "            ind += 1\n",
    "\n",
    "        # train with planning\n",
    "        # for i in range(5):\n",
    "        loss = 20\n",
    "        count = 0\n",
    "        while loss > 0.02 and count < 10:\n",
    "            # tic()\n",
    "            y_train = self.create_targets(memory)\n",
    "            # toc()\n",
    "            self.model.fit(x_train, y_train, epochs=5, batch_size=256, verbose=0)\n",
    "            loss = self.model.evaluate(x_train, y_train, batch_size=256, verbose=0)[0]\n",
    "            count += 1\n",
    "            print('planning number:', count, 'loss', loss)\n",
    "\n",
    "        loss = self.model.evaluate(x_train, y_train, batch_size=256, verbose=0)\n",
    "        print('player:', self.tag, loss, 'loops', count)\n",
    "\n",
    "        self.save_model()\n",
    "\n",
    "    def create_targets(self, memory):\n",
    "        y_train_ = np.zeros((len(memory), 1))\n",
    "        count_ = 0\n",
    "        for v_ in memory:\n",
    "            [prev_state_, prev_move_, state_, reward_] = v_\n",
    "            target = self.calc_target(prev_state_, prev_move_, state_, reward_)\n",
    "            y_train_[count_, :] = target\n",
    "            count_ += 1\n",
    "\n",
    "            # print('---------')\n",
    "            # print('player', self.tag)\n",
    "            # print('prev state', prev_state_)\n",
    "            # print('prev move', prev_move_)\n",
    "            # print('state', state_)\n",
    "            # print('ava moves', ava_moves_)\n",
    "            # print('reward', reward_)\n",
    "            # print('target', target)\n",
    "            #\n",
    "            # value = self.calc_value(prev_state_, prev_move_)\n",
    "            # print('value through net', value)\n",
    "            # time.sleep(0.2)\n",
    "\n",
    "        return y_train_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras.layers as kl\n",
    "import keras.models as km\n",
    "import keras.optimizers as ko\n",
    "\n",
    "\n",
    "class TicTacToeModel(Model):\n",
    "\n",
    "    def __init__(self, tag):\n",
    "        super().__init__(tag)\n",
    "        pass\n",
    "\n",
    "    def create_model(self):\n",
    "        print('new model')\n",
    "\n",
    "        #model = km.load_model(\"qnn_model\")\n",
    "\n",
    "        model = Km.Sequential()\n",
    "        model.add(kl.Flatten(input_shape=(2, 9)))\n",
    "        model.add(kl.Dense(18))\n",
    "        model.add(kl.LeakyReLU(alpha=0.3))\n",
    "        model.add(kl.Dense(18))\n",
    "        model.add(kl.LeakyReLU(alpha=0.3))\n",
    "        model.add(kl.Dense(18))\n",
    "        model.add(kl.LeakyReLU(alpha=0.3))\n",
    "        model.add(kl.Dense(18))\n",
    "        model.add(kl.LeakyReLU(alpha=0.3))\n",
    "        model.add(kl.Dense(18))\n",
    "        model.add(kl.LeakyReLU(alpha=0.3))\n",
    "        model.add(kl.Dense(18))\n",
    "        model.add(kl.LeakyReLU(alpha=0.3))\n",
    "        model.add(kl.Dense(18))\n",
    "        model.add(kl.LeakyReLU(alpha=0.3))\n",
    "        model.add(kl.Dense(9))\n",
    "        model.add(kl.LeakyReLU(alpha=0.3))\n",
    "        model.add(kl.Dense(1, activation='linear'))\n",
    "\n",
    "        # adam = ko.Adam(lr=0.001)\n",
    "\n",
    "        model.compile(optimizer='Adam', loss='mean_absolute_error', metrics=['accuracy'])\n",
    "        #model.save(\"qnn_model\")\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        return model\n",
    "\n",
    "    def state_to_tensor(self, state, move):\n",
    "\n",
    "        state = np.array(state)\n",
    "        state = state.flatten()\n",
    "        state = self.one_hot_encode_state(state)\n",
    "\n",
    "        a = np.zeros(9)\n",
    "        a = np.asarray(a).astype('float32')\n",
    "        a[move] = 1\n",
    "\n",
    "        state = np.asarray(state).astype('float32')\n",
    "        tensor = np.array((a, state))\n",
    "        #print(tensor)\n",
    "        tensor = tensor.reshape((1, 2, 9))\n",
    "\n",
    "        return tensor\n",
    "\n",
    "    def one_hot_encode_state(self, state):\n",
    "        for i in range(len(state)):\n",
    "            if state[i] is None:\n",
    "                state[i] = 0\n",
    "            if state[i] == 'x':\n",
    "                state[i] = 1\n",
    "            if state[i] == 'o':\n",
    "                state[i] = -1\n",
    "\n",
    "        return state"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "from abc import abstractmethod\n",
    "import random\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "class Agent:\n",
    "\n",
    "    def __init__(self, tag, exploration_factor=1):\n",
    "        self.tag = tag\n",
    "        self.exp_factor = exploration_factor\n",
    "        self.prev_state = np.zeros(9)\n",
    "        self.prev_move = -1\n",
    "        self.state = None\n",
    "        self.move = None\n",
    "        self.print_value = False\n",
    "        self.model = Model(self.tag)\n",
    "        self.memory = []\n",
    "        self.count_memory = 0\n",
    "        self.winner_flag = False\n",
    "\n",
    "    def choose_move(self, state, winner, learn):\n",
    "\n",
    "        self.load_to_memory(self.prev_state, self.prev_move, state, self.ava_moves(state), self.reward(winner))\n",
    "\n",
    "        if winner is not None:\n",
    "\n",
    "            self.count_memory += 1\n",
    "\n",
    "            self.prev_state = np.zeros(9)\n",
    "            self.prev_move = -1\n",
    "            print(self.count_memory)\n",
    "            if learn is True and self.count_memory == 2:\n",
    "                self.count_memory = 0\n",
    "                # Offline training\n",
    "                self.model.learn_batch(self.memory)\n",
    "                self.memory = []\n",
    "                # Online training\n",
    "                #self.learn(self.prev_state, self.prev_move, state, self.ava_moves(state),  -1, self.reward(winner))\n",
    "            return None\n",
    "\n",
    "        p = random.uniform(0, 1)\n",
    "\n",
    "        if p < self.exp_factor:\n",
    "            idx = self.choose_optimal_move(state)\n",
    "        else:\n",
    "            ava_moves = self.ava_moves(state)\n",
    "            idx = random.choice(ava_moves)\n",
    "\n",
    "        self.prev_state = state\n",
    "        self.prev_move = idx\n",
    "\n",
    "        return idx\n",
    "\n",
    "    def choose_optimal_move(self, state):\n",
    "\n",
    "        ava_moves = self.ava_moves(state)\n",
    "        v = -float('Inf')\n",
    "        v_list = []\n",
    "        idx = []\n",
    "        for move in ava_moves:\n",
    "            value = self.model.calc_value(state, move)\n",
    "            v_list.append(round(float(value), 5))\n",
    "\n",
    "            if value > v:\n",
    "                v = value\n",
    "                idx = [move]\n",
    "            elif v == value:\n",
    "                idx.append(move)\n",
    "\n",
    "        idx = random.choice(idx)\n",
    "        return idx\n",
    "\n",
    "    def learn(self, prev_state, prev_move, state, ava_moves, move, reward):\n",
    "\n",
    "        if prev_move != -1:\n",
    "\n",
    "            target = self.model.calc_target(prev_state, prev_move, state, reward)\n",
    "            #print(target)\n",
    "            self.model.train_model(prev_state, prev_move, target, 1)\n",
    "\n",
    "    @abstractmethod\n",
    "    def ava_moves(self, state):\n",
    "        pass\n",
    "\n",
    "    def load_to_memory(self, prev_state, prev_move, state, ava_moves, reward):\n",
    "        self.memory.append([prev_state, prev_move, state, ava_moves, reward])\n",
    "\n",
    "    def save_memory(self):\n",
    "        is_file_ = True\n",
    "        count = 1\n",
    "        s = ''\n",
    "        while is_file_:\n",
    "            s = 'data4/value_list_' + str(self.tag) + '_' + str(count) + '.pkl'\n",
    "            if Path(s).is_file():\n",
    "                is_file_ = True\n",
    "                count = count + 1\n",
    "            else:\n",
    "                is_file_ = False\n",
    "\n",
    "        with open(s, 'wb') as output:\n",
    "            pickle.dump(self.memory, output)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model\n",
      "load model: model_values_first.h5\n",
      "load model\n",
      "load model: model_values_first.h5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    C:\\Users\\ke-ch\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1478 predict_function  *\n        return step_function(self, iterator)\n    C:\\Users\\ke-ch\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1468 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\ke-ch\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1259 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\ke-ch\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2730 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\ke-ch\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3417 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\ke-ch\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1461 run_step  **\n        outputs = model.predict_step(data)\n    C:\\Users\\ke-ch\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1434 predict_step\n        return self(x, training=False)\n    C:\\Users\\ke-ch\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:998 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    C:\\Users\\ke-ch\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py:234 assert_input_compatibility\n        raise ValueError('Input ' + str(input_index) + ' of layer ' +\n\n    ValueError: Input 0 of layer sequential_7 is incompatible with the layer: : expected min_ndim=4, found ndim=3. Full shape received: (None, 2, 9)\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-48-cd59e7a33404>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      8\u001B[0m     \u001B[0mkm\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mload_model\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      9\u001B[0m     \u001B[1;31m#print(tensor)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 10\u001B[1;33m     \u001B[0mvalue\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mi\u001B[0m\u001B[1;33m/\u001B[0m\u001B[1;36m3\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mi\u001B[0m\u001B[1;33m%\u001B[0m\u001B[1;36m3\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mkm\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpredict\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtensor\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     11\u001B[0m     \u001B[0mpstate\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mi\u001B[0m\u001B[1;33m/\u001B[0m\u001B[1;36m3\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mi\u001B[0m\u001B[1;33m%\u001B[0m\u001B[1;36m3\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mstate\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mi\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     12\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001B[0m in \u001B[0;36mpredict\u001B[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[0;32m   1627\u001B[0m           \u001B[1;32mfor\u001B[0m \u001B[0mstep\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mdata_handler\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msteps\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1628\u001B[0m             \u001B[0mcallbacks\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mon_predict_batch_begin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mstep\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1629\u001B[1;33m             \u001B[0mtmp_batch_outputs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpredict_function\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0miterator\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1630\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mdata_handler\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshould_sync\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1631\u001B[0m               \u001B[0mcontext\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0masync_wait\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m    826\u001B[0m     \u001B[0mtracing_count\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexperimental_get_tracing_count\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    827\u001B[0m     \u001B[1;32mwith\u001B[0m \u001B[0mtrace\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mTrace\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_name\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mtm\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 828\u001B[1;33m       \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    829\u001B[0m       \u001B[0mcompiler\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34m\"xla\"\u001B[0m \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_experimental_compile\u001B[0m \u001B[1;32melse\u001B[0m \u001B[1;34m\"nonXla\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    830\u001B[0m       \u001B[0mnew_tracing_count\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexperimental_get_tracing_count\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001B[0m in \u001B[0;36m_call\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m    869\u001B[0m       \u001B[1;31m# This is the first call of __call__, so we have to initialize.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    870\u001B[0m       \u001B[0minitializers\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 871\u001B[1;33m       \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_initialize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwds\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0madd_initializers_to\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0minitializers\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    872\u001B[0m     \u001B[1;32mfinally\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    873\u001B[0m       \u001B[1;31m# At this point we know that the initialization is complete (or less\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001B[0m in \u001B[0;36m_initialize\u001B[1;34m(self, args, kwds, add_initializers_to)\u001B[0m\n\u001B[0;32m    723\u001B[0m     \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_graph_deleter\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mFunctionDeleter\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_lifted_initializer_graph\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    724\u001B[0m     self._concrete_stateful_fn = (\n\u001B[1;32m--> 725\u001B[1;33m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001B[0m\u001B[0;32m    726\u001B[0m             *args, **kwds))\n\u001B[0;32m    727\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001B[0m in \u001B[0;36m_get_concrete_function_internal_garbage_collected\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   2967\u001B[0m       \u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwargs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2968\u001B[0m     \u001B[1;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_lock\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2969\u001B[1;33m       \u001B[0mgraph_function\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0m_\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_maybe_define_function\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   2970\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0mgraph_function\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2971\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001B[0m in \u001B[0;36m_maybe_define_function\u001B[1;34m(self, args, kwargs)\u001B[0m\n\u001B[0;32m   3359\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3360\u001B[0m           \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_function_cache\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmissed\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0madd\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcall_context_key\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 3361\u001B[1;33m           \u001B[0mgraph_function\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_create_graph_function\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   3362\u001B[0m           \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_function_cache\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mprimary\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mcache_key\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mgraph_function\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3363\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001B[0m in \u001B[0;36m_create_graph_function\u001B[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001B[0m\n\u001B[0;32m   3194\u001B[0m     \u001B[0marg_names\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mbase_arg_names\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0mmissing_arg_names\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3195\u001B[0m     graph_function = ConcreteFunction(\n\u001B[1;32m-> 3196\u001B[1;33m         func_graph_module.func_graph_from_py_func(\n\u001B[0m\u001B[0;32m   3197\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_name\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3198\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_python_function\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001B[0m in \u001B[0;36mfunc_graph_from_py_func\u001B[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001B[0m\n\u001B[0;32m    988\u001B[0m         \u001B[0m_\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0moriginal_func\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtf_decorator\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0munwrap\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpython_func\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    989\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 990\u001B[1;33m       \u001B[0mfunc_outputs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpython_func\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0mfunc_args\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mfunc_kwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    991\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    992\u001B[0m       \u001B[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001B[0m in \u001B[0;36mwrapped_fn\u001B[1;34m(*args, **kwds)\u001B[0m\n\u001B[0;32m    632\u001B[0m             \u001B[0mxla_context\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mExit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    633\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 634\u001B[1;33m           \u001B[0mout\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mweak_wrapped_fn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__wrapped__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    635\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mout\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    636\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001B[0m in \u001B[0;36mwrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    975\u001B[0m           \u001B[1;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m  \u001B[1;31m# pylint:disable=broad-except\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    976\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mhasattr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"ag_error_metadata\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 977\u001B[1;33m               \u001B[1;32mraise\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mag_error_metadata\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto_exception\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    978\u001B[0m             \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    979\u001B[0m               \u001B[1;32mraise\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mValueError\u001B[0m: in user code:\n\n    C:\\Users\\ke-ch\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1478 predict_function  *\n        return step_function(self, iterator)\n    C:\\Users\\ke-ch\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1468 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\ke-ch\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1259 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\ke-ch\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2730 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\ke-ch\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3417 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\ke-ch\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1461 run_step  **\n        outputs = model.predict_step(data)\n    C:\\Users\\ke-ch\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1434 predict_step\n        return self(x, training=False)\n    C:\\Users\\ke-ch\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:998 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    C:\\Users\\ke-ch\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py:234 assert_input_compatibility\n        raise ValueError('Input ' + str(input_index) + ' of layer ' +\n\n    ValueError: Input 0 of layer sequential_7 is incompatible with the layer: : expected min_ndim=4, found ndim=3. Full shape received: (None, 2, 9)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "model = TicTacToeModel(1)\n",
    "value = np.ndarray(shape=[3,3])\n",
    "pstate = np.ndarray(shape=[3,3])\n",
    "state = [-1,0,-1,0,1,0,0,1,0]\n",
    "for i in [0,1,2,3,4,5,6,7,8]:\n",
    "    tensor = model.state_to_tensor(state, i)\n",
    "    km = model.load_model()\n",
    "    #print(tensor)\n",
    "    value[int(i/3)][i%3] = km.predict(tensor)\n",
    "    pstate[int(i/3)][i%3] = state[i]\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.35171202 -0.39011419 -0.36668181]\n",
      " [-0.3843089  -0.35754976 -0.40643951]\n",
      " [-0.26949695 -0.39932159 -0.33464077]]\n",
      "[[-1.  0. -1.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]]\n",
      "Optimal: 6\n",
      "[[-1.  0. -1.]\n",
      " [ 0.  1.  0.]\n",
      " [ 1.  1.  0.]]\n"
     ]
    }
   ],
   "source": [
    "for i in [0,1,2,3,4,5,6,7,8]:\n",
    "    pstate[int(i/3)][i%3] = state[i]\n",
    "print(value)\n",
    "print(pstate)\n",
    "print(f\"Optimal: {np.argmax(value)}\")\n",
    "newstate = pstate\n",
    "newstate[int(np.argmax(value)/3)][np.argmax(value)%3] = 1\n",
    "print(newstate)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "from Game import Game\n",
    "from tqdm.notebook import trange"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/100 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "aa625dd767d54dd19a6a25abace82b20"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new model\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 18)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 18)                342       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 18)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 18)                342       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 18)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 18)                342       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 18)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 18)                342       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 18)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 18)                342       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 18)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 18)                342       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 18)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 18)                342       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 18)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 9)                 171       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 9)                 0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 10        \n",
      "=================================================================\n",
      "Total params: 2,575\n",
      "Trainable params: 2,575\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "new model\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 18)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 18)                342       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 18)                0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 18)                342       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 18)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 18)                342       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 18)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 18)                342       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 18)                0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 18)                342       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)   (None, 18)                0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 18)                342       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)   (None, 18)                0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 18)                342       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)   (None, 18)                0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 9)                 171       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)   (None, 9)                 0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1)                 10        \n",
      "=================================================================\n",
      "Total params: 2,575\n",
      "Trainable params: 2,575\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [2, 2]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 1]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [3, 2]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 2]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [3, 2]\n",
      "start learning player 1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.010565971955657005\n",
      "player: 1 [0.010565971955657005, 0.0] loops 1\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [2, 2]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [2, 2]\n",
      "start learning player 1\n",
      "data length: 2\n",
      "planning number: 1 loss 99.88724517822266\n",
      "planning number: 2 loss 99.74821472167969\n",
      "planning number: 3 loss 99.53153991699219\n",
      "planning number: 4 loss 99.17483520507812\n",
      "planning number: 5 loss 98.58491516113281\n",
      "planning number: 6 loss 97.63367462158203\n",
      "planning number: 7 loss 96.10067749023438\n",
      "planning number: 8 loss 93.67176818847656\n",
      "planning number: 9 loss 89.75648498535156\n",
      "planning number: 10 loss 83.44593048095703\n",
      "player: 1 [83.44593048095703, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 1]\n",
      "start learning player -1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.007340066134929657\n",
      "player: -1 [0.007340066134929657, 0.0] loops 1\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 1]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 2]\n",
      "start learning player -1\n",
      "data length: 2\n",
      "planning number: 1 loss 99.6292724609375\n",
      "planning number: 2 loss 99.32491302490234\n",
      "planning number: 3 loss 98.77740478515625\n",
      "planning number: 4 loss 97.83993530273438\n",
      "planning number: 5 loss 96.26551818847656\n",
      "planning number: 6 loss 93.6323471069336\n",
      "planning number: 7 loss 89.2695541381836\n",
      "planning number: 8 loss 82.09380340576172\n",
      "planning number: 9 loss 70.31886291503906\n",
      "planning number: 10 loss 51.110652923583984\n",
      "player: -1 [51.110652923583984, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 2]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 2]\n",
      "start learning player -1\n",
      "data length: 2\n",
      "planning number: 1 loss 30.72295379638672\n",
      "planning number: 2 loss 9.295440673828125\n",
      "planning number: 3 loss 9.526153564453125\n",
      "planning number: 4 loss 9.298294067382812\n",
      "planning number: 5 loss 4.35296630859375\n",
      "planning number: 6 loss 6.8377685546875\n",
      "planning number: 7 loss 3.4620590209960938\n",
      "planning number: 8 loss 0.43714141845703125\n",
      "planning number: 9 loss 0.6551589965820312\n",
      "planning number: 10 loss 0.7578048706054688\n",
      "player: -1 [0.7578048706054688, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 2]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 2]\n",
      "start learning player -1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.075592041015625\n",
      "planning number: 2 loss 1.7188186645507812\n",
      "planning number: 3 loss 1.79156494140625\n",
      "planning number: 4 loss 0.05682373046875\n",
      "planning number: 5 loss 1.6987762451171875\n",
      "planning number: 6 loss 1.4801025390625\n",
      "planning number: 7 loss 0.42037200927734375\n",
      "planning number: 8 loss 0.8965530395507812\n",
      "planning number: 9 loss 1.2553558349609375\n",
      "planning number: 10 loss 1.7578964233398438\n",
      "player: -1 [1.7578964233398438, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 2]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 2]\n",
      "start learning player -1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.23241424560546875\n",
      "planning number: 2 loss 0.7611465454101562\n",
      "planning number: 3 loss 0.2212371826171875\n",
      "planning number: 4 loss 0.04381561279296875\n",
      "planning number: 5 loss 1.1648330688476562\n",
      "planning number: 6 loss 0.14571380615234375\n",
      "planning number: 7 loss 0.5285415649414062\n",
      "planning number: 8 loss 0.8044815063476562\n",
      "planning number: 9 loss 0.5454330444335938\n",
      "planning number: 10 loss 0.3638916015625\n",
      "player: -1 [0.3638916015625, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 2]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 2]\n",
      "start learning player -1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.6436767578125\n",
      "planning number: 2 loss 0.25028228759765625\n",
      "planning number: 3 loss 0.8566741943359375\n",
      "planning number: 4 loss 0.28379058837890625\n",
      "planning number: 5 loss 0.6023941040039062\n",
      "planning number: 6 loss 0.0565338134765625\n",
      "planning number: 7 loss 0.85589599609375\n",
      "planning number: 8 loss 0.0477142333984375\n",
      "planning number: 9 loss 0.7969207763671875\n",
      "planning number: 10 loss 0.09786224365234375\n",
      "player: -1 [0.09786224365234375, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 2]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [3, 1]\n",
      "start learning player -1\n",
      "data length: 2\n",
      "planning number: 1 loss 2.792999267578125\n",
      "planning number: 2 loss 3.780517578125\n",
      "planning number: 3 loss 1.3119277954101562\n",
      "planning number: 4 loss 2.1570663452148438\n",
      "planning number: 5 loss 1.9159088134765625\n",
      "planning number: 6 loss 2.0902557373046875\n",
      "planning number: 7 loss 1.5734634399414062\n",
      "planning number: 8 loss 0.23700714111328125\n",
      "planning number: 9 loss 1.1620254516601562\n",
      "planning number: 10 loss 2.00164794921875\n",
      "player: -1 [2.00164794921875, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "start learning player 1\n",
      "data length: 2\n",
      "planning number: 1 loss 40.974571228027344\n",
      "planning number: 2 loss 36.53585433959961\n",
      "planning number: 3 loss 31.980453491210938\n",
      "planning number: 4 loss 27.942710876464844\n",
      "planning number: 5 loss 23.85169792175293\n",
      "planning number: 6 loss 17.32305145263672\n",
      "planning number: 7 loss 11.486064910888672\n",
      "planning number: 8 loss 2.498790740966797\n",
      "planning number: 9 loss 1.332672119140625\n",
      "planning number: 10 loss 0.4288444519042969\n",
      "player: 1 [0.4288444519042969, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "start learning player 1\n",
      "data length: 2\n",
      "planning number: 1 loss 1.0772171020507812\n",
      "planning number: 2 loss 12.133209228515625\n",
      "planning number: 3 loss 7.566383361816406\n",
      "planning number: 4 loss 6.0313262939453125\n",
      "planning number: 5 loss 3.7807693481445312\n",
      "planning number: 6 loss 4.606452941894531\n",
      "planning number: 7 loss 3.7038345336914062\n",
      "planning number: 8 loss 1.098541259765625\n",
      "planning number: 9 loss 1.3883209228515625\n",
      "planning number: 10 loss 1.791229248046875\n",
      "player: 1 [1.791229248046875, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "start learning player 1\n",
      "data length: 2\n",
      "planning number: 1 loss 2.6751556396484375\n",
      "planning number: 2 loss 1.4738082885742188\n",
      "planning number: 3 loss 2.1644973754882812\n",
      "planning number: 4 loss 2.1027908325195312\n",
      "planning number: 5 loss 1.15765380859375\n",
      "planning number: 6 loss 0.6366119384765625\n",
      "planning number: 7 loss 0.32906341552734375\n",
      "planning number: 8 loss 0.26146697998046875\n",
      "planning number: 9 loss 0.3370819091796875\n",
      "planning number: 10 loss 0.0429840087890625\n",
      "player: 1 [0.0429840087890625, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "start learning player 1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.2428436279296875\n",
      "planning number: 2 loss 0.00705718994140625\n",
      "player: 1 [0.00705718994140625, 0.0] loops 2\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "start learning player 1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.34355926513671875\n",
      "planning number: 2 loss 0.13970184326171875\n",
      "planning number: 3 loss 0.6125640869140625\n",
      "planning number: 4 loss 0.5640716552734375\n",
      "planning number: 5 loss 0.1685943603515625\n",
      "planning number: 6 loss 0.197967529296875\n",
      "planning number: 7 loss 0.3986358642578125\n",
      "planning number: 8 loss 0.27623748779296875\n",
      "planning number: 9 loss 0.47620391845703125\n",
      "planning number: 10 loss 0.5288848876953125\n",
      "player: 1 [0.5288848876953125, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [2, 1]\n",
      "start learning player 1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.6912384033203125\n",
      "planning number: 2 loss 0.8353729248046875\n",
      "planning number: 3 loss 1.5057830810546875\n",
      "planning number: 4 loss 1.385711669921875\n",
      "planning number: 5 loss 0.21392822265625\n",
      "planning number: 6 loss 0.3238525390625\n",
      "planning number: 7 loss 0.295074462890625\n",
      "planning number: 8 loss 1.0853347778320312\n",
      "planning number: 9 loss 1.79241943359375\n",
      "planning number: 10 loss 0.28922271728515625\n",
      "player: 1 [0.28922271728515625, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 2]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 2]\n",
      "start learning player -1\n",
      "data length: 2\n",
      "planning number: 1 loss 3.3570327758789062\n",
      "planning number: 2 loss 5.6952362060546875\n",
      "planning number: 3 loss 7.424160003662109\n",
      "planning number: 4 loss 8.499439239501953\n",
      "planning number: 5 loss 9.186103820800781\n",
      "planning number: 6 loss 10.345458984375\n",
      "planning number: 7 loss 10.689838409423828\n",
      "planning number: 8 loss 9.885284423828125\n",
      "planning number: 9 loss 8.898849487304688\n",
      "planning number: 10 loss 7.451267242431641\n",
      "player: -1 [7.451267242431641, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [3, 1]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [3, 1]\n",
      "start learning player -1\n",
      "data length: 2\n",
      "planning number: 1 loss 3.463958740234375\n",
      "planning number: 2 loss 7.7010040283203125\n",
      "planning number: 3 loss 3.2005767822265625\n",
      "planning number: 4 loss 2.9864349365234375\n",
      "planning number: 5 loss 2.1015968322753906\n",
      "planning number: 6 loss 1.2405319213867188\n",
      "planning number: 7 loss 0.33263397216796875\n",
      "planning number: 8 loss 0.2860374450683594\n",
      "planning number: 9 loss 0.22917938232421875\n",
      "planning number: 10 loss 0.029453277587890625\n",
      "player: -1 [0.029453277587890625, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [2, 1]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [2, 1]\n",
      "start learning player -1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.4888954162597656\n",
      "planning number: 2 loss 0.4803009033203125\n",
      "planning number: 3 loss 0.8356056213378906\n",
      "planning number: 4 loss 0.3582801818847656\n",
      "planning number: 5 loss 0.6483268737792969\n",
      "planning number: 6 loss 0.3906822204589844\n",
      "planning number: 7 loss 0.16245269775390625\n",
      "planning number: 8 loss 0.39043426513671875\n",
      "planning number: 9 loss 1.2292556762695312\n",
      "planning number: 10 loss 0.3201560974121094\n",
      "player: -1 [0.3201560974121094, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [2, 2]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [2, 2]\n",
      "start learning player -1\n",
      "data length: 2\n",
      "planning number: 1 loss 1.7813568115234375\n",
      "planning number: 2 loss 1.8296356201171875\n",
      "planning number: 3 loss 1.45074462890625\n",
      "planning number: 4 loss 0.2743492126464844\n",
      "planning number: 5 loss 0.280548095703125\n",
      "planning number: 6 loss 0.8247795104980469\n",
      "planning number: 7 loss 0.22266006469726562\n",
      "planning number: 8 loss 0.18563079833984375\n",
      "planning number: 9 loss 0.17159271240234375\n",
      "planning number: 10 loss 0.20080184936523438\n",
      "player: -1 [0.20080184936523438, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [3, 1]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [3, 1]\n",
      "start learning player -1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.8602714538574219\n",
      "planning number: 2 loss 0.3289299011230469\n",
      "planning number: 3 loss 0.7429008483886719\n",
      "planning number: 4 loss 0.3590202331542969\n",
      "planning number: 5 loss 0.30539703369140625\n",
      "planning number: 6 loss 1.1239013671875\n",
      "planning number: 7 loss 0.21254348754882812\n",
      "planning number: 8 loss 0.04073333740234375\n",
      "planning number: 9 loss 0.39737701416015625\n",
      "planning number: 10 loss 0.31938934326171875\n",
      "player: -1 [0.31938934326171875, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [2, 2]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [2, 2]\n",
      "start learning player -1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.062236785888671875\n",
      "planning number: 2 loss 1.2790946960449219\n",
      "planning number: 3 loss 0.2663154602050781\n",
      "planning number: 4 loss 1.3148574829101562\n",
      "planning number: 5 loss 0.7732124328613281\n",
      "planning number: 6 loss 0.222991943359375\n",
      "planning number: 7 loss 0.5380973815917969\n",
      "planning number: 8 loss 0.4798622131347656\n",
      "planning number: 9 loss 0.5904006958007812\n",
      "planning number: 10 loss 0.5627517700195312\n",
      "player: -1 [0.5627517700195312, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [2, 2]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [2, 2]\n",
      "start learning player -1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.36101531982421875\n",
      "planning number: 2 loss 0.40966796875\n",
      "planning number: 3 loss 0.36717987060546875\n",
      "planning number: 4 loss 0.3770294189453125\n",
      "planning number: 5 loss 0.40607452392578125\n",
      "planning number: 6 loss 0.20737457275390625\n",
      "planning number: 7 loss 0.0817413330078125\n",
      "planning number: 8 loss 0.6966323852539062\n",
      "planning number: 9 loss 1.474029541015625\n",
      "planning number: 10 loss 0.7740478515625\n",
      "player: -1 [0.7740478515625, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [2, 2]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [2, 2]\n",
      "start learning player -1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.20055389404296875\n",
      "planning number: 2 loss 0.10003662109375\n",
      "planning number: 3 loss 1.2168655395507812\n",
      "planning number: 4 loss 1.7352828979492188\n",
      "planning number: 5 loss 0.48361968994140625\n",
      "planning number: 6 loss 0.5108489990234375\n",
      "planning number: 7 loss 0.5236740112304688\n",
      "planning number: 8 loss 0.3874969482421875\n",
      "planning number: 9 loss 0.8006210327148438\n",
      "planning number: 10 loss 0.6171646118164062\n",
      "player: -1 [0.6171646118164062, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [2, 2]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [2, 2]\n",
      "start learning player -1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.26030731201171875\n",
      "planning number: 2 loss 0.52117919921875\n",
      "planning number: 3 loss 0.1087493896484375\n",
      "planning number: 4 loss 0.00379180908203125\n",
      "player: -1 [0.00379180908203125, 0.0] loops 4\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [2, 2]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [2, 2]\n",
      "start learning player -1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.4057464599609375\n",
      "planning number: 2 loss 0.3824615478515625\n",
      "planning number: 3 loss 0.7567291259765625\n",
      "planning number: 4 loss 0.82647705078125\n",
      "planning number: 5 loss 0.74285888671875\n",
      "planning number: 6 loss 0.9469757080078125\n",
      "planning number: 7 loss 1.177764892578125\n",
      "planning number: 8 loss 0.40370941162109375\n",
      "planning number: 9 loss 0.09497833251953125\n",
      "planning number: 10 loss 0.8927764892578125\n",
      "player: -1 [0.8927764892578125, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [2, 1]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [2, 2]\n",
      "start learning player -1\n",
      "data length: 2\n",
      "planning number: 1 loss 1.0242347717285156\n",
      "planning number: 2 loss 1.4710655212402344\n",
      "planning number: 3 loss 0.37935638427734375\n",
      "planning number: 4 loss 0.9009666442871094\n",
      "planning number: 5 loss 1.0214385986328125\n",
      "planning number: 6 loss 0.21945953369140625\n",
      "planning number: 7 loss 0.177093505859375\n",
      "planning number: 8 loss 1.0900192260742188\n",
      "planning number: 9 loss 1.598236083984375\n",
      "planning number: 10 loss 0.47429656982421875\n",
      "player: -1 [0.47429656982421875, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [2, 2]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [2, 2]\n",
      "start learning player -1\n",
      "data length: 2\n",
      "planning number: 1 loss 1.3492431640625\n",
      "planning number: 2 loss 1.9613189697265625\n",
      "planning number: 3 loss 2.4341506958007812\n",
      "planning number: 4 loss 1.65814208984375\n",
      "planning number: 5 loss 2.290679931640625\n",
      "planning number: 6 loss 2.0299911499023438\n",
      "planning number: 7 loss 1.72479248046875\n",
      "planning number: 8 loss 2.62841796875\n",
      "planning number: 9 loss 1.0230712890625\n",
      "planning number: 10 loss 0.04831695556640625\n",
      "player: -1 [0.04831695556640625, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [2, 2]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [2, 2]\n",
      "start learning player -1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.13897705078125\n",
      "planning number: 2 loss 0.2692718505859375\n",
      "planning number: 3 loss 0.02660369873046875\n",
      "planning number: 4 loss 0.238983154296875\n",
      "planning number: 5 loss 0.023193359375\n",
      "planning number: 6 loss 0.5205612182617188\n",
      "planning number: 7 loss 0.257781982421875\n",
      "planning number: 8 loss 0.47757720947265625\n",
      "planning number: 9 loss 0.4697418212890625\n",
      "planning number: 10 loss 0.1820220947265625\n",
      "player: -1 [0.1820220947265625, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [2, 2]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [2, 2]\n",
      "start learning player -1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.19390106201171875\n",
      "planning number: 2 loss 0.3760223388671875\n",
      "planning number: 3 loss 0.2722015380859375\n",
      "planning number: 4 loss 0.43106842041015625\n",
      "planning number: 5 loss 0.4759521484375\n",
      "planning number: 6 loss 0.06966400146484375\n",
      "planning number: 7 loss 0.04467010498046875\n",
      "planning number: 8 loss 0.259185791015625\n",
      "planning number: 9 loss 0.0005035400390625\n",
      "player: -1 [0.0005035400390625, 0.0] loops 9\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [2, 2]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [2, 2]\n",
      "start learning player -1\n",
      "data length: 2\n",
      "planning number: 1 loss 1.4776458740234375\n",
      "planning number: 2 loss 1.4546890258789062\n",
      "planning number: 3 loss 0.2171173095703125\n",
      "planning number: 4 loss 0.7657470703125\n",
      "planning number: 5 loss 0.088775634765625\n",
      "planning number: 6 loss 0.1519775390625\n",
      "planning number: 7 loss 0.9650039672851562\n",
      "planning number: 8 loss 0.05402374267578125\n",
      "planning number: 9 loss 0.5524749755859375\n",
      "planning number: 10 loss 0.7961959838867188\n",
      "player: -1 [0.7961959838867188, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [2, 2]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [2, 2]\n",
      "start learning player -1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.5398330688476562\n",
      "planning number: 2 loss 0.334136962890625\n",
      "planning number: 3 loss 0.6163558959960938\n",
      "planning number: 4 loss 0.257293701171875\n",
      "planning number: 5 loss 0.7900466918945312\n",
      "planning number: 6 loss 0.2315673828125\n",
      "planning number: 7 loss 0.8620681762695312\n",
      "planning number: 8 loss 0.20639801025390625\n",
      "planning number: 9 loss 0.0467071533203125\n",
      "planning number: 10 loss 1.0111083984375\n",
      "player: -1 [1.0111083984375, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [2, 2]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [2, 2]\n",
      "start learning player -1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.0151519775390625\n",
      "player: -1 [0.0151519775390625, 0.0] loops 1\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [2, 2]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 2]\n",
      "start learning player -1\n",
      "data length: 2\n",
      "planning number: 1 loss 1.095001220703125\n",
      "planning number: 2 loss 1.8296585083007812\n",
      "planning number: 3 loss 0.38594818115234375\n",
      "planning number: 4 loss 0.32000732421875\n",
      "planning number: 5 loss 0.4881439208984375\n",
      "planning number: 6 loss 0.045501708984375\n",
      "planning number: 7 loss 1.440338134765625\n",
      "planning number: 8 loss 1.25897216796875\n",
      "planning number: 9 loss 0.2777252197265625\n",
      "planning number: 10 loss 0.6489639282226562\n",
      "player: -1 [0.6489639282226562, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [2, 2]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [2, 2]\n",
      "start learning player -1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.3790130615234375\n",
      "planning number: 2 loss 0.4916419982910156\n",
      "planning number: 3 loss 0.1520233154296875\n",
      "planning number: 4 loss 0.3084602355957031\n",
      "planning number: 5 loss 0.12893295288085938\n",
      "planning number: 6 loss 0.4947967529296875\n",
      "planning number: 7 loss 0.411346435546875\n",
      "planning number: 8 loss 0.07773971557617188\n",
      "planning number: 9 loss 0.16218185424804688\n",
      "planning number: 10 loss 0.3151283264160156\n",
      "player: -1 [0.3151283264160156, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [2, 1]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [2, 1]\n",
      "start learning player -1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.065826416015625\n",
      "planning number: 2 loss 0.16714096069335938\n",
      "planning number: 3 loss 0.22903823852539062\n",
      "planning number: 4 loss 0.3220481872558594\n",
      "planning number: 5 loss 0.39293670654296875\n",
      "planning number: 6 loss 0.1177825927734375\n",
      "planning number: 7 loss 0.07631301879882812\n",
      "planning number: 8 loss 1.1181640625\n",
      "planning number: 9 loss 1.5708999633789062\n",
      "planning number: 10 loss 0.4442939758300781\n",
      "player: -1 [0.4442939758300781, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 2]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 2]\n",
      "start learning player -1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.4788780212402344\n",
      "planning number: 2 loss 0.5430030822753906\n",
      "planning number: 3 loss 0.2595024108886719\n",
      "planning number: 4 loss 0.6167716979980469\n",
      "planning number: 5 loss 0.4303627014160156\n",
      "planning number: 6 loss 0.3945198059082031\n",
      "planning number: 7 loss 0.6494255065917969\n",
      "planning number: 8 loss 0.2803306579589844\n",
      "planning number: 9 loss 0.2841796875\n",
      "planning number: 10 loss 0.22440338134765625\n",
      "player: -1 [0.22440338134765625, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [2, 2]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [2, 1]\n",
      "start learning player -1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.6974258422851562\n",
      "planning number: 2 loss 0.9580764770507812\n",
      "planning number: 3 loss 0.1665802001953125\n",
      "planning number: 4 loss 0.16821670532226562\n",
      "planning number: 5 loss 1.1501388549804688\n",
      "planning number: 6 loss 1.7173233032226562\n",
      "planning number: 7 loss 0.34531402587890625\n",
      "planning number: 8 loss 0.3809852600097656\n",
      "planning number: 9 loss 0.044452667236328125\n",
      "planning number: 10 loss 0.11623764038085938\n",
      "player: -1 [0.11623764038085938, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [2, 2]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [2, 2]\n",
      "start learning player -1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.11971664428710938\n",
      "planning number: 2 loss 0.37427520751953125\n",
      "planning number: 3 loss 0.7708282470703125\n",
      "planning number: 4 loss 0.3925590515136719\n",
      "planning number: 5 loss 0.168060302734375\n",
      "planning number: 6 loss 0.4291725158691406\n",
      "planning number: 7 loss 0.09811782836914062\n",
      "planning number: 8 loss 0.40093231201171875\n",
      "planning number: 9 loss 0.24929428100585938\n",
      "planning number: 10 loss 0.4031982421875\n",
      "player: -1 [0.4031982421875, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 2]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 2]\n",
      "start learning player -1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.2146759033203125\n",
      "planning number: 2 loss 0.4194374084472656\n",
      "planning number: 3 loss 0.14324188232421875\n",
      "planning number: 4 loss 0.4202384948730469\n",
      "planning number: 5 loss 0.40289306640625\n",
      "planning number: 6 loss 0.3191719055175781\n",
      "planning number: 7 loss 0.2801361083984375\n",
      "planning number: 8 loss 0.4906158447265625\n",
      "planning number: 9 loss 0.505767822265625\n",
      "planning number: 10 loss 0.33359527587890625\n",
      "player: -1 [0.33359527587890625, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 2]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 2]\n",
      "start learning player -1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.3339996337890625\n",
      "planning number: 2 loss 0.43412017822265625\n",
      "planning number: 3 loss 0.1108856201171875\n",
      "planning number: 4 loss 0.06977081298828125\n",
      "planning number: 5 loss 1.0947647094726562\n",
      "planning number: 6 loss 1.5517959594726562\n",
      "planning number: 7 loss 0.43135833740234375\n",
      "planning number: 8 loss 0.48352813720703125\n",
      "planning number: 9 loss 0.5120773315429688\n",
      "planning number: 10 loss 0.2968902587890625\n",
      "player: -1 [0.2968902587890625, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 2]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 2]\n",
      "start learning player -1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.6588973999023438\n",
      "planning number: 2 loss 0.481231689453125\n",
      "planning number: 3 loss 0.32733154296875\n",
      "planning number: 4 loss 0.5769500732421875\n",
      "planning number: 5 loss 0.2135772705078125\n",
      "planning number: 6 loss 0.8124313354492188\n",
      "planning number: 7 loss 0.27262115478515625\n",
      "planning number: 8 loss 0.5671234130859375\n",
      "planning number: 9 loss 0.0461578369140625\n",
      "planning number: 10 loss 0.8203964233398438\n",
      "player: -1 [0.8203964233398438, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 2]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 2]\n",
      "start learning player -1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.067138671875\n",
      "planning number: 2 loss 0.7844085693359375\n",
      "planning number: 3 loss 0.05510711669921875\n",
      "planning number: 4 loss 0.041839599609375\n",
      "planning number: 5 loss 0.1018218994140625\n",
      "planning number: 6 loss 0.4153289794921875\n",
      "planning number: 7 loss 0.1062774658203125\n",
      "planning number: 8 loss 0.8285598754882812\n",
      "planning number: 9 loss 1.016082763671875\n",
      "planning number: 10 loss 1.341522216796875\n",
      "player: -1 [1.341522216796875, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 2]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 2]\n",
      "start learning player -1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.320556640625\n",
      "planning number: 2 loss 0.774444580078125\n",
      "planning number: 3 loss 1.0273513793945312\n",
      "planning number: 4 loss 1.511444091796875\n",
      "planning number: 5 loss 0.18817138671875\n",
      "planning number: 6 loss 0.6243133544921875\n",
      "planning number: 7 loss 0.24523162841796875\n",
      "planning number: 8 loss 0.0506591796875\n",
      "planning number: 9 loss 0.13983917236328125\n",
      "planning number: 10 loss 1.269683837890625\n",
      "player: -1 [1.269683837890625, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 2]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 2]\n",
      "start learning player -1\n",
      "data length: 2\n",
      "planning number: 1 loss 1.2244110107421875\n",
      "planning number: 2 loss 0.7415924072265625\n",
      "planning number: 3 loss 1.2611312866210938\n",
      "planning number: 4 loss 0.9812774658203125\n",
      "planning number: 5 loss 0.8339309692382812\n",
      "planning number: 6 loss 1.484832763671875\n",
      "planning number: 7 loss 0.25177001953125\n",
      "planning number: 8 loss 0.967742919921875\n",
      "planning number: 9 loss 0.187164306640625\n",
      "planning number: 10 loss 1.3173141479492188\n",
      "player: -1 [1.3173141479492188, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 2]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 2]\n",
      "start learning player -1\n",
      "data length: 2\n",
      "planning number: 1 loss 1.4131240844726562\n",
      "planning number: 2 loss 0.00783538818359375\n",
      "player: -1 [0.00783538818359375, 0.0] loops 2\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 2]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [2, 3]\n",
      "start learning player -1\n",
      "data length: 2\n",
      "planning number: 1 loss 1.4732894897460938\n",
      "planning number: 2 loss 1.9568557739257812\n",
      "planning number: 3 loss 1.2247238159179688\n",
      "planning number: 4 loss 2.2592697143554688\n",
      "planning number: 5 loss 0.631439208984375\n",
      "planning number: 6 loss 0.278717041015625\n",
      "planning number: 7 loss 0.22121429443359375\n",
      "planning number: 8 loss 1.1823959350585938\n",
      "planning number: 9 loss 0.7599258422851562\n",
      "planning number: 10 loss 0.2392730712890625\n",
      "player: -1 [0.2392730712890625, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "start learning player 1\n",
      "data length: 2\n",
      "planning number: 1 loss 4.702079772949219\n",
      "planning number: 2 loss 4.415031433105469\n",
      "planning number: 3 loss 4.455924987792969\n",
      "planning number: 4 loss 4.808349609375\n",
      "planning number: 5 loss 5.163669586181641\n",
      "planning number: 6 loss 5.491447448730469\n",
      "planning number: 7 loss 5.664356231689453\n",
      "planning number: 8 loss 5.3054351806640625\n",
      "planning number: 9 loss 3.657135009765625\n",
      "planning number: 10 loss 2.5647621154785156\n",
      "player: 1 [2.5647621154785156, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [2, 1]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [2, 1]\n",
      "start learning player 1\n",
      "data length: 2\n",
      "planning number: 1 loss 3.4941940307617188\n",
      "planning number: 2 loss 2.0590896606445312\n",
      "planning number: 3 loss 1.8517417907714844\n",
      "planning number: 4 loss 1.3147430419921875\n",
      "planning number: 5 loss 0.6674766540527344\n",
      "planning number: 6 loss 1.1676750183105469\n",
      "planning number: 7 loss 0.7449569702148438\n",
      "planning number: 8 loss 0.5573234558105469\n",
      "planning number: 9 loss 0.5377159118652344\n",
      "planning number: 10 loss 0.5256996154785156\n",
      "player: 1 [0.5256996154785156, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "start learning player 1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.11968612670898438\n",
      "planning number: 2 loss 0.4917755126953125\n",
      "planning number: 3 loss 1.1195831298828125\n",
      "planning number: 4 loss 0.81500244140625\n",
      "planning number: 5 loss 0.8030166625976562\n",
      "planning number: 6 loss 0.9076042175292969\n",
      "planning number: 7 loss 0.2668647766113281\n",
      "planning number: 8 loss 0.34551239013671875\n",
      "planning number: 9 loss 0.4956512451171875\n",
      "planning number: 10 loss 0.1678924560546875\n",
      "player: 1 [0.1678924560546875, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [2, 1]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "start learning player 1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.6758537292480469\n",
      "planning number: 2 loss 0.6682167053222656\n",
      "planning number: 3 loss 0.5452194213867188\n",
      "planning number: 4 loss 0.3085479736328125\n",
      "planning number: 5 loss 0.3176422119140625\n",
      "planning number: 6 loss 0.39328765869140625\n",
      "planning number: 7 loss 0.4044647216796875\n",
      "planning number: 8 loss 0.21666336059570312\n",
      "planning number: 9 loss 0.09849166870117188\n",
      "planning number: 10 loss 0.5890007019042969\n",
      "player: 1 [0.5890007019042969, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "start learning player 1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.09011077880859375\n",
      "planning number: 2 loss 1.625640869140625\n",
      "planning number: 3 loss 1.6759262084960938\n",
      "planning number: 4 loss 0.36257171630859375\n",
      "planning number: 5 loss 0.17655181884765625\n",
      "planning number: 6 loss 1.3135223388671875\n",
      "planning number: 7 loss 2.1313858032226562\n",
      "planning number: 8 loss 2.5105361938476562\n",
      "planning number: 9 loss 1.78399658203125\n",
      "planning number: 10 loss 2.3490524291992188\n",
      "player: 1 [2.3490524291992188, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "start learning player 1\n",
      "data length: 2\n",
      "planning number: 1 loss 2.1774673461914062\n",
      "planning number: 2 loss 1.7279815673828125\n",
      "planning number: 3 loss 0.6143798828125\n",
      "planning number: 4 loss 0.9314804077148438\n",
      "planning number: 5 loss 1.3163604736328125\n",
      "planning number: 6 loss 2.5129623413085938\n",
      "planning number: 7 loss 3.4110794067382812\n",
      "planning number: 8 loss 2.4011001586914062\n",
      "planning number: 9 loss 1.4360733032226562\n",
      "planning number: 10 loss 0.12281036376953125\n",
      "player: 1 [0.12281036376953125, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "start learning player 1\n",
      "data length: 2\n",
      "planning number: 1 loss 1.2104644775390625\n",
      "planning number: 2 loss 1.9323806762695312\n",
      "planning number: 3 loss 2.612640380859375\n",
      "planning number: 4 loss 1.45086669921875\n",
      "planning number: 5 loss 0.7602310180664062\n",
      "planning number: 6 loss 0.3975372314453125\n",
      "planning number: 7 loss 0.8386001586914062\n",
      "planning number: 8 loss 0.5508880615234375\n",
      "planning number: 9 loss 0.48767852783203125\n",
      "planning number: 10 loss 0.9769363403320312\n",
      "player: 1 [0.9769363403320312, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "start learning player 1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.8568267822265625\n",
      "planning number: 2 loss 0.05332183837890625\n",
      "planning number: 3 loss 0.0780181884765625\n",
      "planning number: 4 loss 0.931610107421875\n",
      "planning number: 5 loss 0.1719207763671875\n",
      "planning number: 6 loss 0.38887786865234375\n",
      "planning number: 7 loss 0.27020263671875\n",
      "planning number: 8 loss 0.5714645385742188\n",
      "planning number: 9 loss 0.9655532836914062\n",
      "planning number: 10 loss 1.1189193725585938\n",
      "player: 1 [1.1189193725585938, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "start learning player 1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.4127960205078125\n",
      "planning number: 2 loss 0.1390228271484375\n",
      "planning number: 3 loss 0.9207763671875\n",
      "planning number: 4 loss 0.9906387329101562\n",
      "planning number: 5 loss 1.4755096435546875\n",
      "planning number: 6 loss 0.25632476806640625\n",
      "planning number: 7 loss 0.7289810180664062\n",
      "planning number: 8 loss 1.1428375244140625\n",
      "planning number: 9 loss 1.4997787475585938\n",
      "planning number: 10 loss 0.128662109375\n",
      "player: 1 [0.128662109375, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "start learning player 1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.5840530395507812\n",
      "planning number: 2 loss 0.31572723388671875\n",
      "planning number: 3 loss 0.11014556884765625\n",
      "planning number: 4 loss 0.09021759033203125\n",
      "planning number: 5 loss 1.260650634765625\n",
      "planning number: 6 loss 1.31329345703125\n",
      "planning number: 7 loss 0.7203903198242188\n",
      "planning number: 8 loss 1.341827392578125\n",
      "planning number: 9 loss 0.9737319946289062\n",
      "planning number: 10 loss 0.8920211791992188\n",
      "player: 1 [0.8920211791992188, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "start learning player 1\n",
      "data length: 2\n",
      "planning number: 1 loss 1.4979629516601562\n",
      "planning number: 2 loss 0.284332275390625\n",
      "planning number: 3 loss 0.9716873168945312\n",
      "planning number: 4 loss 0.17145538330078125\n",
      "planning number: 5 loss 1.332794189453125\n",
      "planning number: 6 loss 1.4676284790039062\n",
      "planning number: 7 loss 0.02466583251953125\n",
      "planning number: 8 loss 0.3481903076171875\n",
      "planning number: 9 loss 0.079315185546875\n",
      "planning number: 10 loss 0.02562713623046875\n",
      "player: 1 [0.02562713623046875, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "start learning player 1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.3293609619140625\n",
      "planning number: 2 loss 0.18387603759765625\n",
      "planning number: 3 loss 0.4551849365234375\n",
      "planning number: 4 loss 0.41312408447265625\n",
      "planning number: 5 loss 0.225433349609375\n",
      "planning number: 6 loss 0.24626922607421875\n",
      "planning number: 7 loss 0.2913055419921875\n",
      "planning number: 8 loss 0.19167327880859375\n",
      "planning number: 9 loss 0.47611236572265625\n",
      "planning number: 10 loss 0.51654052734375\n",
      "player: 1 [0.51654052734375, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "start learning player 1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.00525665283203125\n",
      "player: 1 [0.00525665283203125, 0.0] loops 1\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "start learning player 1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.2925262451171875\n",
      "planning number: 2 loss 0.0776214599609375\n",
      "planning number: 3 loss 1.0219192504882812\n",
      "planning number: 4 loss 1.4087371826171875\n",
      "planning number: 5 loss 0.5643692016601562\n",
      "planning number: 6 loss 0.632720947265625\n",
      "planning number: 7 loss 0.9345245361328125\n",
      "planning number: 8 loss 1.1601486206054688\n",
      "planning number: 9 loss 1.9725875854492188\n",
      "planning number: 10 loss 0.8403167724609375\n",
      "player: 1 [0.8403167724609375, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "start learning player 1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.8307113647460938\n",
      "planning number: 2 loss 0.4410400390625\n",
      "planning number: 3 loss 0.48299407958984375\n",
      "planning number: 4 loss 0.8086318969726562\n",
      "planning number: 5 loss 0.5010757446289062\n",
      "planning number: 6 loss 0.478118896484375\n",
      "planning number: 7 loss 0.9394149780273438\n",
      "planning number: 8 loss 0.8295822143554688\n",
      "planning number: 9 loss 0.0811920166015625\n",
      "planning number: 10 loss 0.099761962890625\n",
      "player: 1 [0.099761962890625, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "start learning player 1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.8873672485351562\n",
      "planning number: 2 loss 0.14550018310546875\n",
      "planning number: 3 loss 0.35338592529296875\n",
      "planning number: 4 loss 0.2513580322265625\n",
      "planning number: 5 loss 0.5395889282226562\n",
      "planning number: 6 loss 0.8883819580078125\n",
      "planning number: 7 loss 1.0689544677734375\n",
      "planning number: 8 loss 0.4210662841796875\n",
      "planning number: 9 loss 0.38922119140625\n",
      "planning number: 10 loss 0.30599212646484375\n",
      "player: 1 [0.30599212646484375, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "start learning player 1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.6359634399414062\n",
      "planning number: 2 loss 1.1304244995117188\n",
      "planning number: 3 loss 0.6073684692382812\n",
      "planning number: 4 loss 0.9610366821289062\n",
      "planning number: 5 loss 1.0391921997070312\n",
      "planning number: 6 loss 1.1422500610351562\n",
      "planning number: 7 loss 0.5377731323242188\n",
      "planning number: 8 loss 1.0118789672851562\n",
      "planning number: 9 loss 0.722015380859375\n",
      "planning number: 10 loss 0.1456451416015625\n",
      "player: 1 [0.1456451416015625, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "start learning player 1\n",
      "data length: 2\n",
      "planning number: 1 loss 1.3219757080078125\n",
      "planning number: 2 loss 1.5037689208984375\n",
      "planning number: 3 loss 0.4127960205078125\n",
      "planning number: 4 loss 0.9256744384765625\n",
      "planning number: 5 loss 1.2325439453125\n",
      "planning number: 6 loss 1.7574996948242188\n",
      "planning number: 7 loss 1.0160980224609375\n",
      "planning number: 8 loss 1.6295013427734375\n",
      "planning number: 9 loss 1.336395263671875\n",
      "planning number: 10 loss 1.1693115234375\n",
      "player: 1 [1.1693115234375, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "start learning player 1\n",
      "data length: 2\n",
      "planning number: 1 loss 1.8388290405273438\n",
      "planning number: 2 loss 0.5999603271484375\n",
      "planning number: 3 loss 0.29052734375\n",
      "planning number: 4 loss 0.237548828125\n",
      "planning number: 5 loss 1.087738037109375\n",
      "planning number: 6 loss 0.610382080078125\n",
      "planning number: 7 loss 0.13956451416015625\n",
      "planning number: 8 loss 0.426666259765625\n",
      "planning number: 9 loss 0.08678436279296875\n",
      "planning number: 10 loss 0.45082855224609375\n",
      "player: 1 [0.45082855224609375, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "start learning player 1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.15692138671875\n",
      "planning number: 2 loss 0.5478286743164062\n",
      "planning number: 3 loss 0.421112060546875\n",
      "planning number: 4 loss 0.33321380615234375\n",
      "planning number: 5 loss 0.381317138671875\n",
      "planning number: 6 loss 0.176025390625\n",
      "planning number: 7 loss 0.16370391845703125\n",
      "planning number: 8 loss 0.371826171875\n",
      "planning number: 9 loss 0.28975677490234375\n",
      "planning number: 10 loss 0.3416595458984375\n",
      "player: 1 [0.3416595458984375, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "start learning player 1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.3740081787109375\n",
      "planning number: 2 loss 0.13219451904296875\n",
      "planning number: 3 loss 0.0352630615234375\n",
      "planning number: 4 loss 0.4882049560546875\n",
      "planning number: 5 loss 0.189666748046875\n",
      "planning number: 6 loss 0.5660018920898438\n",
      "planning number: 7 loss 1.1799087524414062\n",
      "planning number: 8 loss 0.7051010131835938\n",
      "planning number: 9 loss 0.223907470703125\n",
      "planning number: 10 loss 0.13132476806640625\n",
      "player: 1 [0.13132476806640625, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "start learning player 1\n",
      "data length: 2\n",
      "planning number: 1 loss 1.0566864013671875\n",
      "planning number: 2 loss 1.4560089111328125\n",
      "planning number: 3 loss 0.376983642578125\n",
      "planning number: 4 loss 0.5097732543945312\n",
      "planning number: 5 loss 0.556671142578125\n",
      "planning number: 6 loss 0.189239501953125\n",
      "planning number: 7 loss 0.5178146362304688\n",
      "planning number: 8 loss 0.33880615234375\n",
      "planning number: 9 loss 0.4351043701171875\n",
      "planning number: 10 loss 0.68145751953125\n",
      "player: 1 [0.68145751953125, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "start learning player 1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.34847259521484375\n",
      "planning number: 2 loss 0.6123428344726562\n",
      "planning number: 3 loss 1.0659637451171875\n",
      "planning number: 4 loss 0.11224365234375\n",
      "planning number: 5 loss 0.42206573486328125\n",
      "planning number: 6 loss 0.39028167724609375\n",
      "planning number: 7 loss 0.7102279663085938\n",
      "planning number: 8 loss 0.6512985229492188\n",
      "planning number: 9 loss 0.36774444580078125\n",
      "planning number: 10 loss 0.7743148803710938\n",
      "player: 1 [0.7743148803710938, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "start learning player 1\n",
      "data length: 2\n",
      "planning number: 1 loss 1.698333740234375\n",
      "planning number: 2 loss 0.31180572509765625\n",
      "planning number: 3 loss 0.39771270751953125\n",
      "planning number: 4 loss 0.054168701171875\n",
      "planning number: 5 loss 0.06827545166015625\n",
      "planning number: 6 loss 0.38315582275390625\n",
      "planning number: 7 loss 0.071197509765625\n",
      "planning number: 8 loss 0.746551513671875\n",
      "planning number: 9 loss 0.9957809448242188\n",
      "planning number: 10 loss 1.2405548095703125\n",
      "player: 1 [1.2405548095703125, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "start learning player 1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.3208160400390625\n",
      "planning number: 2 loss 0.7393875122070312\n",
      "planning number: 3 loss 0.9803543090820312\n",
      "planning number: 4 loss 1.4124984741210938\n",
      "planning number: 5 loss 0.1436767578125\n",
      "planning number: 6 loss 0.5468597412109375\n",
      "planning number: 7 loss 0.28829193115234375\n",
      "planning number: 8 loss 0.1168365478515625\n",
      "planning number: 9 loss 0.05266571044921875\n",
      "planning number: 10 loss 1.1167068481445312\n",
      "player: 1 [1.1167068481445312, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "start learning player 1\n",
      "data length: 2\n",
      "planning number: 1 loss 1.2677154541015625\n",
      "planning number: 2 loss 0.5855178833007812\n",
      "planning number: 3 loss 1.3376235961914062\n",
      "planning number: 4 loss 0.7793731689453125\n",
      "planning number: 5 loss 0.9689178466796875\n",
      "planning number: 6 loss 1.2235488891601562\n",
      "planning number: 7 loss 0.4527740478515625\n",
      "planning number: 8 loss 0.6933975219726562\n",
      "planning number: 9 loss 0.0655364990234375\n",
      "planning number: 10 loss 0.12921142578125\n",
      "player: 1 [0.12921142578125, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "start learning player 1\n",
      "data length: 2\n",
      "planning number: 1 loss 1.0266647338867188\n",
      "planning number: 2 loss 0.5838623046875\n",
      "planning number: 3 loss 0.0980987548828125\n",
      "planning number: 4 loss 1.3209304809570312\n",
      "planning number: 5 loss 1.1828384399414062\n",
      "planning number: 6 loss 0.19112396240234375\n",
      "planning number: 7 loss 0.5067901611328125\n",
      "planning number: 8 loss 1.26947021484375\n",
      "planning number: 9 loss 1.0814132690429688\n",
      "planning number: 10 loss 0.2062530517578125\n",
      "player: 1 [0.2062530517578125, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "start learning player 1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.31906890869140625\n",
      "planning number: 2 loss 0.3144378662109375\n",
      "planning number: 3 loss 0.07695770263671875\n",
      "planning number: 4 loss 0.0269317626953125\n",
      "planning number: 5 loss 0.4766082763671875\n",
      "planning number: 6 loss 0.29166412353515625\n",
      "planning number: 7 loss 0.085357666015625\n",
      "planning number: 8 loss 0.10330963134765625\n",
      "planning number: 9 loss 1.168731689453125\n",
      "planning number: 10 loss 1.1869049072265625\n",
      "player: 1 [1.1869049072265625, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "start learning player 1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.65142822265625\n",
      "planning number: 2 loss 1.2524795532226562\n",
      "planning number: 3 loss 0.8440704345703125\n",
      "planning number: 4 loss 0.88922119140625\n",
      "planning number: 5 loss 1.28143310546875\n",
      "planning number: 6 loss 0.3814239501953125\n",
      "planning number: 7 loss 0.7529144287109375\n",
      "planning number: 8 loss 0.00026702880859375\n",
      "player: 1 [0.00026702880859375, 0.0] loops 8\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "start learning player 1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.06441497802734375\n",
      "planning number: 2 loss 0.9550552368164062\n",
      "planning number: 3 loss 0.6400222778320312\n",
      "planning number: 4 loss 0.15752410888671875\n",
      "planning number: 5 loss 0.41790771484375\n",
      "planning number: 6 loss 0.07814788818359375\n",
      "planning number: 7 loss 1.3741683959960938\n",
      "planning number: 8 loss 1.0565338134765625\n",
      "planning number: 9 loss 0.332244873046875\n",
      "planning number: 10 loss 0.6604232788085938\n",
      "player: 1 [0.6604232788085938, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "start learning player 1\n",
      "data length: 2\n",
      "planning number: 1 loss 1.0876922607421875\n",
      "planning number: 2 loss 1.2427520751953125\n",
      "planning number: 3 loss 0.02898406982421875\n",
      "planning number: 4 loss 0.14006805419921875\n",
      "planning number: 5 loss 0.13559722900390625\n",
      "planning number: 6 loss 0.807586669921875\n",
      "planning number: 7 loss 1.5648422241210938\n",
      "planning number: 8 loss 0.20361328125\n",
      "planning number: 9 loss 1.061431884765625\n",
      "planning number: 10 loss 1.775299072265625\n",
      "player: 1 [1.775299072265625, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "start learning player 1\n",
      "data length: 2\n",
      "planning number: 1 loss 2.0294189453125\n",
      "planning number: 2 loss 1.4636077880859375\n",
      "planning number: 3 loss 1.9800491333007812\n",
      "planning number: 4 loss 1.7175979614257812\n",
      "planning number: 5 loss 1.576995849609375\n",
      "planning number: 6 loss 2.1655502319335938\n",
      "planning number: 7 loss 1.0550918579101562\n",
      "planning number: 8 loss 0.158294677734375\n",
      "planning number: 9 loss 0.7060317993164062\n",
      "planning number: 10 loss 0.13014984130859375\n",
      "player: 1 [0.13014984130859375, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "start learning player 1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.1464691162109375\n",
      "planning number: 2 loss 0.21282958984375\n",
      "planning number: 3 loss 0.9736480712890625\n",
      "planning number: 4 loss 1.2056884765625\n",
      "planning number: 5 loss 0.8629989624023438\n",
      "planning number: 6 loss 0.5642929077148438\n",
      "planning number: 7 loss 0.7683944702148438\n",
      "planning number: 8 loss 1.1915054321289062\n",
      "planning number: 9 loss 0.828643798828125\n",
      "planning number: 10 loss 0.7756729125976562\n",
      "player: 1 [0.7756729125976562, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "start learning player 1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.18752288818359375\n",
      "planning number: 2 loss 0.0235748291015625\n",
      "planning number: 3 loss 0.8763275146484375\n",
      "planning number: 4 loss 0.528076171875\n",
      "planning number: 5 loss 0.9004058837890625\n",
      "planning number: 6 loss 1.6292953491210938\n",
      "planning number: 7 loss 2.029754638671875\n",
      "planning number: 8 loss 1.2178802490234375\n",
      "planning number: 9 loss 0.03180694580078125\n",
      "planning number: 10 loss 0.5756301879882812\n",
      "player: 1 [0.5756301879882812, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "start learning player 1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.43531036376953125\n",
      "planning number: 2 loss 0.39139556884765625\n",
      "planning number: 3 loss 0.7695159912109375\n",
      "planning number: 4 loss 0.6326141357421875\n",
      "planning number: 5 loss 0.09256744384765625\n",
      "planning number: 6 loss 0.319244384765625\n",
      "planning number: 7 loss 0.2703399658203125\n",
      "planning number: 8 loss 0.4902801513671875\n",
      "planning number: 9 loss 0.02173614501953125\n",
      "planning number: 10 loss 1.1748580932617188\n",
      "player: 1 [1.1748580932617188, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "start learning player 1\n",
      "data length: 2\n",
      "planning number: 1 loss 1.2964248657226562\n",
      "planning number: 2 loss 0.14356231689453125\n",
      "planning number: 3 loss 1.0817947387695312\n",
      "planning number: 4 loss 1.372100830078125\n",
      "planning number: 5 loss 0.014007568359375\n",
      "player: 1 [0.014007568359375, 0.0] loops 5\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [3, 1]\n",
      "start learning player 1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.28336334228515625\n",
      "planning number: 2 loss 0.11773681640625\n",
      "planning number: 3 loss 0.0269317626953125\n",
      "planning number: 4 loss 0.5354385375976562\n",
      "planning number: 5 loss 0.485992431640625\n",
      "planning number: 6 loss 0.10358428955078125\n",
      "planning number: 7 loss 0.12348175048828125\n",
      "planning number: 8 loss 0.37180328369140625\n",
      "planning number: 9 loss 0.2834014892578125\n",
      "planning number: 10 loss 0.326934814453125\n",
      "player: 1 [0.326934814453125, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "start learning player 1\n",
      "data length: 2\n",
      "planning number: 1 loss 8.684219360351562\n",
      "planning number: 2 loss 6.878543853759766\n",
      "planning number: 3 loss 5.140048980712891\n",
      "planning number: 4 loss 4.19915771484375\n",
      "planning number: 5 loss 2.8424148559570312\n",
      "planning number: 6 loss 1.8912239074707031\n",
      "planning number: 7 loss 0.896484375\n",
      "planning number: 8 loss 0.46547698974609375\n",
      "planning number: 9 loss 0.2774505615234375\n",
      "planning number: 10 loss 0.5107421875\n",
      "player: 1 [0.5107421875, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [2, 1]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [2, 1]\n",
      "start learning player 1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.5346755981445312\n",
      "planning number: 2 loss 0.6842613220214844\n",
      "planning number: 3 loss 0.7400436401367188\n",
      "planning number: 4 loss 0.02072906494140625\n",
      "planning number: 5 loss 0.42528533935546875\n",
      "planning number: 6 loss 0.5607566833496094\n",
      "planning number: 7 loss 0.26798248291015625\n",
      "planning number: 8 loss 0.17444229125976562\n",
      "planning number: 9 loss 0.18065643310546875\n",
      "planning number: 10 loss 0.2859382629394531\n",
      "player: 1 [0.2859382629394531, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [2, 1]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [2, 1]\n",
      "start learning player 1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.3842926025390625\n",
      "planning number: 2 loss 0.3369140625\n",
      "planning number: 3 loss 0.5279998779296875\n",
      "planning number: 4 loss 0.13426971435546875\n",
      "planning number: 5 loss 0.03290557861328125\n",
      "planning number: 6 loss 0.39223480224609375\n",
      "planning number: 7 loss 0.3763885498046875\n",
      "planning number: 8 loss 0.7173004150390625\n",
      "planning number: 9 loss 0.6879425048828125\n",
      "planning number: 10 loss 0.3923797607421875\n",
      "player: 1 [0.3923797607421875, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "start learning player 1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.8159217834472656\n",
      "planning number: 2 loss 1.6122169494628906\n",
      "planning number: 3 loss 0.3739509582519531\n",
      "planning number: 4 loss 0.07599639892578125\n",
      "planning number: 5 loss 0.029285430908203125\n",
      "planning number: 6 loss 0.4259223937988281\n",
      "planning number: 7 loss 0.26476287841796875\n",
      "planning number: 8 loss 0.5746307373046875\n",
      "planning number: 9 loss 1.1455955505371094\n",
      "planning number: 10 loss 0.4957695007324219\n",
      "player: 1 [0.4957695007324219, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [2, 1]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [2, 1]\n",
      "start learning player 1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.8255882263183594\n",
      "planning number: 2 loss 1.0849533081054688\n",
      "planning number: 3 loss 0.9984893798828125\n",
      "planning number: 4 loss 0.60162353515625\n",
      "planning number: 5 loss 1.0503997802734375\n",
      "planning number: 6 loss 0.6110267639160156\n",
      "planning number: 7 loss 0.06567001342773438\n",
      "planning number: 8 loss 1.1930809020996094\n",
      "planning number: 9 loss 1.5028800964355469\n",
      "planning number: 10 loss 0.9884681701660156\n",
      "player: 1 [0.9884681701660156, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [2, 1]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [2, 1]\n",
      "start learning player 1\n",
      "data length: 2\n",
      "planning number: 1 loss 1.1232147216796875\n",
      "planning number: 2 loss 1.890228271484375\n",
      "planning number: 3 loss 0.8572235107421875\n",
      "planning number: 4 loss 0.8558883666992188\n",
      "planning number: 5 loss 0.48162078857421875\n",
      "planning number: 6 loss 0.41231536865234375\n",
      "planning number: 7 loss 0.7280197143554688\n",
      "planning number: 8 loss 0.43196868896484375\n",
      "planning number: 9 loss 0.5125579833984375\n",
      "planning number: 10 loss 0.955291748046875\n",
      "player: 1 [0.955291748046875, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "start learning player 1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.12364959716796875\n",
      "planning number: 2 loss 0.048309326171875\n",
      "planning number: 3 loss 0.3515052795410156\n",
      "planning number: 4 loss 0.3226318359375\n",
      "planning number: 5 loss 0.6349449157714844\n",
      "planning number: 6 loss 0.6876869201660156\n",
      "planning number: 7 loss 0.11771774291992188\n",
      "planning number: 8 loss 0.22137069702148438\n",
      "planning number: 9 loss 1.13055419921875\n",
      "planning number: 10 loss 0.4422111511230469\n",
      "player: 1 [0.4422111511230469, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "start learning player 1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.0380401611328125\n",
      "planning number: 2 loss 0.0473175048828125\n",
      "planning number: 3 loss 0.41562652587890625\n",
      "planning number: 4 loss 0.32623291015625\n",
      "planning number: 5 loss 0.5846099853515625\n",
      "planning number: 6 loss 0.4216461181640625\n",
      "planning number: 7 loss 0.5373764038085938\n",
      "planning number: 8 loss 1.3369903564453125\n",
      "planning number: 9 loss 0.9212570190429688\n",
      "planning number: 10 loss 0.39456939697265625\n",
      "player: 1 [0.39456939697265625, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [2, 1]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [2, 1]\n",
      "start learning player 1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.5055656433105469\n",
      "planning number: 2 loss 1.4665069580078125\n",
      "planning number: 3 loss 0.5178298950195312\n",
      "planning number: 4 loss 0.4919395446777344\n",
      "planning number: 5 loss 0.4023094177246094\n",
      "planning number: 6 loss 0.3944587707519531\n",
      "planning number: 7 loss 0.4151611328125\n",
      "planning number: 8 loss 0.9633903503417969\n",
      "planning number: 9 loss 1.0227241516113281\n",
      "planning number: 10 loss 1.8677749633789062\n",
      "player: 1 [1.8677749633789062, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [2, 1]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [2, 1]\n",
      "start learning player 1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.8079910278320312\n",
      "planning number: 2 loss 0.7764968872070312\n",
      "planning number: 3 loss 0.3882904052734375\n",
      "planning number: 4 loss 0.5076141357421875\n",
      "planning number: 5 loss 0.82769775390625\n",
      "planning number: 6 loss 0.5394287109375\n",
      "planning number: 7 loss 0.39354705810546875\n",
      "planning number: 8 loss 0.8307876586914062\n",
      "planning number: 9 loss 0.721038818359375\n",
      "planning number: 10 loss 0.00064849853515625\n",
      "player: 1 [0.00064849853515625, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "start learning player 1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.3583335876464844\n",
      "planning number: 2 loss 0.3703575134277344\n",
      "planning number: 3 loss 0.6991424560546875\n",
      "planning number: 4 loss 0.08414840698242188\n",
      "planning number: 5 loss 0.20305252075195312\n",
      "planning number: 6 loss 0.6342506408691406\n",
      "planning number: 7 loss 0.7105140686035156\n",
      "planning number: 8 loss 0.66363525390625\n",
      "planning number: 9 loss 0.6429901123046875\n",
      "planning number: 10 loss 0.20783615112304688\n",
      "player: 1 [0.20783615112304688, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "start learning player 1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.25917816162109375\n",
      "planning number: 2 loss 0.34088134765625\n",
      "planning number: 3 loss 0.0620574951171875\n",
      "planning number: 4 loss 0.4396820068359375\n",
      "planning number: 5 loss 0.1732025146484375\n",
      "planning number: 6 loss 0.49504852294921875\n",
      "planning number: 7 loss 0.5062484741210938\n",
      "planning number: 8 loss 0.05142974853515625\n",
      "planning number: 9 loss 0.05083465576171875\n",
      "planning number: 10 loss 0.4595947265625\n",
      "player: 1 [0.4595947265625, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "start learning player 1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.37755584716796875\n",
      "planning number: 2 loss 0.232940673828125\n",
      "planning number: 3 loss 0.26454925537109375\n",
      "planning number: 4 loss 0.22457122802734375\n",
      "planning number: 5 loss 0.13195037841796875\n",
      "planning number: 6 loss 0.4816741943359375\n",
      "planning number: 7 loss 0.5168380737304688\n",
      "planning number: 8 loss 0.23044586181640625\n",
      "planning number: 9 loss 0.42084503173828125\n",
      "planning number: 10 loss 0.02822113037109375\n",
      "player: 1 [0.02822113037109375, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "start learning player 1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.7477340698242188\n",
      "planning number: 2 loss 0.7384262084960938\n",
      "planning number: 3 loss 0.1011810302734375\n",
      "planning number: 4 loss 1.16094970703125\n",
      "planning number: 5 loss 1.5406494140625\n",
      "planning number: 6 loss 2.3234100341796875\n",
      "planning number: 7 loss 1.12109375\n",
      "planning number: 8 loss 0.5094757080078125\n",
      "planning number: 9 loss 0.5024032592773438\n",
      "planning number: 10 loss 0.90045166015625\n",
      "player: 1 [0.90045166015625, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "start learning player 1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.671722412109375\n",
      "planning number: 2 loss 0.207550048828125\n",
      "planning number: 3 loss 0.6161270141601562\n",
      "planning number: 4 loss 0.49764251708984375\n",
      "planning number: 5 loss 0.21588134765625\n",
      "planning number: 6 loss 0.43548583984375\n",
      "planning number: 7 loss 0.10595703125\n",
      "planning number: 8 loss 0.03076934814453125\n",
      "planning number: 9 loss 0.37481689453125\n",
      "planning number: 10 loss 0.37174224853515625\n",
      "player: 1 [0.37174224853515625, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "start learning player 1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.6976242065429688\n",
      "planning number: 2 loss 0.609039306640625\n",
      "planning number: 3 loss 0.3279571533203125\n",
      "planning number: 4 loss 0.717254638671875\n",
      "planning number: 5 loss 1.6792755126953125\n",
      "planning number: 6 loss 2.2301788330078125\n",
      "planning number: 7 loss 1.557098388671875\n",
      "planning number: 8 loss 0.581878662109375\n",
      "planning number: 9 loss 1.3844833374023438\n",
      "planning number: 10 loss 2.1429519653320312\n",
      "player: 1 [2.1429519653320312, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "start learning player 1\n",
      "data length: 2\n",
      "planning number: 1 loss 2.88995361328125\n",
      "planning number: 2 loss 1.4085845947265625\n",
      "planning number: 3 loss 1.3684616088867188\n",
      "planning number: 4 loss 0.123565673828125\n",
      "planning number: 5 loss 0.8161468505859375\n",
      "planning number: 6 loss 0.7654953002929688\n",
      "planning number: 7 loss 0.00760650634765625\n",
      "player: 1 [0.00760650634765625, 0.0] loops 7\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "start learning player 1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.3551177978515625\n",
      "planning number: 2 loss 0.20050811767578125\n",
      "planning number: 3 loss 0.5346832275390625\n",
      "planning number: 4 loss 0.7671966552734375\n",
      "planning number: 5 loss 0.445404052734375\n",
      "planning number: 6 loss 0.4793853759765625\n",
      "planning number: 7 loss 0.9152755737304688\n",
      "planning number: 8 loss 0.81646728515625\n",
      "planning number: 9 loss 0.12227630615234375\n",
      "planning number: 10 loss 0.7987060546875\n",
      "player: 1 [0.7987060546875, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "start learning player 1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.19928741455078125\n",
      "planning number: 2 loss 0.03269195556640625\n",
      "planning number: 3 loss 0.8869857788085938\n",
      "planning number: 4 loss 0.0014190673828125\n",
      "player: 1 [0.0014190673828125, 0.0] loops 4\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "start learning player 1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.9509735107421875\n",
      "planning number: 2 loss 1.6483230590820312\n",
      "planning number: 3 loss 2.4190750122070312\n",
      "planning number: 4 loss 2.4890670776367188\n",
      "planning number: 5 loss 1.047607421875\n",
      "planning number: 6 loss 0.26840972900390625\n",
      "planning number: 7 loss 1.2881011962890625\n",
      "planning number: 8 loss 2.7253341674804688\n",
      "planning number: 9 loss 1.6081390380859375\n",
      "planning number: 10 loss 1.4217987060546875\n",
      "player: 1 [1.4217987060546875, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "start learning player 1\n",
      "data length: 2\n",
      "planning number: 1 loss 2.1928558349609375\n",
      "planning number: 2 loss 1.3192138671875\n",
      "planning number: 3 loss 2.0173263549804688\n",
      "planning number: 4 loss 1.6381607055664062\n",
      "planning number: 5 loss 1.5797271728515625\n",
      "planning number: 6 loss 2.0997848510742188\n",
      "planning number: 7 loss 1.0560302734375\n",
      "planning number: 8 loss 0.174468994140625\n",
      "planning number: 9 loss 0.2211151123046875\n",
      "planning number: 10 loss 1.1636276245117188\n",
      "player: 1 [1.1636276245117188, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "start learning player 1\n",
      "data length: 2\n",
      "planning number: 1 loss 1.413238525390625\n",
      "planning number: 2 loss 0.164215087890625\n",
      "planning number: 3 loss 0.48554229736328125\n",
      "planning number: 4 loss 0.31943511962890625\n",
      "planning number: 5 loss 0.526336669921875\n",
      "planning number: 6 loss 0.484222412109375\n",
      "planning number: 7 loss 0.611083984375\n",
      "planning number: 8 loss 1.2151336669921875\n",
      "planning number: 9 loss 0.9953689575195312\n",
      "planning number: 10 loss 0.29199981689453125\n",
      "player: 1 [0.29199981689453125, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "start learning player 1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.42696380615234375\n",
      "planning number: 2 loss 1.5192718505859375\n",
      "planning number: 3 loss 0.42771148681640625\n",
      "planning number: 4 loss 0.566497802734375\n",
      "planning number: 5 loss 0.922027587890625\n",
      "planning number: 6 loss 0.9496078491210938\n",
      "planning number: 7 loss 0.5476531982421875\n",
      "planning number: 8 loss 0.0468292236328125\n",
      "planning number: 9 loss 0.23976898193359375\n",
      "planning number: 10 loss 0.6979141235351562\n",
      "player: 1 [0.6979141235351562, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "start learning player 1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.09310150146484375\n",
      "planning number: 2 loss 0.560089111328125\n",
      "planning number: 3 loss 0.40149688720703125\n",
      "planning number: 4 loss 0.1485748291015625\n",
      "planning number: 5 loss 0.8651351928710938\n",
      "planning number: 6 loss 0.353515625\n",
      "planning number: 7 loss 0.098602294921875\n",
      "planning number: 8 loss 0.554595947265625\n",
      "planning number: 9 loss 0.18758392333984375\n",
      "planning number: 10 loss 0.16106414794921875\n",
      "player: 1 [0.16106414794921875, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "start learning player 1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.1992950439453125\n",
      "planning number: 2 loss 0.9265823364257812\n",
      "planning number: 3 loss 1.212921142578125\n",
      "planning number: 4 loss 0.8018798828125\n",
      "planning number: 5 loss 0.597930908203125\n",
      "planning number: 6 loss 0.80133056640625\n",
      "planning number: 7 loss 1.1119461059570312\n",
      "planning number: 8 loss 0.8657073974609375\n",
      "planning number: 9 loss 0.70074462890625\n",
      "planning number: 10 loss 0.1232147216796875\n",
      "player: 1 [0.1232147216796875, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "start learning player 1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.03911590576171875\n",
      "planning number: 2 loss 0.4097137451171875\n",
      "planning number: 3 loss 0.3722381591796875\n",
      "planning number: 4 loss 0.2222442626953125\n",
      "planning number: 5 loss 1.2321395874023438\n",
      "planning number: 6 loss 1.4336471557617188\n",
      "planning number: 7 loss 0.44300079345703125\n",
      "planning number: 8 loss 0.787994384765625\n",
      "planning number: 9 loss 1.2167205810546875\n",
      "planning number: 10 loss 1.5632400512695312\n",
      "player: 1 [1.5632400512695312, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "start learning player 1\n",
      "data length: 2\n",
      "planning number: 1 loss 1.0031509399414062\n",
      "planning number: 2 loss 1.4658966064453125\n",
      "planning number: 3 loss 1.2791900634765625\n",
      "planning number: 4 loss 1.065826416015625\n",
      "planning number: 5 loss 1.7223968505859375\n",
      "planning number: 6 loss 0.5648956298828125\n",
      "planning number: 7 loss 0.28824615478515625\n",
      "planning number: 8 loss 0.4468536376953125\n",
      "planning number: 9 loss 0.9208602905273438\n",
      "planning number: 10 loss 0.8417282104492188\n",
      "player: 1 [0.8417282104492188, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "start learning player 1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.23610687255859375\n",
      "planning number: 2 loss 0.00377655029296875\n",
      "player: 1 [0.00377655029296875, 0.0] loops 2\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "start learning player 1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.6858291625976562\n",
      "planning number: 2 loss 0.9448699951171875\n",
      "planning number: 3 loss 1.2000656127929688\n",
      "planning number: 4 loss 0.272369384765625\n",
      "planning number: 5 loss 0.6616592407226562\n",
      "planning number: 6 loss 0.9844894409179688\n",
      "planning number: 7 loss 1.288665771484375\n",
      "planning number: 8 loss 0.0730743408203125\n",
      "planning number: 9 loss 0.4530181884765625\n",
      "planning number: 10 loss 0.3765411376953125\n",
      "player: 1 [0.3765411376953125, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "start learning player 1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.239227294921875\n",
      "planning number: 2 loss 0.3359375\n",
      "planning number: 3 loss 0.8914566040039062\n",
      "planning number: 4 loss 1.4612884521484375\n",
      "planning number: 5 loss 0.21947479248046875\n",
      "planning number: 6 loss 1.0313796997070312\n",
      "planning number: 7 loss 1.7227096557617188\n",
      "planning number: 8 loss 1.9543533325195312\n",
      "planning number: 9 loss 1.42474365234375\n",
      "planning number: 10 loss 1.9093551635742188\n",
      "player: 1 [1.9093551635742188, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "start learning player 1\n",
      "data length: 2\n",
      "planning number: 1 loss 1.6660842895507812\n",
      "planning number: 2 loss 1.5269317626953125\n",
      "planning number: 3 loss 2.0922470092773438\n",
      "planning number: 4 loss 1.031707763671875\n",
      "planning number: 5 loss 0.16745758056640625\n",
      "planning number: 6 loss 0.21929168701171875\n",
      "planning number: 7 loss 1.1475906372070312\n",
      "planning number: 8 loss 1.3997955322265625\n",
      "planning number: 9 loss 0.16875457763671875\n",
      "planning number: 10 loss 0.48813629150390625\n",
      "player: 1 [0.48813629150390625, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "start learning player 1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.325531005859375\n",
      "planning number: 2 loss 0.5313186645507812\n",
      "planning number: 3 loss 0.465728759765625\n",
      "planning number: 4 loss 0.5894012451171875\n",
      "planning number: 5 loss 1.216888427734375\n",
      "planning number: 6 loss 0.9658355712890625\n",
      "planning number: 7 loss 0.30831146240234375\n",
      "planning number: 8 loss 0.44350433349609375\n",
      "planning number: 9 loss 1.4787139892578125\n",
      "planning number: 10 loss 0.44834136962890625\n",
      "player: 1 [0.44834136962890625, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "start learning player 1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.5331878662109375\n",
      "planning number: 2 loss 0.939605712890625\n",
      "planning number: 3 loss 0.9093475341796875\n",
      "planning number: 4 loss 0.5100021362304688\n",
      "planning number: 5 loss 0.07961273193359375\n",
      "planning number: 6 loss 0.20166778564453125\n",
      "planning number: 7 loss 0.7283096313476562\n",
      "planning number: 8 loss 0.05107879638671875\n",
      "planning number: 9 loss 0.5979156494140625\n",
      "planning number: 10 loss 0.3503875732421875\n",
      "player: 1 [0.3503875732421875, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "start learning player 1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.196685791015625\n",
      "planning number: 2 loss 0.8033447265625\n",
      "planning number: 3 loss 0.2943572998046875\n",
      "planning number: 4 loss 0.039825439453125\n",
      "planning number: 5 loss 0.6081085205078125\n",
      "planning number: 6 loss 0.12532806396484375\n",
      "planning number: 7 loss 0.0978240966796875\n",
      "planning number: 8 loss 0.13455963134765625\n",
      "planning number: 9 loss 0.8539505004882812\n",
      "planning number: 10 loss 1.2652969360351562\n",
      "player: 1 [1.2652969360351562, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 1]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "start learning player 1\n",
      "data length: 2\n",
      "planning number: 1 loss 2.90777587890625\n",
      "planning number: 2 loss 2.5245132446289062\n",
      "planning number: 3 loss 1.7395248413085938\n",
      "planning number: 4 loss 0.9833297729492188\n",
      "planning number: 5 loss 0.369903564453125\n",
      "planning number: 6 loss 0.6891670227050781\n",
      "planning number: 7 loss 0.41120147705078125\n",
      "planning number: 8 loss 0.23847198486328125\n",
      "planning number: 9 loss 0.04662322998046875\n",
      "planning number: 10 loss 0.24665069580078125\n",
      "player: 1 [0.24665069580078125, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [2, 1]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [2, 1]\n",
      "start learning player 1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.17803192138671875\n",
      "planning number: 2 loss 0.17752838134765625\n",
      "planning number: 3 loss 0.7020339965820312\n",
      "planning number: 4 loss 0.3798828125\n",
      "planning number: 5 loss 0.8233489990234375\n",
      "planning number: 6 loss 0.6309394836425781\n",
      "planning number: 7 loss 0.2127838134765625\n",
      "planning number: 8 loss 0.6019248962402344\n",
      "planning number: 9 loss 0.4775428771972656\n",
      "planning number: 10 loss 0.22849273681640625\n",
      "player: 1 [0.22849273681640625, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "start learning player 1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.4477882385253906\n",
      "planning number: 2 loss 0.12949752807617188\n",
      "planning number: 3 loss 0.8120574951171875\n",
      "planning number: 4 loss 0.2381439208984375\n",
      "planning number: 5 loss 0.598388671875\n",
      "planning number: 6 loss 0.6158027648925781\n",
      "planning number: 7 loss 0.29785919189453125\n",
      "planning number: 8 loss 0.6560516357421875\n",
      "planning number: 9 loss 0.24129104614257812\n",
      "planning number: 10 loss 0.70343017578125\n",
      "player: 1 [0.70343017578125, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 1]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [2, 1]\n",
      "start learning player 1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.8214645385742188\n",
      "planning number: 2 loss 0.16795730590820312\n",
      "planning number: 3 loss 1.1394424438476562\n",
      "planning number: 4 loss 1.3778190612792969\n",
      "planning number: 5 loss 0.043552398681640625\n",
      "planning number: 6 loss 0.33202362060546875\n",
      "planning number: 7 loss 0.05926513671875\n",
      "planning number: 8 loss 0.29021453857421875\n",
      "planning number: 9 loss 0.4708137512207031\n",
      "planning number: 10 loss 0.14668655395507812\n",
      "player: 1 [0.14668655395507812, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [2, 1]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [2, 1]\n",
      "start learning player 1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.2429962158203125\n",
      "planning number: 2 loss 0.147430419921875\n",
      "planning number: 3 loss 0.47960662841796875\n",
      "planning number: 4 loss 0.5156631469726562\n",
      "planning number: 5 loss 0.22428131103515625\n",
      "planning number: 6 loss 0.41803741455078125\n",
      "planning number: 7 loss 0.01953887939453125\n",
      "player: 1 [0.01953887939453125, 0.0] loops 7\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [3, 1]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [2, 1]\n",
      "start learning player 1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.6324653625488281\n",
      "planning number: 2 loss 0.21734619140625\n",
      "planning number: 3 loss 0.3735771179199219\n",
      "planning number: 4 loss 0.4033317565917969\n",
      "planning number: 5 loss 0.40503692626953125\n",
      "planning number: 6 loss 0.4653205871582031\n",
      "planning number: 7 loss 0.8850288391113281\n",
      "planning number: 8 loss 1.1665229797363281\n",
      "planning number: 9 loss 1.7448158264160156\n",
      "planning number: 10 loss 0.9004135131835938\n",
      "player: 1 [0.9004135131835938, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [2, 1]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [2, 1]\n",
      "start learning player 1\n",
      "data length: 2\n",
      "planning number: 1 loss 1.5914993286132812\n",
      "planning number: 2 loss 1.2781524658203125\n",
      "planning number: 3 loss 1.1314010620117188\n",
      "planning number: 4 loss 1.76318359375\n",
      "planning number: 5 loss 0.58807373046875\n",
      "planning number: 6 loss 0.29476165771484375\n",
      "planning number: 7 loss 0.45284271240234375\n",
      "planning number: 8 loss 0.9590835571289062\n",
      "planning number: 9 loss 0.850677490234375\n",
      "planning number: 10 loss 0.2231597900390625\n",
      "player: 1 [0.2231597900390625, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [2, 1]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [2, 1]\n",
      "start learning player 1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.01918792724609375\n",
      "player: 1 [0.01918792724609375, 0.0] loops 1\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [2, 1]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [2, 1]\n",
      "start learning player 1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.349151611328125\n",
      "planning number: 2 loss 0.0030517578125\n",
      "player: 1 [0.0030517578125, 0.0] loops 2\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [2, 1]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [2, 1]\n",
      "start learning player 1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.6359786987304688\n",
      "planning number: 2 loss 1.071624755859375\n",
      "planning number: 3 loss 1.0976028442382812\n",
      "planning number: 4 loss 0.42287445068359375\n",
      "planning number: 5 loss 0.8296127319335938\n",
      "planning number: 6 loss 0.84271240234375\n",
      "planning number: 7 loss 1.4798736572265625\n",
      "planning number: 8 loss 0.2444610595703125\n",
      "planning number: 9 loss 0.6339187622070312\n",
      "planning number: 10 loss 0.1792755126953125\n",
      "player: 1 [0.1792755126953125, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [2, 1]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [2, 1]\n",
      "start learning player 1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.0152740478515625\n",
      "player: 1 [0.0152740478515625, 0.0] loops 1\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [2, 1]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [2, 1]\n",
      "start learning player 1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.14642333984375\n",
      "planning number: 2 loss 1.17608642578125\n",
      "planning number: 3 loss 1.1409378051757812\n",
      "planning number: 4 loss 0.652679443359375\n",
      "planning number: 5 loss 1.2172927856445312\n",
      "planning number: 6 loss 0.8315658569335938\n",
      "planning number: 7 loss 0.8694000244140625\n",
      "planning number: 8 loss 1.2523345947265625\n",
      "planning number: 9 loss 0.37899017333984375\n",
      "planning number: 10 loss 0.72808837890625\n",
      "player: 1 [0.72808837890625, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [2, 1]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [2, 1]\n",
      "start learning player 1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.01149749755859375\n",
      "player: 1 [0.01149749755859375, 0.0] loops 1\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [2, 1]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [2, 1]\n",
      "start learning player 1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.07558441162109375\n",
      "planning number: 2 loss 0.9484024047851562\n",
      "planning number: 3 loss 0.6096954345703125\n",
      "planning number: 4 loss 0.135955810546875\n",
      "planning number: 5 loss 0.38930511474609375\n",
      "planning number: 6 loss 0.0559539794921875\n",
      "planning number: 7 loss 1.3220901489257812\n",
      "planning number: 8 loss 1.05596923828125\n",
      "planning number: 9 loss 0.29935455322265625\n",
      "planning number: 10 loss 0.61798095703125\n",
      "player: 1 [0.61798095703125, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [2, 1]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [2, 1]\n",
      "start learning player 1\n",
      "data length: 2\n",
      "planning number: 1 loss 1.0928421020507812\n",
      "planning number: 2 loss 1.1818389892578125\n",
      "planning number: 3 loss 0.0638427734375\n",
      "planning number: 4 loss 0.175079345703125\n",
      "planning number: 5 loss 0.17327880859375\n",
      "planning number: 6 loss 0.8314971923828125\n",
      "planning number: 7 loss 1.4843521118164062\n",
      "planning number: 8 loss 0.24542236328125\n",
      "planning number: 9 loss 1.0848617553710938\n",
      "planning number: 10 loss 1.6837310791015625\n",
      "player: 1 [1.6837310791015625, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [2, 1]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [3, 3]\n",
      "start learning player 1\n",
      "data length: 2\n",
      "planning number: 1 loss 2.9012985229492188\n",
      "planning number: 2 loss 1.4153900146484375\n",
      "planning number: 3 loss 0.7279281616210938\n",
      "planning number: 4 loss 0.407562255859375\n",
      "planning number: 5 loss 0.8284912109375\n",
      "planning number: 6 loss 0.5462265014648438\n",
      "planning number: 7 loss 0.45074462890625\n",
      "planning number: 8 loss 0.9100265502929688\n",
      "planning number: 9 loss 0.782318115234375\n",
      "planning number: 10 loss 0.00370025634765625\n",
      "player: 1 [0.00370025634765625, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [2, 1]\n",
      "start learning player 1\n",
      "data length: 2\n",
      "planning number: 1 loss 51.587528228759766\n",
      "planning number: 2 loss 47.57286071777344\n",
      "planning number: 3 loss 42.82727813720703\n",
      "planning number: 4 loss 38.44919967651367\n",
      "planning number: 5 loss 35.134803771972656\n",
      "planning number: 6 loss 32.57070541381836\n",
      "planning number: 7 loss 30.45504379272461\n",
      "planning number: 8 loss 28.575618743896484\n",
      "planning number: 9 loss 27.098121643066406\n",
      "planning number: 10 loss 25.897289276123047\n",
      "player: 1 [25.897289276123047, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [3, 1]\n",
      "start learning player -1\n",
      "data length: 2\n",
      "planning number: 1 loss 52.8646354675293\n",
      "planning number: 2 loss 48.205596923828125\n",
      "planning number: 3 loss 43.345909118652344\n",
      "planning number: 4 loss 39.41240692138672\n",
      "planning number: 5 loss 36.13957595825195\n",
      "planning number: 6 loss 33.31626510620117\n",
      "planning number: 7 loss 30.823150634765625\n",
      "planning number: 8 loss 28.47753143310547\n",
      "planning number: 9 loss 27.11975860595703\n",
      "planning number: 10 loss 26.331350326538086\n",
      "player: -1 [26.331350326538086, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [3, 3]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [3, 1]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [3, 1]\n",
      "start learning player -1\n",
      "data length: 2\n",
      "planning number: 1 loss 46.550235748291016\n",
      "planning number: 2 loss 46.37627029418945\n",
      "planning number: 3 loss 46.207462310791016\n",
      "planning number: 4 loss 46.108097076416016\n",
      "planning number: 5 loss 46.04945755004883\n",
      "planning number: 6 loss 46.014827728271484\n",
      "planning number: 7 loss 45.99449157714844\n",
      "planning number: 8 loss 45.982452392578125\n",
      "planning number: 9 loss 45.9753303527832\n",
      "planning number: 10 loss 45.97111129760742\n",
      "player: -1 [45.97111129760742, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [2, 3]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 1]\n",
      "start learning player 1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.4118404984474182\n",
      "planning number: 2 loss 0.14091885089874268\n",
      "planning number: 3 loss 0.1574488878250122\n",
      "planning number: 4 loss 0.0845000147819519\n",
      "planning number: 5 loss 0.028427064418792725\n",
      "planning number: 6 loss 0.013034522533416748\n",
      "player: 1 [0.013034522533416748, 0.0] loops 6\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [3, 1]\n",
      "start learning player -1\n",
      "data length: 2\n",
      "planning number: 1 loss 48.009620666503906\n",
      "planning number: 2 loss 47.54494857788086\n",
      "planning number: 3 loss 47.17619705200195\n",
      "planning number: 4 loss 46.712562561035156\n",
      "planning number: 5 loss 46.172996520996094\n",
      "planning number: 6 loss 45.47406005859375\n",
      "planning number: 7 loss 44.6264533996582\n",
      "planning number: 8 loss 43.66952133178711\n",
      "planning number: 9 loss 42.69524383544922\n",
      "planning number: 10 loss 41.50140380859375\n",
      "player: -1 [41.50140380859375, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [2, 3]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [2, 3]\n",
      "start learning player -1\n",
      "data length: 2\n",
      "planning number: 1 loss 85.54923248291016\n",
      "planning number: 2 loss 80.74671936035156\n",
      "planning number: 3 loss 73.23500061035156\n",
      "planning number: 4 loss 62.13794708251953\n",
      "planning number: 5 loss 44.745086669921875\n",
      "planning number: 6 loss 16.56365203857422\n",
      "planning number: 7 loss 17.144683837890625\n",
      "planning number: 8 loss 17.096900939941406\n",
      "planning number: 9 loss 12.460502624511719\n",
      "planning number: 10 loss 12.084754943847656\n",
      "player: -1 [12.084754943847656, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 2]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [3, 3]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [3, 3]\n",
      "start learning player 1\n",
      "data length: 2\n",
      "planning number: 1 loss 49.56276321411133\n",
      "planning number: 2 loss 49.194889068603516\n",
      "planning number: 3 loss 48.87489318847656\n",
      "planning number: 4 loss 48.9325065612793\n",
      "planning number: 5 loss 48.89067077636719\n",
      "planning number: 6 loss 48.730979919433594\n",
      "planning number: 7 loss 48.62246322631836\n",
      "planning number: 8 loss 48.46868133544922\n",
      "planning number: 9 loss 48.258304595947266\n",
      "planning number: 10 loss 48.02177429199219\n",
      "player: 1 [48.02177429199219, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [2, 1]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [2, 1]\n",
      "start learning player 1\n",
      "data length: 2\n",
      "planning number: 1 loss 96.7601318359375\n",
      "planning number: 2 loss 95.49263763427734\n",
      "planning number: 3 loss 93.4664306640625\n",
      "planning number: 4 loss 90.85035705566406\n",
      "planning number: 5 loss 87.14335632324219\n",
      "planning number: 6 loss 82.89460754394531\n",
      "planning number: 7 loss 77.89846801757812\n",
      "planning number: 8 loss 70.79302978515625\n",
      "planning number: 9 loss 59.13328552246094\n",
      "planning number: 10 loss 37.78856658935547\n",
      "player: 1 [37.78856658935547, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 1]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 1]\n",
      "start learning player 1\n",
      "data length: 2\n",
      "planning number: 1 loss 3.107715606689453\n",
      "planning number: 2 loss 22.345077514648438\n",
      "planning number: 3 loss 5.108329772949219\n",
      "planning number: 4 loss 8.307918548583984\n",
      "planning number: 5 loss 6.865009307861328\n",
      "planning number: 6 loss 4.388725280761719\n",
      "planning number: 7 loss 1.2388458251953125\n",
      "planning number: 8 loss 1.5696983337402344\n",
      "planning number: 9 loss 1.9327278137207031\n",
      "planning number: 10 loss 0.21541976928710938\n",
      "player: 1 [0.21541976928710938, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A78FDFD0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 3]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 2]\n",
      "start learning player -1\n",
      "data length: 2\n",
      "planning number: 1 loss 1.3515396118164062\n",
      "planning number: 2 loss 0.4686851501464844\n",
      "planning number: 3 loss 0.446014404296875\n",
      "planning number: 4 loss 0.2775535583496094\n",
      "planning number: 5 loss 0.17097091674804688\n",
      "planning number: 6 loss 0.3815345764160156\n",
      "planning number: 7 loss 0.455780029296875\n",
      "planning number: 8 loss 0.8399810791015625\n",
      "planning number: 9 loss 1.1417503356933594\n",
      "planning number: 10 loss 0.35693359375\n",
      "player: -1 [0.35693359375, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 2]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 2]\n",
      "start learning player -1\n",
      "data length: 2\n",
      "planning number: 1 loss 1.8877105712890625\n",
      "planning number: 2 loss 0.0302734375\n",
      "planning number: 3 loss 0.39563751220703125\n",
      "planning number: 4 loss 0.0523834228515625\n",
      "planning number: 5 loss 0.44512176513671875\n",
      "planning number: 6 loss 0.5480117797851562\n",
      "planning number: 7 loss 0.36930084228515625\n",
      "planning number: 8 loss 0.5207748413085938\n",
      "planning number: 9 loss 1.1006088256835938\n",
      "planning number: 10 loss 0.941009521484375\n",
      "player: -1 [0.941009521484375, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 2]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 2]\n",
      "start learning player -1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.5353851318359375\n",
      "planning number: 2 loss 1.164337158203125\n",
      "planning number: 3 loss 2.40643310546875\n",
      "planning number: 4 loss 0.6534423828125\n",
      "planning number: 5 loss 0.6901168823242188\n",
      "planning number: 6 loss 0.5072784423828125\n",
      "planning number: 7 loss 0.6482772827148438\n",
      "planning number: 8 loss 0.748260498046875\n",
      "planning number: 9 loss 0.25809478759765625\n",
      "planning number: 10 loss 0.40175628662109375\n",
      "player: -1 [0.40175628662109375, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 2]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 2]\n",
      "start learning player -1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.33681488037109375\n",
      "planning number: 2 loss 0.3170013427734375\n",
      "planning number: 3 loss 0.3993377685546875\n",
      "planning number: 4 loss 0.2623748779296875\n",
      "planning number: 5 loss 0.6196670532226562\n",
      "planning number: 6 loss 0.6931304931640625\n",
      "planning number: 7 loss 0.3208160400390625\n",
      "planning number: 8 loss 0.61541748046875\n",
      "planning number: 9 loss 0.37530517578125\n",
      "planning number: 10 loss 0.7429046630859375\n",
      "player: -1 [0.7429046630859375, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 2]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 2]\n",
      "start learning player -1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.4569091796875\n",
      "planning number: 2 loss 0.22126007080078125\n",
      "planning number: 3 loss 0.03475189208984375\n",
      "planning number: 4 loss 1.4832534790039062\n",
      "planning number: 5 loss 1.59124755859375\n",
      "planning number: 6 loss 0.9495849609375\n",
      "planning number: 7 loss 1.4523391723632812\n",
      "planning number: 8 loss 1.4215087890625\n",
      "planning number: 9 loss 0.7142715454101562\n",
      "planning number: 10 loss 0.8534088134765625\n",
      "player: -1 [0.8534088134765625, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 2]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 2]\n",
      "start learning player -1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.0682525634765625\n",
      "planning number: 2 loss 0.102508544921875\n",
      "planning number: 3 loss 1.2070159912109375\n",
      "planning number: 4 loss 0.9313125610351562\n",
      "planning number: 5 loss 0.3695831298828125\n",
      "planning number: 6 loss 0.6171035766601562\n",
      "planning number: 7 loss 0.6899642944335938\n",
      "planning number: 8 loss 0.24983978271484375\n",
      "planning number: 9 loss 0.38225555419921875\n",
      "planning number: 10 loss 0.27510833740234375\n",
      "player: -1 [0.27510833740234375, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 2]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 2]\n",
      "start learning player -1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.24842071533203125\n",
      "planning number: 2 loss 0.41075897216796875\n",
      "planning number: 3 loss 0.298370361328125\n",
      "planning number: 4 loss 0.493408203125\n",
      "planning number: 5 loss 0.55029296875\n",
      "planning number: 6 loss 0.05035400390625\n",
      "planning number: 7 loss 0.3136138916015625\n",
      "planning number: 8 loss 0.1046142578125\n",
      "planning number: 9 loss 1.194000244140625\n",
      "planning number: 10 loss 1.6752853393554688\n",
      "player: -1 [1.6752853393554688, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 2]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 2]\n",
      "start learning player -1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.5871429443359375\n",
      "planning number: 2 loss 0.4778900146484375\n",
      "planning number: 3 loss 1.0463638305664062\n",
      "planning number: 4 loss 2.0782089233398438\n",
      "planning number: 5 loss 2.4842071533203125\n",
      "planning number: 6 loss 1.6204910278320312\n",
      "planning number: 7 loss 2.3712921142578125\n",
      "planning number: 8 loss 2.0199203491210938\n",
      "planning number: 9 loss 1.7253189086914062\n",
      "planning number: 10 loss 0.26123809814453125\n",
      "player: -1 [0.26123809814453125, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 2]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 2]\n",
      "start learning player -1\n",
      "data length: 2\n",
      "planning number: 1 loss 1.2886734008789062\n",
      "planning number: 2 loss 2.027069091796875\n",
      "planning number: 3 loss 3.029083251953125\n",
      "planning number: 4 loss 3.0331878662109375\n",
      "planning number: 5 loss 1.4371795654296875\n",
      "planning number: 6 loss 0.5725555419921875\n",
      "planning number: 7 loss 1.2546920776367188\n",
      "planning number: 8 loss 0.6271743774414062\n",
      "planning number: 9 loss 0.167877197265625\n",
      "planning number: 10 loss 0.7837753295898438\n",
      "player: -1 [0.7837753295898438, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 2]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 2]\n",
      "start learning player -1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.08303070068359375\n",
      "planning number: 2 loss 0.092193603515625\n",
      "planning number: 3 loss 0.9927139282226562\n",
      "planning number: 4 loss 0.9484634399414062\n",
      "planning number: 5 loss 0.0019073486328125\n",
      "player: -1 [0.0019073486328125, 0.0] loops 5\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 2]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 2]\n",
      "start learning player -1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.0880126953125\n",
      "planning number: 2 loss 0.8924484252929688\n",
      "planning number: 3 loss 0.32132720947265625\n",
      "planning number: 4 loss 0.7861404418945312\n",
      "planning number: 5 loss 0.4914093017578125\n",
      "planning number: 6 loss 0.5838546752929688\n",
      "planning number: 7 loss 1.09515380859375\n",
      "planning number: 8 loss 0.978607177734375\n",
      "planning number: 9 loss 0.1560211181640625\n",
      "planning number: 10 loss 0.9403076171875\n",
      "player: -1 [0.9403076171875, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 2]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 2]\n",
      "start learning player -1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.1963348388671875\n",
      "planning number: 2 loss 0.1025238037109375\n",
      "planning number: 3 loss 1.1455764770507812\n",
      "planning number: 4 loss 0.1078033447265625\n",
      "planning number: 5 loss 1.2642974853515625\n",
      "planning number: 6 loss 1.8166961669921875\n",
      "planning number: 7 loss 0.2699432373046875\n",
      "planning number: 8 loss 0.6265029907226562\n",
      "planning number: 9 loss 0.5358657836914062\n",
      "planning number: 10 loss 0.67559814453125\n",
      "player: -1 [0.67559814453125, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 2]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 2]\n",
      "start learning player -1\n",
      "data length: 2\n",
      "planning number: 1 loss 1.5050277709960938\n",
      "planning number: 2 loss 1.169158935546875\n",
      "planning number: 3 loss 0.33889007568359375\n",
      "planning number: 4 loss 0.4609222412109375\n",
      "planning number: 5 loss 0.43802642822265625\n",
      "planning number: 6 loss 0.031158447265625\n",
      "planning number: 7 loss 0.07428741455078125\n",
      "planning number: 8 loss 0.5198440551757812\n",
      "planning number: 9 loss 0.5143356323242188\n",
      "planning number: 10 loss 1.1478195190429688\n",
      "player: -1 [1.1478195190429688, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 2]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 2]\n",
      "start learning player -1\n",
      "data length: 2\n",
      "planning number: 1 loss 1.161163330078125\n",
      "planning number: 2 loss 2.2022705078125\n",
      "planning number: 3 loss 0.8875732421875\n",
      "planning number: 4 loss 0.9079055786132812\n",
      "planning number: 5 loss 0.510589599609375\n",
      "planning number: 6 loss 0.46846771240234375\n",
      "planning number: 7 loss 0.8023834228515625\n",
      "planning number: 8 loss 0.4525909423828125\n",
      "planning number: 9 loss 0.6205673217773438\n",
      "planning number: 10 loss 1.1328125\n",
      "player: -1 [1.1328125, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 2]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 2]\n",
      "start learning player -1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.17938995361328125\n",
      "planning number: 2 loss 0.7581024169921875\n",
      "planning number: 3 loss 0.647430419921875\n",
      "planning number: 4 loss 0.26425933837890625\n",
      "planning number: 5 loss 0.6867294311523438\n",
      "planning number: 6 loss 0.209136962890625\n",
      "planning number: 7 loss 0.76507568359375\n",
      "planning number: 8 loss 1.932647705078125\n",
      "planning number: 9 loss 0.37332916259765625\n",
      "planning number: 10 loss 0.6977386474609375\n",
      "player: -1 [0.6977386474609375, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 2]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 2]\n",
      "start learning player -1\n",
      "data length: 2\n",
      "planning number: 1 loss 1.1880569458007812\n",
      "planning number: 2 loss 0.697265625\n",
      "planning number: 3 loss 1.0730972290039062\n",
      "planning number: 4 loss 1.1046905517578125\n",
      "planning number: 5 loss 1.236419677734375\n",
      "planning number: 6 loss 0.6011199951171875\n",
      "planning number: 7 loss 1.1336593627929688\n",
      "planning number: 8 loss 0.7145462036132812\n",
      "planning number: 9 loss 0.072723388671875\n",
      "planning number: 10 loss 1.3264999389648438\n",
      "player: -1 [1.3264999389648438, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 2]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 2]\n",
      "start learning player -1\n",
      "data length: 2\n",
      "planning number: 1 loss 1.7374343872070312\n",
      "planning number: 2 loss 2.6583023071289062\n",
      "planning number: 3 loss 1.2945785522460938\n",
      "planning number: 4 loss 0.6274490356445312\n",
      "planning number: 5 loss 0.498077392578125\n",
      "planning number: 6 loss 0.9277420043945312\n",
      "planning number: 7 loss 0.6467056274414062\n",
      "planning number: 8 loss 0.36992645263671875\n",
      "planning number: 9 loss 0.8508148193359375\n",
      "planning number: 10 loss 0.73468017578125\n",
      "player: -1 [0.73468017578125, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 2]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 2]\n",
      "start learning player -1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.05348968505859375\n",
      "planning number: 2 loss 0.28275299072265625\n",
      "planning number: 3 loss 0.7846145629882812\n",
      "planning number: 4 loss 0.6263046264648438\n",
      "planning number: 5 loss 0.27202606201171875\n",
      "planning number: 6 loss 0.6962738037109375\n",
      "planning number: 7 loss 0.23487091064453125\n",
      "planning number: 8 loss 0.7810821533203125\n",
      "planning number: 9 loss 1.856719970703125\n",
      "planning number: 10 loss 0.395751953125\n",
      "player: -1 [0.395751953125, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 2]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 2]\n",
      "start learning player -1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.65447998046875\n",
      "planning number: 2 loss 1.188690185546875\n",
      "planning number: 3 loss 0.6602249145507812\n",
      "planning number: 4 loss 1.0323333740234375\n",
      "planning number: 5 loss 1.0963363647460938\n",
      "planning number: 6 loss 1.2011566162109375\n",
      "planning number: 7 loss 0.594512939453125\n",
      "planning number: 8 loss 1.1125030517578125\n",
      "planning number: 9 loss 0.7044677734375\n",
      "planning number: 10 loss 0.0790557861328125\n",
      "player: -1 [0.0790557861328125, 0.0] loops 10\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 2]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 2]\n",
      "start learning player -1\n",
      "data length: 2\n",
      "planning number: 1 loss 1.3113861083984375\n",
      "planning number: 2 loss 1.6915969848632812\n",
      "planning number: 3 loss 0.555267333984375\n",
      "planning number: 4 loss 0.8460006713867188\n",
      "planning number: 5 loss 0.38709259033203125\n",
      "planning number: 6 loss 0.6800765991210938\n",
      "planning number: 7 loss 0.0098419189453125\n",
      "player: -1 [0.0098419189453125, 0.0] loops 7\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 2]\n",
      "Calling makeMove(<__main__.QPlayer object at 0x000001A6A00760A0>, <Board.Board object at 0x000001A6A7C57580>)\n",
      "'makeMove' returned [1, 2]\n",
      "start learning player -1\n",
      "data length: 2\n",
      "planning number: 1 loss 0.04721832275390625\n",
      "planning number: 2 loss 0.802734375\n",
      "planning number: 3 loss 0.726898193359375\n",
      "planning number: 4 loss 0.15592193603515625\n",
      "planning number: 5 loss 0.5606689453125\n",
      "planning number: 6 loss 0.404296875\n",
      "planning number: 7 loss 0.4001922607421875\n"
     ]
    }
   ],
   "source": [
    "for j in trange(100):\n",
    "\n",
    "    player1 = QPlayer('x', QNNLearner(model=TicTacToeModel(1)))\n",
    "    player2 = QPlayer('o', QNNLearner(model=TicTacToeModel(-1)))\n",
    "    players = [player1, player2]\n",
    "\n",
    "    game = Game(players)\n",
    "\n",
    "    for i in range(1000):\n",
    "        game.run()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}